{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Robot MCP Documentation","text":"<p>Welcome to the Robot MCP documentation!</p> <p>Control robotic arms through natural language using FastMCP and multiple LLM providers (OpenAI, Groq, Gemini, Ollama).</p>"},{"location":"#quick-links","title":"\ud83c\udfaf Quick Links","text":"<ul> <li> <p> Quick Start</p> <p>Get started in minutes with our quick start guide</p> <p> Installation</p> </li> <li> <p> User Guide</p> <p>Learn how to use Robot MCP effectively</p> <p> Setup Guide</p> </li> <li> <p> API Reference</p> <p>Complete API documentation for all tools</p> <p> API Docs</p> </li> <li> <p> Examples</p> <p>Real-world examples and use cases</p> <p> Examples</p> </li> </ul>"},{"location":"#features","title":"Features","text":"<p>\u2728 Natural Language Control - No programming required \ud83e\udd16 Multi-LLM Support - OpenAI, Groq, Gemini, Ollama \ud83c\udfaf Auto-Detection - Automatically selects available API \ud83d\udd04 Hot-Swapping - Switch providers during runtime \ud83e\udd16 Multi-Robot Support - Niryo Ned2 and WidowX \ud83d\udc41\ufe0f Vision-Based Detection - Automatic object detection</p>"},{"location":"#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Multi-    \u2502  HTTP   \u2502              \u2502  Python \u2502             \u2502\n\u2502   LLM       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 FastMCP      \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   Niryo/    \u2502\n\u2502  (OpenAI/   \u2502  SSE    \u2502 Server       \u2502   API   \u2502   WidowX    \u2502\n\u2502 Groq/Gemini)\u2502         \u2502              \u2502         \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install the package</li> <li>Configure API keys</li> <li>Start the server</li> <li>Try examples</li> </ol>"},{"location":"#support","title":"Support","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"INSTALLATION/","title":"Installation Guide - Robot MCP v0.3.0","text":"<p>Complete installation guide for the Robot MCP Control System with multi-LLM support.</p>"},{"location":"INSTALLATION/#prerequisites","title":"\ud83d\udccb Prerequisites","text":""},{"location":"INSTALLATION/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>Operating System: Linux, macOS, or Windows</li> <li>RAM: Minimum 4 GB (8 GB recommended)</li> <li>GPU: Optional, but recommended for better performance</li> <li>Redis: Required for camera streaming and communication</li> </ul>"},{"location":"INSTALLATION/#software-prerequisites","title":"Software Prerequisites","text":"<pre><code># Check Python version\npython --version  # or python3 --version\n\n# Update pip\npip install --upgrade pip\n\n# Git (for development)\ngit --version\n</code></pre>"},{"location":"INSTALLATION/#installation","title":"\ud83d\ude80 Installation","text":""},{"location":"INSTALLATION/#option-1-standard-installation-recommended","title":"Option 1: Standard Installation (Recommended)","text":"<pre><code># 1. Clone repository (if not already done)\ngit clone https://github.com/dgaida/robot_mcp.git\ncd robot_mcp\n\n# 2. Create virtual environment\npython -m venv venv\n\n# 3. Activate virtual environment\n# Linux/Mac:\nsource venv/bin/activate\n# Windows:\nvenv\\Scripts\\activate\n\n# 4. Install package with all dependencies\npip install -e \".[all]\"\n</code></pre>"},{"location":"INSTALLATION/#option-2-minimal-installation","title":"Option 2: Minimal Installation","text":"<p>Only MCP Server and Client, without GUI:</p> <pre><code>pip install -e \".\"\n</code></pre>"},{"location":"INSTALLATION/#option-3-custom-installation","title":"Option 3: Custom Installation","text":"<p>Choose only the components you need:</p> <pre><code># Only GUI components\npip install -e \".[gui]\"\n\n# Development tools\npip install -e \".[dev]\"\n\n# Documentation tools\npip install -e \".[docs]\"\n\n# Everything\npip install -e \".[all]\"\n</code></pre>"},{"location":"INSTALLATION/#dependencies-by-component","title":"\ud83d\udce6 Dependencies by Component","text":""},{"location":"INSTALLATION/#core-always-required","title":"Core (Always Required)","text":"<p>These are installed automatically with the base package:</p> <pre><code>fastmcp&gt;=0.1.0              # Modern MCP implementation\nllm_client                  # Multi-LLM support (from GitHub)\npython-dotenv&gt;=1.0.0        # Environment variable management\nrobot-environment           # Robot control (from GitHub)\ntext2speech                 # TTS integration (from GitHub)\nspeech2text                 # STT integration (from GitHub)\n</code></pre>"},{"location":"INSTALLATION/#gui-components","title":"GUI Components","text":"<pre><code>pip install -e \".[gui]\"\n\n# Installs:\n# - gradio&gt;=4.0.0\n# - redis_robot_comm (from GitHub)\n</code></pre>"},{"location":"INSTALLATION/#development-tools","title":"Development Tools","text":"<pre><code>pip install -e \".[dev]\"\n\n# Installs:\n# - pytest&gt;=7.0.0\n# - pytest-asyncio&gt;=0.21.0\n# - pytest-cov&gt;=4.0.0\n# - black&gt;=23.0.0\n# - ruff&gt;=0.1.0\n# - mypy&gt;=1.0.0\n# - isort&gt;=5.12.0\n</code></pre>"},{"location":"INSTALLATION/#documentation","title":"Documentation","text":"<pre><code>pip install -e \".[docs]\"\n\n# Installs:\n# - sphinx&gt;=5.0.0\n# - sphinx-rtd-theme&gt;=1.2.0\n# - myst-parser&gt;=1.0.0\n</code></pre>"},{"location":"INSTALLATION/#api-keys-configuration","title":"\ud83d\udd11 API Keys Configuration","text":"<p>The system now supports 4 LLM providers. You need at least one API key (or use Ollama for local/offline operation).</p>"},{"location":"INSTALLATION/#supported-providers","title":"Supported Providers","text":"Provider Cost Speed Best For OpenAI $$ Fast Production, complex reasoning Groq Free tier Very Fast Development, prototyping Gemini Free tier Fast Long context, multimodal Ollama Free (local) Variable Offline, privacy"},{"location":"INSTALLATION/#1-get-api-keys","title":"1. Get API Keys","text":"<p>OpenAI (GPT-4o, GPT-4o-mini): <pre><code># 1. Go to https://platform.openai.com/api-keys\n# 2. Create account or sign in\n# 3. Click \"Create new secret key\"\n# 4. Copy the key (starts with sk-...)\n</code></pre></p> <p>Groq (Kimi, Llama, Mixtral) - Free tier available: <pre><code># 1. Go to https://console.groq.com/keys\n# 2. Create account or sign in\n# 3. Click \"Create API Key\"\n# 4. Copy the key (starts with gsk_...)\n</code></pre></p> <p>Google Gemini (Gemini 2.0, 2.5): <pre><code># 1. Go to https://aistudio.google.com/apikey\n# 2. Sign in with Google account\n# 3. Click \"Create API Key\"\n# 4. Copy the key (starts with AIzaSy...)\n</code></pre></p> <p>Ollama (Local models) - No API key needed: <pre><code># 1. Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# 2. Pull a model (e.g., llama3.2:1b)\nollama pull llama3.2:1b\n\n# 3. Start Ollama service\nollama serve\n</code></pre></p>"},{"location":"INSTALLATION/#2-optional-elevenlabs-api-key","title":"2. Optional: ElevenLabs API Key","text":"<p>For enhanced text-to-speech quality:</p> <pre><code># 1. Go to https://elevenlabs.io\n# 2. Create account\n# 3. Go to Settings &gt; API Keys\n# 4. Copy the key\n</code></pre>"},{"location":"INSTALLATION/#3-store-keys-in-secretsenv","title":"3. Store Keys in secrets.env","text":"<pre><code># Copy template\ncp secrets.env.template secrets.env\n\n# Edit the file\nnano secrets.env  # or use your preferred editor\n</code></pre> <p>secrets.env content:</p> <pre><code># ============================================================================\n# LLM Provider API Keys (add at least one)\n# ============================================================================\n\n# OpenAI (best for production)\nOPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n# Groq (fastest, free tier available) - RECOMMENDED FOR GETTING STARTED\nGROQ_API_KEY=gsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n# Google Gemini (free tier available)\nGEMINI_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n# Ollama - No key needed, just install and run\n# Installation: curl -fsSL https://ollama.ai/install.sh | sh\n\n# ============================================================================\n# Optional: Enhanced TTS\n# ============================================================================\n\n# ElevenLabs (for better voice quality)\nELEVENLABS_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Set file permissions (Linux/Mac):</p> <pre><code>chmod 600 secrets.env\n</code></pre> <p>Add to .gitignore:</p> <pre><code>echo \"secrets.env\" &gt;&gt; .gitignore\n</code></pre>"},{"location":"INSTALLATION/#4-api-selection-priority","title":"4. API Selection Priority","text":"<p>If you configure multiple API keys, the system uses this priority:</p> <ol> <li>OpenAI (if <code>OPENAI_API_KEY</code> is set)</li> <li>Groq (if <code>GROQ_API_KEY</code> is set)</li> <li>Gemini (if <code>GEMINI_API_KEY</code> is set)</li> <li>Ollama (fallback if installed, no key needed)</li> </ol> <p>Override with --api flag:</p> <pre><code># Force specific provider\npython client/fastmcp_universal_client.py --api groq\npython client/fastmcp_universal_client.py --api gemini\npython client/fastmcp_universal_client.py --api ollama\n</code></pre>"},{"location":"INSTALLATION/#verification-of-installation","title":"\ud83d\udd27 Verification of Installation","text":""},{"location":"INSTALLATION/#1-test-python-imports","title":"1. Test Python Imports","text":"<pre><code># test_imports.py\nimport fastmcp\nfrom llm_client import LLMClient\nfrom robot_environment import Environment\nfrom text2speech import Text2Speech\nimport gradio\n\nprint(\"\u2713 All core imports successful!\")\n</code></pre> <pre><code>python test_imports.py\n</code></pre>"},{"location":"INSTALLATION/#2-check-package-installation","title":"2. Check Package Installation","text":"<pre><code># Show installed package\npip list | grep robot-mcp\n\n# Show package details\npip show robot-mcp\n</code></pre> <p>Expected output: <pre><code>Name: robot-mcp\nVersion: 0.3.0\nSummary: Natural language robot control using FastMCP with multi-LLM support\n</code></pre></p>"},{"location":"INSTALLATION/#3-test-command-line-scripts","title":"3. Test Command-line Scripts","text":"<p>The package installs several command-line scripts:</p> <pre><code># Test if scripts are installed\nrobot-fastmcp-server --help\nrobot-universal-client --help\nrobot-groq-client --help\nrobot-gui --help\nrobot-examples --help\n</code></pre>"},{"location":"INSTALLATION/#4-test-mcp-server","title":"4. Test MCP Server","text":"<pre><code># Start server in background\npython server/fastmcp_robot_server.py --robot niryo &amp;\n\n# Wait for startup\nsleep 5\n\n# Test if server is running\ncurl http://127.0.0.1:8000/sse\n\n# Expected: Connection established or SSE stream starts\n\n# Stop server\npkill -f fastmcp_robot_server\n</code></pre>"},{"location":"INSTALLATION/#5-test-universal-client-multi-llm","title":"5. Test Universal Client (Multi-LLM)","text":"<pre><code># Auto-detect available API\npython client/fastmcp_universal_client.py --command \"What LLM provider am I using?\"\n\n# Test specific provider\npython client/fastmcp_universal_client.py --api groq --command \"Hello\"\npython client/fastmcp_universal_client.py --api openai --command \"Hello\"\n</code></pre>"},{"location":"INSTALLATION/#6-verify-redis-connection","title":"6. Verify Redis Connection","text":"<pre><code># Check if Redis is running\nredis-cli ping\n\n# Expected output: PONG\n\n# If Redis is not running:\ndocker run -p 6379:6379 redis:alpine\n# or\nredis-server\n</code></pre>"},{"location":"INSTALLATION/#common-installation-problems","title":"\ud83d\udee0\ufe0f Common Installation Problems","text":""},{"location":"INSTALLATION/#problem-package-not-found-after-installation","title":"Problem: Package not found after installation","text":"<p>Symptom: <pre><code>ModuleNotFoundError: No module named 'robot_mcp'\n</code></pre></p> <p>Solution: <pre><code># Make sure you're in the virtual environment\nwhich python  # Should show venv path\n\n# Reinstall package\npip install -e \".[all]\"\n\n# Verify installation\npip show robot-mcp\n</code></pre></p>"},{"location":"INSTALLATION/#problem-llm_client-installation-failed","title":"Problem: llm_client installation failed","text":"<p>Symptom: <pre><code>ERROR: Could not find a version that satisfies the requirement llm_client\n</code></pre></p> <p>Solution: <pre><code># Install directly from GitHub\npip install git+https://github.com/dgaida/llm_client.git\n\n# Then reinstall robot-mcp\npip install -e \".[all]\"\n</code></pre></p>"},{"location":"INSTALLATION/#problem-robot-environment-installation-failed","title":"Problem: robot-environment installation failed","text":"<p>Symptom: <pre><code>ERROR: Could not find a version that satisfies the requirement robot-environment\n</code></pre></p> <p>Solution: <pre><code># Install directly from GitHub\npip install git+https://github.com/dgaida/robot_environment.git\n\n# Then reinstall robot-mcp\npip install -e \".[all]\"\n</code></pre></p>"},{"location":"INSTALLATION/#problem-openai-client-not-available","title":"Problem: OpenAI client not available","text":"<p>Symptom: <pre><code>ImportError: No module named 'openai'\n</code></pre></p> <p>Solution: <pre><code># The llm_client package should install this, but if not:\npip install openai\n\n# Or reinstall with all dependencies\npip install -e \".[all]\" --force-reinstall\n</code></pre></p>"},{"location":"INSTALLATION/#problem-groq-client-not-available","title":"Problem: Groq client not available","text":"<p>Symptom: <pre><code>ImportError: No module named 'groq'\n</code></pre></p> <p>Solution: <pre><code># Install Groq client\npip install groq\n\n# Or reinstall with all dependencies\npip install -e \".[all]\" --force-reinstall\n</code></pre></p>"},{"location":"INSTALLATION/#problem-ollama-not-responding","title":"Problem: Ollama not responding","text":"<p>Symptom: <pre><code>Error: Ollama server not responding\n</code></pre></p> <p>Solution: <pre><code># Check if Ollama is installed\nollama --version\n\n# If not installed:\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Start Ollama service\nollama serve\n\n# Pull a model\nollama pull llama3.2:1b\n\n# Test\nollama run llama3.2:1b \"Hello\"\n</code></pre></p>"},{"location":"INSTALLATION/#problem-fastmcp-not-found","title":"Problem: fastmcp not found","text":"<p>Symptom: <pre><code>ModuleNotFoundError: No module named 'fastmcp'\n</code></pre></p> <p>Solution: <pre><code># Install fastmcp\npip install fastmcp\n\n# Or reinstall robot-mcp with all dependencies\npip install -e \".[all]\" --force-reinstall\n</code></pre></p>"},{"location":"INSTALLATION/#problem-gradio-installation-failed","title":"Problem: gradio installation failed","text":"<p>Symptom: <pre><code>ERROR: Could not install packages due to an OSError\n</code></pre></p> <p>Solution: <pre><code># Update pip and setuptools\npip install --upgrade pip setuptools wheel\n\n# Install gradio separately\npip install gradio\n\n# Then install GUI components\npip install -e \".[gui]\"\n</code></pre></p>"},{"location":"INSTALLATION/#problem-redis-connection-error","title":"Problem: Redis connection error","text":"<p>Symptom: <pre><code>redis.exceptions.ConnectionError: Error connecting to Redis\n</code></pre></p> <p>Solution: <pre><code># Start Redis with Docker\ndocker run -p 6379:6379 redis:alpine\n\n# Or install and start Redis locally\n# Ubuntu/Debian:\nsudo apt-get install redis-server\nsudo systemctl start redis\n\n# macOS:\nbrew install redis\nbrew services start redis\n\n# Test connection\nredis-cli ping\n# Should return: PONG\n</code></pre></p>"},{"location":"INSTALLATION/#problem-api-key-not-recognized","title":"Problem: API key not recognized","text":"<p>Symptom: <pre><code>Error: No valid API key found\n</code></pre></p> <p>Solution: <pre><code># Check if secrets.env exists\nls -la secrets.env\n\n# Check file content (without revealing keys)\ncat secrets.env | grep -o \".*_API_KEY=.*\" | sed 's/=.*/=***/'\n\n# Make sure the file is in the project root\npwd  # Should show robot_mcp directory\n\n# Check environment variables are loaded\npython -c \"from dotenv import load_dotenv; load_dotenv('secrets.env'); import os; print('GROQ' in os.environ)\"\n# Should print: True (if Groq key is set)\n</code></pre></p>"},{"location":"INSTALLATION/#platform-specific-notes","title":"\ud83d\udcbb Platform-Specific Notes","text":""},{"location":"INSTALLATION/#windows","title":"Windows","text":"<pre><code># Use PowerShell\n# Activate virtual environment:\nvenv\\Scripts\\Activate.ps1\n\n# If execution policy blocks:\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n\n# Then activate again\nvenv\\Scripts\\Activate.ps1\n\n# Install package\npip install -e \".[all]\"\n</code></pre>"},{"location":"INSTALLATION/#macos-m1m2-apple-silicon","title":"macOS (M1/M2 Apple Silicon)","text":"<pre><code># ARM64 architecture\n# Use miniforge for better compatibility\nbrew install miniforge\nconda init zsh  # or bash\n\n# Create new environment\nconda create -n robot-mcp python=3.10\nconda activate robot-mcp\n\n# Install dependencies\npip install -e \".[all]\"\n\n# Note: Some packages may need ARM64 builds\n# If issues occur, use Rosetta mode:\narch -x86_64 pip install -e \".[all]\"\n</code></pre>"},{"location":"INSTALLATION/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Install system dependencies\nsudo apt-get update\nsudo apt-get install -y \\\n    python3-dev \\\n    python3-pip \\\n    python3-venv \\\n    portaudio19-dev \\\n    libopencv-dev \\\n    build-essential \\\n    redis-server\n\n# Start Redis\nsudo systemctl start redis\nsudo systemctl enable redis\n\n# Normal installation\npython3 -m venv venv\nsource venv/bin/activate\npip install -e \".[all]\"\n</code></pre>"},{"location":"INSTALLATION/#update-uninstallation","title":"\ud83d\udcc4 Update &amp; Uninstallation","text":""},{"location":"INSTALLATION/#update-package","title":"Update Package","text":"<pre><code># For development version (editable install)\ncd robot_mcp\ngit pull origin master\npip install -e \".[all]\" --upgrade\n\n# Update only dependencies\npip install -e \".[all]\" --upgrade --force-reinstall --no-cache-dir\n</code></pre>"},{"location":"INSTALLATION/#uninstall-package","title":"Uninstall Package","text":"<pre><code># Remove package\npip uninstall robot-mcp\n\n# Remove virtual environment\ndeactivate\nrm -rf venv/\n\n# Remove all cache files\nfind . -type d -name \"__pycache__\" -exec rm -rf {} +\nfind . -type f -name \"*.pyc\" -delete\nfind . -type d -name \"*.egg-info\" -exec rm -rf {} +\n</code></pre>"},{"location":"INSTALLATION/#testing-the-installation","title":"\ud83e\uddea Testing the Installation","text":""},{"location":"INSTALLATION/#run-test-suite","title":"Run Test Suite","text":"<pre><code># All tests\npytest\n\n# Only unit tests\npytest -m unit\n\n# With coverage report\npytest --cov=client --cov=server --cov=robot_gui\n\n# Verbose output\npytest -v -s\n</code></pre>"},{"location":"INSTALLATION/#quick-functionality-test","title":"Quick Functionality Test","text":"<pre><code># 1. Start Redis\ndocker run -p 6379:6379 redis:alpine &amp;\n\n# 2. Start MCP Server\npython server/fastmcp_robot_server.py --robot niryo &amp;\nsleep 5\n\n# 3. Test universal client with auto-detection\npython client/fastmcp_universal_client.py --command \"test connection\"\n\n# 4. Test specific provider\npython client/fastmcp_universal_client.py --api groq --command \"What API am I using?\"\n\n# 5. Cleanup\npkill -f fastmcp_robot_server\npkill -f redis-server\n</code></pre>"},{"location":"INSTALLATION/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>After successful installation:</p> <ol> <li>Configure API Keys: Complete <code>secrets.env</code> with at least one API key</li> <li>Quick Start: See <code>docs/mcp_setup_guide.md</code> for usage examples</li> <li>Examples: Try <code>python examples/universal_examples.py workspace_scan</code></li> <li>GUI: Launch <code>python robot_gui/mcp_app.py --robot niryo</code></li> <li>Documentation: Read <code>README.md</code> for system overview</li> </ol>"},{"location":"INSTALLATION/#recommended-setup-for-beginners","title":"\ud83c\udfaf Recommended Setup for Beginners","text":"<p>Fastest way to get started:</p> <pre><code># 1. Get a free Groq API key (fastest, no credit card)\n# Visit: https://console.groq.com/keys\n\n# 2. Install with all components\npip install -e \".[all]\"\n\n# 3. Add Groq key to secrets.env\necho \"GROQ_API_KEY=gsk_your_key_here\" &gt; secrets.env\n\n# 4. Start Redis\ndocker run -p 6379:6379 redis:alpine &amp;\n\n# 5. Start server\npython server/fastmcp_robot_server.py --robot niryo &amp;\n\n# 6. Try the universal client (will auto-detect Groq)\npython client/fastmcp_universal_client.py\n\n# 7. Type: \"What objects do you see?\"\n</code></pre>"},{"location":"INSTALLATION/#getting-help","title":"\ud83c\udd98 Getting Help","text":"<p>If problems persist:</p> <ol> <li>Check Logs: <code>tail -f log/mcp_server_*.log</code></li> <li>GitHub Issues: https://github.com/dgaida/robot-mcp/issues</li> <li>Documentation: See <code>docs/</code> directory for detailed guides</li> <li>Community: Ask in GitHub Discussions</li> </ol>"},{"location":"INSTALLATION/#installation-checklist","title":"\ud83d\udcdd Installation Checklist","text":"<p>Use this checklist to verify complete installation:</p> <ul> <li>[ ] Python 3.8+ installed</li> <li>[ ] Virtual environment created and activated</li> <li>[ ] Package installed: <code>pip show robot-mcp</code> works</li> <li>[ ] At least one API key configured in <code>secrets.env</code></li> <li>[ ] Redis server running: <code>redis-cli ping</code> returns PONG</li> <li>[ ] MCP server starts: <code>python server/fastmcp_robot_server.py</code></li> <li>[ ] Universal client works: <code>python client/fastmcp_universal_client.py --command \"test\"</code></li> <li>[ ] LLM provider detected: Check client output shows API name</li> <li>[ ] Command-line scripts available: <code>robot-universal-client --help</code></li> </ul>"},{"location":"INSTALLATION/#troubleshooting-quick-reference","title":"\ud83d\udd0d Troubleshooting Quick Reference","text":"Issue Quick Fix ModuleNotFoundError <code>pip install -e \".[all]\" --force-reinstall</code> Redis connection error <code>docker run -p 6379:6379 redis:alpine</code> No API key found Check <code>secrets.env</code> exists and has valid keys Server won't start Check port 8000 is free: <code>lsof -i :8000</code> Ollama not responding <code>ollama serve</code> then <code>ollama pull llama3.2:1b</code> Import errors Ensure virtual environment is activated Permission denied <code>chmod 600 secrets.env</code> <p>Installation complete? \u2192 Start with <code>docs/mcp_setup_guide.md</code>! \ud83d\ude80</p>"},{"location":"api_reference_auto/","title":"Robot MCP - Auto-Generated API Reference","text":"<p>Generated: 2026-01-31 16:21:13 Source: <code>server/fastmcp_robot_server_unified.py</code> Total Tools: 19</p> <p>\u26a0\ufe0f Note: This documentation is auto-generated from source code. Do not edit manually. Run <code>python docs/generate_api_docs.py</code> to update.</p>"},{"location":"api_reference_auto/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Quick Reference</li> <li>API Tools</li> <li>Feedback (2)</li> <li>Object Detection (6)</li> <li>Robot Control (8)</li> <li>Workspace (3)</li> </ul>"},{"location":"api_reference_auto/#overview","title":"Overview","text":"<p>The Robot MCP system provides 19 tools organized into 4 categories:</p> Category Tools Description Feedback 2 User feedback via speech and text Object Detection 6 Vision-based object recognition and querying Robot Control 8 Physical robot manipulation and movement Workspace 3 Workspace configuration and coordinate queries"},{"location":"api_reference_auto/#tool-categories","title":"Tool Categories","text":"<p>Feedback tools provide user communication: - Text-to-speech output - Task tracking for video overlays - Status updates</p> <p>Object Detection tools use computer vision: - Real-time object detection via camera - Spatial filtering (left/right/above/below) - Size-based sorting and querying - Label-based filtering</p> <p>Robot Control tools provide physical manipulation capabilities: - Pick and place operations - Pushing large objects - Precise movement control - Calibration and safety</p> <p>Workspace tools manage coordinate systems: - Workspace boundary queries - Free space detection - Object label management - Coordinate transformations</p>"},{"location":"api_reference_auto/#using-the-api","title":"Using the API","text":"<p>All tools are exposed via FastMCP and can be called through:</p> <ol> <li>Universal Client - Multi-LLM support (OpenAI, Groq, Gemini, Ollama)</li> <li>Direct MCP Client - Low-level FastMCP protocol</li> <li>Web GUI - Gradio interface with voice input</li> <li>REST API - HTTP endpoints (if enabled)</li> </ol> <p>See Setup Guide for usage instructions.</p>"},{"location":"api_reference_auto/#quick-reference","title":"Quick Reference","text":""},{"location":"api_reference_auto/#all-tools-at-a-glance","title":"All Tools at a Glance","text":"Tool Category Input Validation Description <code>set_user_task</code> Feedback \u2014 Set the current user task for display in video recordings. <code>speak</code> Feedback \u2014 Make the robot speak a message using text-to-speech. <code>get_detected_object</code> Object Detection \u2014 Retrieve a detected object at or near a specified world coor... <code>get_detected_objects</code> Object Detection \u2713 Get list of all objects detected by the camera in the worksp... <code>get_detected_objects_sorted</code> Object Detection \u2014 Get detected objects sorted by size (area in square meters). <code>get_largest_detected_object</code> Object Detection \u2014 Return the largest detected object based on its area in squa... <code>get_largest_free_space_with_center</code> Object Detection \u2014 Determine the largest free space in the workspace and its ce... <code>get_smallest_detected_object</code> Object Detection \u2014 Return the smallest detected object based on its area in squ... <code>calibrate</code> Robot Control \u2014 Calibrate the robot's joints for accurate movement. <code>clear_collision_detected</code> Robot Control \u2014 Reset the internal collision detection flag of the Niryo rob... <code>move2by</code> Robot Control \u2014 Pick an object and move it a specified distance in a given d... <code>move2observation_pose</code> Robot Control \u2014 Move robot to observation position above the specified works... <code>pick_object</code> Robot Control \u2713 Pick up a specific object using the robot gripper. <code>pick_place_object</code> Robot Control \u2713 Pick an object and place it at a target location in a single... <code>place_object</code> Robot Control \u2713 Place a previously picked object at the specified location. <code>push_object</code> Robot Control \u2713 Push a specific object to a new position using the robot gri... <code>add_object_name2object_labels</code> Workspace \u2014 Add a new object type to the list of recognizable objects. <code>get_object_labels_as_string</code> Workspace \u2014 Return all object labels that the detection model can recogn... <code>get_workspace_coordinate_from_point</code> Workspace \u2713 Get the world coordinate of a special point in the workspace..."},{"location":"api_reference_auto/#api-tools","title":"API Tools","text":""},{"location":"api_reference_auto/#tools-feedback","title":"Tools: Feedback","text":""},{"location":"api_reference_auto/#set_user_task","title":"set_user_task","text":"<p>Category: Feedback  </p> <p>Set the current user task for display in video recordings.</p> <p>Signature: <pre><code>def set_user_task(task: str) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>task</code> (<code>str</code>): The user's natural language task or command.</li> </ul> <p>Returns: <code>str - str: Confirmation message that the task has been set.</code></p> <p>Examples:</p> <pre><code>set_user_task(\"Pick up the red cube\")\n    # Video overlay shows: \"Task: Pick up the red cube\"\n\n    set_user_task(\"Organize workspace by color\")\n    # Video overlay updates to show new task\n\n    set_user_task(\"Sort objects by size\")\n    # Task text changes in video feed\n</code></pre>"},{"location":"api_reference_auto/#speak","title":"speak","text":"<p>Category: Feedback  </p> <p>Make the robot speak a message using text-to-speech.</p> <p>Signature: <pre><code>def speak(text: str) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>text</code> (<code>str</code>): The message to speak. Must be a non-empty string.</li> </ul> <p>Returns: <code>str - str: Confirmation message with the text being spoken, or error if failed.</code></p> <p>Examples:</p> <pre><code>speak(\"I have picked up the pencil\")\n    # Robot says: \"I have picked up the pencil\"\n\n    speak(\"Task completed successfully\")\n    # Robot announces task completion\n\n    speak(\"Warning: Object detected in workspace\")\n    # Robot provides audio warning\n</code></pre>"},{"location":"api_reference_auto/#tools-object-detection","title":"Tools: Object Detection","text":""},{"location":"api_reference_auto/#get_detected_object","title":"get_detected_object","text":"<p>Category: Object Detection  </p> <p>Retrieve a detected object at or near a specified world coordinate.</p> <p>Signature: <pre><code>def get_detected_object(coordinate: List[float], label: Optional[str] = None) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>coordinate</code> (<code>List[float]</code>): A 2D coordinate in the world coordinate system [x, y].</p> </li> <li> <p><code>label</code> (<code>Optional[str]</code>, default: <code>None</code>)</p> </li> </ul> <p>Returns: <code>str - Object at [0.180, -0.050] (any object type)</code></p> <p>Examples:</p> <pre><code>get_detected_object([0.18, -0.05])\n    # Returns: Object at [0.180, -0.050] (any object type)\n\n    get_detected_object([0.18, -0.05], label=\"pen\")\n    # Returns: Only a \"pen\" object at that location\n\n    get_detected_object([0.25, 0.0])\n    # Returns: Object near workspace center\n</code></pre>"},{"location":"api_reference_auto/#get_detected_objects","title":"get_detected_objects","text":"<p>Category: Object Detection Validation: <code>GetDetectedObjectsInput</code> </p> <p>Get list of all objects detected by the camera in the workspace.</p> <p>Signature: <pre><code>def get_detected_objects(location: Union[Location, str] = Location.NONE, coordinate: Optional[List[float]] = None, label: Optional[str] = None) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>location</code> (<code>Union[Location, str]</code>, default: <code>Location.NONE</code>): Spatial filter relative to coordinate. Options:</p> </li> <li> <p><code>coordinate</code> (<code>Optional[List[float]]</code>, default: <code>None</code>): Reference coordinate [x, y] in meters.</p> </li> <li> <p><code>label</code> (<code>Optional[str]</code>, default: <code>None</code>)</p> </li> </ul> <p>Returns: <code>str - All detected objects in workspace</code></p> <p>Examples:</p> <pre><code>get_detected_objects()\n    # Returns: All detected objects in workspace\n\n    get_detected_objects(location=\"close to\", coordinate=[0.2, 0.0])\n    # Returns: Objects within 2cm of [0.2, 0.0]\n\n    get_detected_objects(label=\"pencil\")\n    # Returns: All objects labeled \"pencil\"\n\n    get_detected_objects(location=\"left next to\", coordinate=[0.20, 0.0], label=\"cube\")\n    # Returns: Cubes to the left of [0.20, 0.0]\n</code></pre>"},{"location":"api_reference_auto/#get_detected_objects_sorted","title":"get_detected_objects_sorted","text":"<p>Category: Object Detection  </p> <p>Get detected objects sorted by size (area in square meters).</p> <p>Signature: <pre><code>def get_detected_objects_sorted(ascending: bool = True) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>ascending</code> (<code>bool</code>, default: <code>True</code>): If True, sort smallest to largest. If False, sort</li> </ul> <p>Returns: <code>str - [smallest object, medium object, largest object]</code></p> <p>Examples:</p> <pre><code>get_detected_objects_sorted(ascending=True)\n    # Returns: [smallest object, medium object, largest object]\n\n    get_detected_objects_sorted(ascending=False)\n    # Returns: [largest object, medium object, smallest object]\n\n    get_detected_objects_sorted()\n    # Returns: Objects sorted smallest to largest (default)\n</code></pre>"},{"location":"api_reference_auto/#get_largest_detected_object","title":"get_largest_detected_object","text":"<p>Category: Object Detection  </p> <p>Return the largest detected object based on its area in square meters.</p> <p>Signature: <pre><code>def get_largest_detected_object() -&gt; str\n</code></pre></p> <p>Returns: <code>str - {\"label\": \"blue square\", \"position\": {...}, ...}</code></p> <p>Examples:</p> <pre><code>get_largest_detected_object()\n    # Returns: {\"label\": \"blue square\", \"position\": {...}, ...}\n\n    # Use result for manipulation:\n    # 1. largest = get_largest_detected_object()\n    # 2. Parse JSON to get position\n    # 3. pick_object(largest[\"label\"], ...)\n</code></pre>"},{"location":"api_reference_auto/#get_largest_free_space_with_center","title":"get_largest_free_space_with_center","text":"<p>Category: Object Detection  </p> <p>Determine the largest free space in the workspace and its center coordinate.</p> <p>Signature: <pre><code>def get_largest_free_space_with_center() -&gt; str\n</code></pre></p> <p>Returns: <code>str - \"0.0045 m\u00b2 at center [0.240, -0.030]\"</code></p> <p>Examples:</p> <pre><code>get_largest_free_space_with_center()\n    # Returns: \"0.0045 m\u00b2 at center [0.240, -0.030]\"\n\n    # Use for safe placement:\n    # 1. result = get_largest_free_space_with_center()\n    # 2. Parse to get center coordinates\n    # 3. place_object(center_coords, None)\n\n    # area, x, y = parse_result()\n    # pick_place_object(\"cube\", [0.2, 0.1], [x, y], None)\n</code></pre>"},{"location":"api_reference_auto/#get_smallest_detected_object","title":"get_smallest_detected_object","text":"<p>Category: Object Detection  </p> <p>Return the smallest detected object based on its area in square meters.</p> <p>Signature: <pre><code>def get_smallest_detected_object() -&gt; str\n</code></pre></p> <p>Returns: <code>str - {\"label\": \"pen\", \"position\": {...}, ...}</code></p> <p>Examples:</p> <pre><code>get_smallest_detected_object()\n    # Returns: {\"label\": \"pen\", \"position\": {...}, ...}\n\n    # Use for precise picking:\n    # 1. smallest = get_smallest_detected_object()\n    # 2. Extract coordinates\n    # 3. pick_object(smallest[\"label\"], ...)\n</code></pre>"},{"location":"api_reference_auto/#tools-robot-control","title":"Tools: Robot Control","text":""},{"location":"api_reference_auto/#calibrate","title":"calibrate","text":"<p>Category: Robot Control  </p> <p>Calibrate the robot's joints for accurate movement.</p> <p>Signature: <pre><code>def calibrate() -&gt; str\n</code></pre></p> <p>Returns: <code>str - \"\u2713 Robot calibration completed successfully\"</code></p> <p>Examples:</p> <pre><code>calibrate()\n    # Robot moves through calibration sequence\n    # Returns: \"\u2713 Robot calibration completed successfully\"\n\n    # Typical usage:\n    # 1. Power on robot\n    # 2. calibrate()\n    # 3. Proceed with normal operations\n\n    # After collision:\n    # 1. clear_collision_detected()\n    # 2. calibrate()\n    # 3. Resume tasks\n</code></pre>"},{"location":"api_reference_auto/#clear_collision_detected","title":"clear_collision_detected","text":"<p>Category: Robot Control  </p> <p>Reset the internal collision detection flag of the Niryo robot.</p> <p>Signature: <pre><code>def clear_collision_detected() -&gt; str\n</code></pre></p> <p>Returns: <code>str - \"\u2713 Collision detection flag cleared\"</code></p> <p>Examples:</p> <pre><code>clear_collision_detected()\n    # Clears collision flag\n    # Returns: \"\u2713 Collision detection flag cleared\"\n\n    # Recovery workflow:\n    # 1. Collision occurs\n    # 2. clear_collision_detected()\n    # 3. calibrate()  # Optional\n    # 4. Resume operations\n\n    # After unexpected stop:\n    # clear_collision_detected()\n    # move2observation_pose(\"niryo_ws\")\n</code></pre> <p>Notes:</p> <ul> <li>Note: This is a Niryo-specific function. May not work with other robot types.</li> </ul>"},{"location":"api_reference_auto/#move2by","title":"move2by","text":"<p>Category: Robot Control  </p> <p>Pick an object and move it a specified distance in a given direction.</p> <p>Signature: <pre><code>def move2by(object_name: str, pick_coordinate: List[float], direction: str, distance: float, z_offset: float = 0.001) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>object_name</code> (<code>str</code>): Name of object to move. Must match detection exactly.</p> </li> <li> <p><code>pick_coordinate</code> (<code>List[float]</code>): Current world coordinates [x, y] in meters</p> </li> <li> <p><code>direction</code> (<code>str</code>)</p> </li> <li> <p><code>distance</code> (<code>float</code>)</p> </li> <li> <p><code>z_offset</code> (<code>float</code>, default: <code>0.001</code>)</p> </li> </ul> <p>Returns: <code>str - str: Success message with pick and place coordinates, or error description.</code></p> <p>Examples:</p> <pre><code>move2by(\"pencil\", [-0.11, 0.21], \"left\", 0.02)\n    # Picks pencil at [-0.11, 0.21] and moves it 2cm left to [-0.11, 0.23]\n\n    move2by(\"cube\", [-0.11, 0.21], \"up\", 0.03)\n    # Moves cube 3cm upward (increases X coordinate)\n\n    move2by(\"pen\", [0.15, -0.05], \"right\", 0.04, z_offset=0.02)\n    # Picks pen with 2cm z-offset and moves it 4cm right\n\n    # Direction mapping:\n    # \"left\"  \u2192 increases Y (toward positive Y)\n    # \"right\" \u2192 decreases Y (toward negative Y)\n    # \"up\"    \u2192 increases X (away from robot)\n    # \"down\"  \u2192 decreases X (toward robot)\n</code></pre>"},{"location":"api_reference_auto/#move2observation_pose","title":"move2observation_pose","text":"<p>Category: Robot Control  </p> <p>Move robot to observation position above the specified workspace.</p> <p>Signature: <pre><code>def move2observation_pose(workspace_id: str) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>workspace_id</code> (<code>str</code>): ID of the workspace (e.g., \"niryo_ws\", \"gazebo_1\").</li> </ul> <p>Returns: <code>str - str: Success message confirming movement to observation pose, or error description if movement failed.</code></p> <p>Examples:</p> <pre><code>move2observation_pose(\"niryo_ws\")\n    # Moves to observation pose above niryo_ws workspace\n\n    move2observation_pose(\"gazebo_1\")\n    # Moves to observation pose for simulation workspace\n\n    # Typical workflow:\n    # 1. move2observation_pose(\"niryo_ws\")\n    # 2. get_detected_objects()\n    # 3. pick_place_object(...)\n    # 4. move2observation_pose(\"niryo_ws\")  # Return to home\n</code></pre> <p>Notes:</p> <ul> <li>Note: The robot automatically moves to observation pose before detection operations, but you may need to call this explicitly for other tasks.</li> </ul>"},{"location":"api_reference_auto/#pick_object","title":"pick_object","text":"<p>Category: Robot Control Validation: <code>PickObjectInput</code> </p> <p>Pick up a specific object using the robot gripper.</p> <p>Signature: <pre><code>def pick_object(object_name: str, pick_coordinate: List[float], z_offset: float = 0.001) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>object_name</code> (<code>str</code>): Name of the object to pick. Ensure this name matches</p> </li> <li> <p><code>pick_coordinate</code> (<code>List[float]</code>)</p> </li> <li> <p><code>z_offset</code> (<code>float</code>, default: <code>0.001</code>)</p> </li> </ul> <p>Returns: <code>str - str: Success message with coordinates, or error description if failed.</code></p> <p>Examples:</p> <pre><code>pick_object(\"pen\", [0.01, -0.15])\n    # Picks pen at world coordinates [0.01, -0.15]\n\n    pick_object(\"cube\", [0.20, 0.05])\n    # Picks cube with default z_offset\n\n    pick_object(\"pen\", [0.01, -0.15], z_offset=0.02)\n    # Picks pen with 2cm offset above detected position\n    # Useful if pen is on top of another object\n</code></pre> <p>Notes:</p> <ul> <li>Note: Must be followed by place_object() to complete pick-and-place operation. For complete operation in one call, use pick_place_object() instead.</li> </ul>"},{"location":"api_reference_auto/#pick_place_object","title":"pick_place_object","text":"<p>Category: Robot Control Validation: <code>PickPlaceInput</code> </p> <p>Pick an object and place it at a target location in a single operation.</p> <p>Signature: <pre><code>def pick_place_object(object_name: str, pick_coordinate: List[float], place_coordinate: List[float], location: Union[Location, str, None] = None, z_offset: float = 0.001) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>object_name</code> (<code>str</code>): Name of object to pick. Must match detection exactly.</p> </li> <li> <p><code>pick_coordinate</code> (<code>List[float]</code>): World coordinates [x, y] in meters where</p> </li> <li> <p><code>place_coordinate</code> (<code>List[float]</code>)</p> </li> <li> <p><code>location</code> (<code>Union[Location, str, None]</code>, default: <code>None</code>)</p> </li> <li> <p><code>z_offset</code> (<code>float</code>, default: <code>0.001</code>)</p> </li> </ul> <p>Returns: <code>str - str: Success message with pick and place coordinates, or error description.</code></p> <p>Examples:</p> <pre><code>pick_place_object(\"chocolate bar\", [-0.1, 0.01], [0.1, 0.11], \"right next to\")\n    # Picks chocolate bar and places it right next to object at [0.1, 0.11]\n\n    pick_place_object(\"cube\", [0.2, 0.05], [0.3, 0.1], \"on top of\", z_offset=0.02)\n    # Picks cube with 2cm z-offset and places it on top of target object\n\n    pick_place_object(\"pen\", [0.15, -0.05], [0.25, 0.0], None)\n    # Picks pen and places at exact coordinates (no relative positioning)\n</code></pre> <p>Notes:</p> <ul> <li>Note: Always call get_detected_objects() first to get current object positions. Object names are case-sensitive and must match detection exactly.</li> </ul>"},{"location":"api_reference_auto/#place_object","title":"place_object","text":"<p>Category: Robot Control Validation: <code>PlaceObjectInput</code> </p> <p>Place a previously picked object at the specified location.</p> <p>Signature: <pre><code>def place_object(place_coordinate: List[float], location: Union[Location, str, None] = None) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>place_coordinate</code> (<code>List[float]</code>): Target coordinates [x, y] in meters where</p> </li> <li> <p><code>location</code> (<code>Union[Location, str, None]</code>, default: <code>None</code>)</p> </li> </ul> <p>Returns: <code>str - str: Success message with placement coordinates and location, or error description if placement failed.</code></p> <p>Examples:</p> <pre><code>place_object([0.2, 0.0], \"left next to\")\n    # Places gripped object to the left of coordinate [0.2, 0.0]\n\n    place_object([0.25, 0.05], \"on top of\")\n    # Stacks gripped object on top of object at [0.25, 0.05]\n\n    place_object([0.18, -0.10], None)\n    # Places object at exact coordinates [0.18, -0.10]\n\n    # Complete workflow:\n    # pick_object(\"cube\", [0.15, -0.05])\n    # place_object([0.20, 0.10], \"right next to\")\n</code></pre> <p>Notes:</p> <ul> <li>Note: Must call pick_object() before calling this function. For complete pick-and-place in one operation, use pick_place_object() instead.</li> </ul>"},{"location":"api_reference_auto/#push_object","title":"push_object","text":"<p>Category: Robot Control Validation: <code>PushObjectInput</code> </p> <p>Push a specific object to a new position using the robot gripper.</p> <p>Signature: <pre><code>def push_object(object_name: str, push_coordinate: List[float], direction: str, distance: float) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>object_name</code> (<code>str</code>): Name of the object to push. Ensure the name matches</p> </li> <li> <p><code>push_coordinate</code> (<code>List[float]</code>)</p> </li> <li> <p><code>direction</code> (<code>str</code>)</p> </li> <li> <p><code>distance</code> (<code>float</code>)</p> </li> </ul> <p>Returns: <code>str - str: Success message with object name, starting position, direction, and distance, or error description if push operation failed.</code></p> <p>Examples:</p> <pre><code>push_object(\"large box\", [0.25, 0.05], \"right\", 50.0)\n    # Pushes large box 50mm (5cm) to the right from its current position\n\n    push_object(\"book\", [0.20, -0.03], \"up\", 30.0)\n    # Pushes book 30mm upward (away from robot, increases X)\n\n    push_object(\"tray\", [0.18, 0.08], \"left\", 40.0)\n    # Pushes tray 40mm to the left (increases Y)\n\n    # Direction mapping:\n    # \"up\"    \u2192 pushes away from robot (increases X coordinate)\n    # \"down\"  \u2192 pushes toward robot (decreases X coordinate)\n    # \"left\"  \u2192 pushes left (increases Y coordinate)\n    # \"right\" \u2192 pushes right (decreases Y coordinate)\n</code></pre> <p>Notes:</p> <ul> <li>Note: This is in millimeters, not meters (50.0 = 5cm).</li> <li>Note: Use push_object() instead of pick_place_object() when object width exceeds gripper capacity (~5cm). The robot approaches from the opposite side of the push direction to avoid collisions.</li> </ul>"},{"location":"api_reference_auto/#tools-workspace","title":"Tools: Workspace","text":""},{"location":"api_reference_auto/#add_object_name2object_labels","title":"add_object_name2object_labels","text":"<p>Category: Workspace  </p> <p>Add a new object type to the list of recognizable objects.</p> <p>Signature: <pre><code>def add_object_name2object_labels(object_name: str) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>object_name</code> (<code>str</code>): Name of the new object to recognize. Should be descriptive</li> </ul> <p>Returns: <code>str - \"\u2713 Added 'screwdriver' to the list of recognizable objects\" # Now the robot will look for screwdrivers in the workspace</code></p> <p>Examples:</p> <pre><code>add_object_name2object_labels(\"screwdriver\")\n    # Returns: \"\u2713 Added 'screwdriver' to the list of recognizable objects\"\n    # Now the robot will look for screwdrivers in the workspace\n\n    add_object_name2object_labels(\"red ball\")\n    # Adds \"red ball\" to detection labels\n\n    add_object_name2object_labels(\"smartphone\")\n    # Vision system will now attempt to detect smartphones\n\n    # Workflow:\n    # 1. add_object_name2object_labels(\"wrench\")\n    # 2. get_object_labels_as_string()  # Verify \"wrench\" is included\n    # 3. get_detected_objects()  # System will now detect wrenches\n</code></pre> <p>Notes:</p> <ul> <li>Note: The vision model's ability to detect the new object depends on its training. Well-known objects (tools, office supplies) are more likely to be detected.</li> </ul>"},{"location":"api_reference_auto/#get_object_labels_as_string","title":"get_object_labels_as_string","text":"<p>Category: Workspace  </p> <p>Return all object labels that the detection model can recognize.</p> <p>Signature: <pre><code>def get_object_labels_as_string() -&gt; str\n</code></pre></p> <p>Returns: <code>str - \"pencil, pen, cube, cylinder, chocolate bar, cigarette, ...\"</code></p> <p>Examples:</p> <pre><code>get_object_labels_as_string()\n    # Returns: \"pencil, pen, cube, cylinder, chocolate bar, cigarette, ...\"\n\n    # List available objects to user:\n    # labels = get_object_labels_as_string()\n    # print(f\"I can detect: {labels}\")\n</code></pre> <p>Notes:</p> <ul> <li>Note: Call this method when the user asks \"What objects can you see?\" or \"What can you pick up?\" to show detection capabilities.</li> </ul>"},{"location":"api_reference_auto/#get_workspace_coordinate_from_point","title":"get_workspace_coordinate_from_point","text":"<p>Category: Workspace Validation: <code>WorkspacePointInput</code> </p> <p>Get the world coordinate of a special point in the workspace.</p> <p>Signature: <pre><code>def get_workspace_coordinate_from_point(workspace_id: str, point: str) -&gt; str\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>workspace_id</code> (<code>str</code>): ID of the workspace (e.g., \"niryo_ws\", \"gazebo_1\").</p> </li> <li> <p><code>point</code> (<code>str</code>)</p> </li> </ul> <p>Returns: <code>str - \"\u2713 Coordinate of 'upper left corner': [0.337, 0.087]\"</code></p> <p>Examples:</p> <pre><code>get_workspace_coordinate_from_point(\"niryo_ws\", \"upper left corner\")\n    # Returns: \"\u2713 Coordinate of 'upper left corner': [0.337, 0.087]\"\n\n    get_workspace_coordinate_from_point(\"niryo_ws\", \"center point\")\n    # Returns: \"\u2713 Coordinate of 'center point': [0.250, 0.000]\"\n\n    get_workspace_coordinate_from_point(\"niryo_ws\", \"lower right corner\")\n    # Returns: \"\u2713 Coordinate of 'lower right corner': [0.163, -0.087]\"\n\n    # Use for boundary placement:\n    # upper_left = get_workspace_coordinate_from_point(\"niryo_ws\", \"upper left corner\")\n    # pick_place_object(\"cube\", [0.2, 0.1], upper_left, None)\n\n    # Organize in corners:\n    # place_object(get_workspace_coordinate_from_point(\"niryo_ws\", \"upper right corner\"))\n</code></pre> <p>Notes:</p> <ul> <li>Note: For Niryo workspace: - Upper left: [0.337, 0.087] (far and left) - Lower right: [0.163, -0.087] (close and right) - Center: ~[0.250, 0.000]</li> </ul>"},{"location":"api_reference_auto/#additional-resources","title":"Additional Resources","text":"<ul> <li>Setup Guide - Installation and configuration</li> <li>Examples - Common use cases and workflows</li> <li>Architecture - System design and data flow</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"api_reference_auto/#contributing","title":"Contributing","text":"<p>To update this documentation:</p> <ol> <li>Modify tool docstrings in <code>server/fastmcp_robot_server_unified.py</code></li> <li>Run: <code>python docs/generate_api_docs.py</code></li> <li>Commit both source and generated docs</li> </ol>"},{"location":"api_reference_auto/#validation","title":"Validation","text":"<p>All tools with <code>@validate_input</code> decorator use Pydantic models for input validation. See <code>server/schemas.py</code> for validation model definitions.</p> <p>Auto-generated by: <code>docs/generate_api_docs.py</code> Last updated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>"},{"location":"communication_readme/","title":"Communicative Robot MCP System","text":"<p>This enhanced version of the robot MCP system adds LLM-generated explanations before every tool execution, making the robot more transparent, user-friendly, and educational.</p>"},{"location":"communication_readme/#features","title":"\ud83c\udfaf Features","text":""},{"location":"communication_readme/#whats-new","title":"\u2705 What's New","text":"<ul> <li>LLM-Generated Explanations: The robot explains what it's about to do before each action</li> <li>Intelligent Voice Output: Only speaks for important operations (configurable)</li> <li>Comprehensive Logging: All explanations are logged for debugging and analysis</li> <li>Configurable Priority System: Control which operations get voice output</li> <li>Multiple LLM Support: Works with OpenAI, Groq, Gemini, or Ollama</li> <li>Fallback System: Graceful degradation when LLM is unavailable</li> <li>Personality Customization: Adjust tone, verbosity, and emoji usage</li> </ul>"},{"location":"communication_readme/#example-interaction","title":"\ud83e\udd16 Example Interaction","text":"<p>Without Explanations: <pre><code>[Robot silently picks up object]\n[Robot silently places object]\n</code></pre></p> <p>With Explanations: <pre><code>\ud83e\udd16 \"I'm moving to observe the workspace so I can see all the objects clearly.\"\n[Robot moves to observation pose]\n\n\ud83d\udd0d \"Let me scan the workspace and identify what's available.\"\n[Robot detects objects]\n\n\ud83e\udd16 \"I'll pick up the pencil and place it right next to the red cube.\"\n[Robot executes pick-and-place]\n\n\u2713 \"Done! The pencil is now positioned next to the red cube.\"\n</code></pre></p>"},{"location":"communication_readme/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"communication_readme/#1-installation","title":"1. Installation","text":"<p>The communicative system requires <code>llm_client</code> for generating explanations:</p> <pre><code># Ensure llm_client is installed\ncd ../llm_client\npip install -e .\n\n# Or install from GitHub\npip install git+https://github.com/dgaida/llm_client.git\n</code></pre>"},{"location":"communication_readme/#2-configure-api-keys","title":"2. Configure API Keys","text":"<p>Add your preferred LLM provider key to <code>secrets.env</code>:</p> <pre><code># For fast, free explanations (recommended)\nGROQ_API_KEY=gsk-xxxxxxxx\n\n# Or use OpenAI\nOPENAI_API_KEY=sk-xxxxxxxx\n\n# Or use Gemini\nGEMINI_API_KEY=AIzaSy-xxxxxxxx\n\n# Ollama requires no API key (runs locally)\n</code></pre>"},{"location":"communication_readme/#3-start-the-communicative-server","title":"3. Start the Communicative Server","text":"<pre><code># Start with default settings (Groq)\npython server/fastmcp_robot_server_communicative.py --robot niryo\n\n# Use OpenAI for explanations\npython server/fastmcp_robot_server_communicative.py --robot niryo --explanation-api openai\n\n# Disable explanations (revert to silent mode)\npython server/fastmcp_robot_server_communicative.py --robot niryo --no-explanations\n\n# Use simulation mode\npython server/fastmcp_robot_server_communicative.py --robot niryo --no-simulation\n</code></pre>"},{"location":"communication_readme/#4-use-with-mcp-client","title":"4. Use with MCP Client","text":"<pre><code># In another terminal\npython client/fastmcp_universal_client.py\n</code></pre> <p>The client will now receive spoken explanations for important operations!</p>"},{"location":"communication_readme/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"communication_readme/#priority-levels","title":"Priority Levels","text":"<p>Control when the robot speaks using <code>config/explanation_config.yaml</code>:</p> <pre><code>voice:\n  priority:\n    pick_place_object: high    # Always speak\n    move2observation_pose: medium  # Speak 50% of time\n    get_detected_object: low    # Speak 10% of time\n    speak: never                # Never announce\n</code></pre> <p>Priority Levels: - <code>high</code>: Always use voice (100%) - <code>medium</code>: Sometimes use voice (50%) - <code>low</code>: Rarely use voice (10%) - <code>never</code>: Never use voice (0%)</p>"},{"location":"communication_readme/#personality-settings","title":"Personality Settings","text":"<p>Customize the robot's communication style:</p> <pre><code>personality:\n  use_emojis: true\n  tone: \"friendly\"  # friendly, professional, playful\n  verbosity: \"concise\"  # concise, detailed\n</code></pre> <p>Tone Examples:</p> <p>Friendly:</p> <p>\"I'm moving to observe the workspace so I can see all the objects clearly.\"</p> <p>Professional:</p> <p>\"Initiating observation pose transition to enable comprehensive workspace analysis.\"</p> <p>Playful:</p> <p>\"Time to get a bird's-eye view of the workspace! Moving into position now!\"</p>"},{"location":"communication_readme/#llm-provider-selection","title":"LLM Provider Selection","text":"<p>Choose your preferred LLM for generating explanations:</p> <pre><code>llm:\n  provider: \"groq\"  # Fast and free\n  # provider: \"openai\"  # High quality\n  # provider: \"gemini\"  # Good balance\n  # provider: \"ollama\"  # Fully local\n\n  temperature: 0.7\n  max_tokens: 150\n</code></pre>"},{"location":"communication_readme/#understanding-the-system","title":"\ud83d\udcca Understanding the System","text":""},{"location":"communication_readme/#how-it-works","title":"How It Works","text":"<pre><code>1. MCP Client sends tool call\n   \u2193\n2. Server receives request\n   \u2193\n3. ExplanationGenerator creates natural language explanation\n   \u2193\n4. Priority system decides: Should we speak?\n   \u2193\n5. If YES: Send to text2speech\n   \u2193\n6. Log explanation\n   \u2193\n7. Execute tool\n   \u2193\n8. Return result with explanation context\n</code></pre>"},{"location":"communication_readme/#priority-decision-logic","title":"Priority Decision Logic","text":"<pre><code>def should_speak(tool_name):\n    priority = VOICE_PRIORITY[tool_name]\n\n    if priority == 'high':\n        return True  # Always speak\n    elif priority == 'medium':\n        return random() &gt; 0.5  # 50% chance\n    elif priority == 'low':\n        return random() &gt; 0.9  # 10% chance\n    else:\n        return False  # Never speak\n</code></pre>"},{"location":"communication_readme/#customization-examples","title":"\ud83c\udfa8 Customization Examples","text":""},{"location":"communication_readme/#example-1-make-robot-more-talkative","title":"Example 1: Make Robot More Talkative","text":"<p>Increase voice output frequency in <code>explanation_config.yaml</code>:</p> <pre><code>voice:\n  priority:\n    # Make all operations at least medium priority\n    get_detected_objects: high      # Was: medium\n    get_largest_detected_object: medium  # Was: low\n    get_workspace_coordinate_from_point: medium  # Was: low\n</code></pre>"},{"location":"communication_readme/#example-2-silent-mode-with-logs","title":"Example 2: Silent Mode with Logs","text":"<p>Keep explanations in logs but disable voice:</p> <pre><code>voice:\n  enabled: false  # No speech\n\nlogging:\n  log_explanations: true  # Still log everything\n</code></pre>"},{"location":"communication_readme/#example-3-detailed-explanations","title":"Example 3: Detailed Explanations","text":"<p>Get more verbose explanations:</p> <pre><code>personality:\n  verbosity: \"detailed\"  # 2-4 sentences instead of 1-2\n\nllm:\n  max_tokens: 300  # Allow longer responses\n</code></pre>"},{"location":"communication_readme/#example-4-multilingual-support","title":"Example 4: Multilingual Support","text":"<p>Generate explanations in different languages:</p> <pre><code>advanced:\n  language: \"de\"  # German\n  multilingual_explanations: true\n\nprompts:\n  system_base: |\n    Du bist ein hilfreicher Roboter-Assistent, der seine Aktionen\n    auf Deutsch erkl\u00e4rt. Halte Erkl\u00e4rungen kurz und freundlich.\n</code></pre>"},{"location":"communication_readme/#monitoring-and-debugging","title":"\ud83d\udd0d Monitoring and Debugging","text":""},{"location":"communication_readme/#log-files","title":"Log Files","text":"<p>All explanations are logged to <code>log/mcp_server_YYYYMMDD_HHMMSS.log</code>:</p> <pre><code>2025-12-12 10:30:45 - TOOL CALL: pick_place_object\n  Kwargs: {'object_name': 'pencil', 'pick_coordinate': [0.15, -0.05], ...}\n  Explanation: \ud83e\udd16 I'm picking up the pencil and placing it right next to the red cube.\n  Status: SUCCESS\n  Result: \u2713 Successfully picked 'pencil' from [0.150, -0.050] and placed it...\n</code></pre>"},{"location":"communication_readme/#real-time-monitoring","title":"Real-Time Monitoring","text":"<pre><code># Watch logs in real-time\ntail -f log/mcp_server_*.log | grep \"Explanation:\"\n\n# Filter for specific tool\ntail -f log/mcp_server_*.log | grep \"pick_place_object\"\n\n# Watch voice output\ntail -f log/mcp_server_*.log | grep \"should_speak\"\n</code></pre>"},{"location":"communication_readme/#performance-metrics","title":"Performance Metrics","text":"<p>Check explanation generation performance:</p> <pre><code># Count explanations generated\ngrep \"Explanation:\" log/mcp_server_*.log | wc -l\n\n# Find slow LLM calls\ngrep \"Failed to generate explanation\" log/mcp_server_*.log\n</code></pre>"},{"location":"communication_readme/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"communication_readme/#test-explanation-generation","title":"Test Explanation Generation","text":"<pre><code>from explanation_generator import ExplanationGenerator\n\n# Initialize\ngen = ExplanationGenerator(api_choice=\"groq\")\n\n# Test explanation\nexplanation = gen.generate_explanation(\n    tool_name=\"pick_place_object\",\n    tool_description=\"Pick and place an object\",\n    arguments={\n        \"object_name\": \"pencil\",\n        \"pick_coordinate\": [0.15, -0.05],\n        \"place_coordinate\": [0.20, 0.10]\n    }\n)\n\nprint(explanation)\n# Output: \"\ud83e\udd16 I'm picking up the pencil and placing it at the target location.\"\n</code></pre>"},{"location":"communication_readme/#test-voice-priority","title":"Test Voice Priority","text":"<pre><code>gen = ExplanationGenerator()\n\n# Test different priorities\nprint(gen.should_speak(\"pick_place_object\"))  # True (high)\nprint(gen.should_speak(\"move2observation_pose\"))  # ~50% (medium)\nprint(gen.should_speak(\"get_detected_object\"))  # ~10% (low)\nprint(gen.should_speak(\"speak\"))  # False (never)\n</code></pre>"},{"location":"communication_readme/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"communication_readme/#no-explanations-generated","title":"No Explanations Generated","text":"<p>Problem: Robot executes silently without explanations.</p> <p>Solutions: 1. Check if <code>llm_client</code> is installed:    <pre><code>python -c \"from llm_client import LLMClient\"\n</code></pre></p> <ol> <li> <p>Verify API key is set:    <pre><code>echo $GROQ_API_KEY\n</code></pre></p> </li> <li> <p>Check server logs for errors:    <pre><code>tail -f log/mcp_server_*.log | grep \"explanation\"\n</code></pre></p> </li> <li> <p>Try with fallback mode:    <pre><code># Server will use simple templates if LLM fails\npython server/fastmcp_robot_server_communicative.py --verbose\n</code></pre></p> </li> </ol>"},{"location":"communication_readme/#no-voice-output","title":"No Voice Output","text":"<p>Problem: Explanations are logged but not spoken.</p> <p>Solutions: 1. Check if tool priority allows voice:    <pre><code># In explanation_config.yaml\nvoice:\n  priority:\n    your_tool: high  # Ensure it's at least high or medium\n</code></pre></p> <ol> <li> <p>Verify text2speech is working:    <pre><code># Test TTS independently\npython -c \"from text2speech import Text2Speech; tts = Text2Speech('', verbose=True); tts.call_text2speech_async('Test').join()\"\n</code></pre></p> </li> <li> <p>Check voice is enabled:    <pre><code>voice:\n  enabled: true\n</code></pre></p> </li> </ol>"},{"location":"communication_readme/#slow-response-times","title":"Slow Response Times","text":"<p>Problem: Long delay before tool execution.</p> <p>Solutions: 1. Use faster LLM provider:    <pre><code># Groq is fastest\npython server/fastmcp_robot_server_communicative.py --explanation-api groq\n</code></pre></p> <ol> <li> <p>Reduce token limit:    <pre><code>llm:\n  max_tokens: 100  # Shorter = faster\n</code></pre></p> </li> <li> <p>Disable for low-priority tools:    <pre><code>voice:\n  priority:\n    get_detected_object: never  # Skip explanation\n</code></pre></p> </li> </ol>"},{"location":"communication_readme/#performance-considerations","title":"\ud83d\udcc8 Performance Considerations","text":""},{"location":"communication_readme/#llm-api-costs","title":"LLM API Costs","text":"<p>Groq (Recommended for development): - \u2705 Free tier available - \u2705 Very fast (~0.5-1s per explanation) - \u2705 30 requests/minute (free tier)</p> <p>OpenAI: - \ud83d\udcb2 ~$0.01 per 1000 explanations (gpt-4o-mini) - \u26a1 Fast (~1-2s per explanation) - \u2705 High quality</p> <p>Ollama: - \u2705 Free (local) - \u26a0\ufe0f Slower (2-5s per explanation) - \u2705 No internet required</p>"},{"location":"communication_readme/#response-time-impact","title":"Response Time Impact","text":"Without Explanations With Explanations (Groq) With Explanations (Ollama) ~0.1s per tool call ~0.6s per tool call ~2.5s per tool call"},{"location":"communication_readme/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Cache common explanations (future feature)</li> <li>Use faster models: <code>gpt-4o-mini</code>, <code>llama-3.1-8b-instant</code></li> <li>Reduce max_tokens: 100-150 is usually sufficient</li> <li>Lower priority for routine operations</li> <li>Disable for debugging/testing</li> </ol>"},{"location":"communication_readme/#future-enhancements","title":"\ud83d\udd2e Future Enhancements","text":""},{"location":"communication_readme/#planned-features","title":"Planned Features","text":"<ul> <li>[ ] Explanation Caching: Cache identical tool calls</li> <li>[ ] Context Awareness: Use previous actions as context</li> <li>[ ] User Preferences: Per-user verbosity settings</li> <li>[ ] Emotion/Tone Adaptation: Adjust based on task success/failure</li> <li>[ ] Multi-Step Explanations: \"I'll do X, then Y, then Z\"</li> <li>[ ] Question Answering: \"Why did you do that?\"</li> <li>[ ] Explanation Summarization: Daily activity summaries</li> <li>[ ] Voice Customization: Different voices per robot/user</li> </ul>"},{"location":"communication_readme/#references","title":"\ud83d\udcda References","text":"<ul> <li>LLM Client: github.com/dgaida/llm_client</li> <li>Text2Speech: github.com/dgaida/text2speech</li> <li>FastMCP: github.com/jlowin/fastmcp</li> <li>Robot Environment: github.com/dgaida/robot_environment</li> </ul>"},{"location":"communication_readme/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>To add explanations for new tools:</p> <ol> <li>Add priority level to <code>explanation_config.yaml</code></li> <li>Decorate function with <code>@log_tool_call_with_explanation</code></li> <li>Add fallback template (optional)</li> <li>Test with different LLM providers</li> </ol> <p>Example: <pre><code>@mcp.tool\n@log_tool_call_with_explanation\n@validate_input(YourInputModel)\ndef your_new_tool(param1: str) -&gt; str:\n    \"\"\"Your tool description for explanation context.\"\"\"\n    # Your implementation\n    pass\n</code></pre></p>"},{"location":"communication_readme/#license","title":"\ud83d\udcc4 License","text":"<p>MIT License - Same as robot_mcp</p> <p>Made with \u2764\ufe0f for transparent robotic automation</p>"},{"location":"config_readme/","title":"Configuration Management System","text":"<p>Centralized configuration for the Robot MCP system using YAML files, Pydantic validation, and environment-specific overrides.</p>"},{"location":"config_readme/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>Overview</li> <li>Quick Start</li> <li>Configuration File</li> <li>Usage</li> <li>Environment Overrides</li> <li>Command-Line Overrides</li> <li>Accessing Configuration</li> <li>Validation</li> <li>Best Practices</li> </ul>"},{"location":"config_readme/#overview","title":"\ud83c\udfaf Overview","text":"<p>The configuration system provides:</p> <p>\u2705 Type-safe configuration - Pydantic models ensure all values are valid \u2705 Centralized settings - No more scattered command-line args \u2705 Environment support - dev/prod/test configs with overrides \u2705 Easy access - Dot notation for nested values \u2705 Runtime updates - Change settings programmatically \u2705 Validation - Catch errors before runtime</p>"},{"location":"config_readme/#before-scattered-configuration","title":"Before (Scattered Configuration)","text":"<pre><code># Command-line arguments\npython server/fastmcp_robot_server.py --robot niryo --no-simulation --port 8000 --verbose\n\n# Environment variables\nexport OPENAI_API_KEY=sk-...\nexport GROQ_API_KEY=gsk-...\nexport ELEVENLABS_API_KEY=...\n\n# Hardcoded defaults in multiple files\n# robot_id = \"niryo\"\n# use_simulation = True\n# temperature = 0.7\n</code></pre>"},{"location":"config_readme/#after-centralized-configuration","title":"After (Centralized Configuration)","text":"<pre><code># config/robot_config.yaml\nserver:\n  port: 8000\nrobot:\n  type: \"niryo\"\n  simulation: false\nllm:\n  temperature: 0.7\n</code></pre> <pre><code># Clean command line\npython server/fastmcp_robot_server_with_config.py\n\n# Or with environment\nROBOT_ENV=production python server/fastmcp_robot_server_with_config.py\n</code></pre>"},{"location":"config_readme/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"config_readme/#1-create-configuration-file","title":"1. Create Configuration File","text":"<pre><code># Copy the template\ncp config/robot_config.yaml.template config/robot_config.yaml\n\n# Edit for your setup\nnano config/robot_config.yaml\n</code></pre>"},{"location":"config_readme/#2-set-api-keys","title":"2. Set API Keys","text":"<pre><code># Keep sensitive keys in .env file\necho \"OPENAI_API_KEY=sk-...\" &gt;&gt; secrets.env\necho \"GROQ_API_KEY=gsk-...\" &gt;&gt; secrets.env\n</code></pre>"},{"location":"config_readme/#3-run-with-config","title":"3. Run with Config","text":"<pre><code># Server uses config automatically\npython server/fastmcp_robot_server_with_config.py\n\n# Client uses config automatically\npython client/fastmcp_universal_client_with_config.py\n</code></pre>"},{"location":"config_readme/#configuration-file","title":"\ud83d\udcc4 Configuration File","text":""},{"location":"config_readme/#structure","title":"Structure","text":"<pre><code>server:          # Server settings\n  host: \"127.0.0.1\"\n  port: 8000\n\nrobot:           # Robot configuration\n  type: \"niryo\"\n  simulation: true\n  workspace:     # Coordinate boundaries\n    niryo:\n      bounds:\n        x_min: 0.163\n        x_max: 0.337\n  motion:        # Motion parameters\n    pick_z_offset: 0.001\n\ndetection:       # Object detection\n  model: \"owlv2\"\n  confidence_threshold: 0.15\n  default_labels:\n    - \"pencil\"\n    - \"cube\"\n\nllm:             # LLM settings\n  default_provider: \"groq\"\n  temperature: 0.7\n  providers:\n    openai:\n      models: [\"gpt-4o\", \"gpt-4o-mini\"]\n    groq:\n      models: [\"llama-3.3-70b-versatile\"]\n\ntts:             # Text-to-speech\n  enabled: true\n  provider: \"elevenlabs\"\n\nredis:           # Redis connection\n  host: \"localhost\"\n  port: 6379\n\ngui:             # Web interface\n  port: 7860\n  enable_voice_input: true\n\nlogging:         # Logging config\n  format: \"%(asctime)s - %(levelname)s - %(message)s\"\n  levels:\n    root: \"INFO\"\n\nenvironments:    # Environment overrides\n  production:\n    robot:\n      simulation: false\n    server:\n      host: \"0.0.0.0\"\n</code></pre>"},{"location":"config_readme/#complete-example","title":"Complete Example","text":"<p>See <code>config/robot_config.yaml</code> (created in first artifact) for the complete configuration with all options documented.</p>"},{"location":"config_readme/#usage","title":"\ud83d\udcbb Usage","text":""},{"location":"config_readme/#in-server","title":"In Server","text":"<pre><code>from config.config_manager import load_config\n\n# Load configuration\nconfig = load_config()\n\n# Access settings\nhost = config.server.host\nport = config.server.port\nrobot_type = config.robot.type\n\nprint(f\"Starting server at {host}:{port}\")\nprint(f\"Robot: {robot_type}\")\n</code></pre>"},{"location":"config_readme/#in-client","title":"In Client","text":"<pre><code>from config.config_manager import ConfigManager\n\n# Load config\nconfig = ConfigManager.load()\n\n# Use in client initialization\nclient = RobotUniversalMCPClient(\n    config=config,\n    api_choice=config.llm.default_provider\n)\n\n# Access nested values\ntemperature = config.llm.temperature\nmax_tokens = config.llm.max_tokens\n</code></pre>"},{"location":"config_readme/#dot-notation-access","title":"Dot Notation Access","text":"<pre><code># Get nested values with dot notation\nx_min = config.get(\"robot.workspace.niryo.bounds.x_min\")\n# Returns: 0.163\n\n# Get with default if not found\ntimeout = config.get(\"server.timeout\", default=30)\n# Returns: 30 (or configured value)\n\n# Set values at runtime\nconfig.set(\"llm.temperature\", 0.9)\nconfig.set(\"robot.verbose\", True)\n</code></pre>"},{"location":"config_readme/#direct-property-access","title":"Direct Property Access","text":"<pre><code># Access via properties\nserver_config = config.server\nrobot_config = config.robot\nllm_config = config.llm\n\n# Properties are type-safe\nprint(f\"Port: {server_config.port}\")  # int\nprint(f\"Simulation: {robot_config.simulation}\")  # bool\nprint(f\"Temperature: {llm_config.temperature}\")  # float\n</code></pre>"},{"location":"config_readme/#environment-overrides","title":"\ud83c\udf0d Environment Overrides","text":"<p>Use environment-specific configs for dev/prod/test without changing the main config.</p>"},{"location":"config_readme/#define-environments","title":"Define Environments","text":"<pre><code># config/robot_config.yaml\nenvironments:\n  development:\n    server:\n      log_level: \"DEBUG\"\n    robot:\n      simulation: true\n      verbose: true\n    llm:\n      temperature: 0.8\n\n  production:\n    server:\n      log_level: \"WARNING\"\n      host: \"0.0.0.0\"  # Allow external connections\n    robot:\n      simulation: false\n      verbose: false\n    llm:\n      temperature: 0.5  # More deterministic\n\n  testing:\n    server:\n      log_level: \"DEBUG\"\n    robot:\n      enable_camera: false  # Faster tests\n    llm:\n      max_iterations: 5\n</code></pre>"},{"location":"config_readme/#activate-environment","title":"Activate Environment","text":"<pre><code># Method 1: Environment variable\nexport ROBOT_ENV=production\npython server/fastmcp_robot_server_with_config.py\n\n# Method 2: Command-line argument\npython server/fastmcp_robot_server_with_config.py --environment production\n\n# Method 3: Programmatically\nconfig = ConfigManager.load(environment=\"production\")\n</code></pre>"},{"location":"config_readme/#how-it-works","title":"How It Works","text":"<ol> <li>Base config is loaded from <code>robot_config.yaml</code></li> <li>Environment overrides are merged on top</li> <li>Only specified values are overridden</li> <li>Unspecified values keep base config values</li> </ol> <p>Example:</p> <pre><code># Base config\nrobot:\n  type: \"niryo\"\n  simulation: true\n  verbose: false\n\n# Production override\nenvironments:\n  production:\n    robot:\n      simulation: false  # Override\n      # verbose stays false (not overridden)\n</code></pre> <p>Result in production: <pre><code>config.robot.type       # \"niryo\" (from base)\nconfig.robot.simulation # False (overridden)\nconfig.robot.verbose    # False (from base)\n</code></pre></p>"},{"location":"config_readme/#command-line-overrides","title":"\u2699\ufe0f Command-Line Overrides","text":"<p>Override specific settings without modifying config files.</p>"},{"location":"config_readme/#server-overrides","title":"Server Overrides","text":"<pre><code># Override server settings\npython server/fastmcp_robot_server_with_config.py --host 0.0.0.0 --port 8080\n\n# Override robot settings\npython server/fastmcp_robot_server_with_config.py --robot-type widowx --no-simulation\n\n# Multiple overrides\npython server/fastmcp_robot_server_with_config.py \\\n  --robot-type niryo \\\n  --no-simulation \\\n  --no-camera \\\n  --verbose \\\n  --port 8080\n</code></pre>"},{"location":"config_readme/#client-overrides","title":"Client Overrides","text":"<pre><code># Override LLM provider\npython client/fastmcp_universal_client_with_config.py --api openai\n\n# Override model\npython client/fastmcp_universal_client_with_config.py --api groq --model llama-3.1-8b-instant\n\n# Single command with overrides\npython client/fastmcp_universal_client_with_config.py \\\n  --api openai \\\n  --model gpt-4o \\\n  --command \"What objects do you see?\"\n</code></pre>"},{"location":"config_readme/#priority-order","title":"Priority Order","text":"<ol> <li>Command-line arguments (highest priority)</li> <li>Environment overrides</li> <li>Base configuration</li> <li>Default values (lowest priority)</li> </ol> <p>Example:</p> <pre><code># config/robot_config.yaml\nrobot:\n  type: \"niryo\"  # Base: niryo\n\nenvironments:\n  production:\n    robot:\n      type: \"widowx\"  # Production: widowx\n</code></pre> <pre><code># Command line overrides all\npython server/fastmcp_robot_server_with_config.py --robot-type niryo\n\n# Result: niryo (CLI wins)\n</code></pre>"},{"location":"config_readme/#accessing-configuration","title":"\ud83d\udd0d Accessing Configuration","text":""},{"location":"config_readme/#in-application-code","title":"In Application Code","text":"<pre><code>from config.config_manager import ConfigManager\n\n# Get singleton instance (after load)\nconfig = ConfigManager.get_instance()\n\n# Or load directly\nconfig = ConfigManager.load()\n\n# Access server config\nprint(f\"Server: {config.server.host}:{config.server.port}\")\nprint(f\"Log level: {config.server.log_level}\")\n\n# Access robot config\nprint(f\"Robot: {config.robot.type}\")\nprint(f\"Simulation: {config.robot.simulation}\")\n\n# Access workspace bounds\nworkspace = config.robot.workspace[\"niryo\"]\nprint(f\"X range: [{workspace.bounds.x_min}, {workspace.bounds.x_max}]\")\nprint(f\"Y range: [{workspace.bounds.y_min}, {workspace.bounds.y_max}]\")\n\n# Access motion parameters\nprint(f\"Pick z-offset: {config.robot.motion.pick_z_offset}\")\nprint(f\"Safe height: {config.robot.motion.safe_height}\")\n\n# Access LLM settings\nprint(f\"Provider: {config.llm.default_provider}\")\nprint(f\"Temperature: {config.llm.temperature}\")\nprint(f\"Max tokens: {config.llm.max_tokens}\")\n\n# Get provider-specific settings\nopenai_config = config.llm.providers[\"openai\"]\nprint(f\"OpenAI models: {openai_config.models}\")\nprint(f\"Rate limit: {openai_config.rate_limit_rpm}\")\n</code></pre>"},{"location":"config_readme/#runtime-modifications","title":"Runtime Modifications","text":"<pre><code># Change settings at runtime\nconfig.set(\"llm.temperature\", 0.9)\nconfig.set(\"robot.verbose\", True)\n\n# Verify changes\nprint(config.get(\"llm.temperature\"))  # 0.9\nprint(config.robot.verbose)  # True\n\n# Save modified config\nconfig.save(\"config/my_modified_config.yaml\")\n</code></pre>"},{"location":"config_readme/#export-configuration","title":"Export Configuration","text":"<pre><code># Export as dictionary\nconfig_dict = config.to_dict()\n\n# Save to new file\nconfig.save(\"config/backup_config.yaml\")\n\n# Pretty print\nimport json\nprint(json.dumps(config.to_dict(), indent=2))\n</code></pre>"},{"location":"config_readme/#validation","title":"\u2705 Validation","text":"<p>All configuration is validated using Pydantic models.</p>"},{"location":"config_readme/#automatic-validation","title":"Automatic Validation","text":"<pre><code># This will raise ValidationError\nconfig_dict = {\n    \"server\": {\n        \"port\": 99999  # \u274c Invalid: must be 1024-65535\n    },\n    \"robot\": {\n        \"type\": \"invalid\"  # \u274c Invalid: must be niryo or widowx\n    },\n    \"llm\": {\n        \"temperature\": 3.0  # \u274c Invalid: must be 0.0-2.0\n    }\n}\n\ntry:\n    config = RobotMCPConfig(**config_dict)\nexcept ValidationError as e:\n    print(\"Validation errors:\")\n    for error in e.errors():\n        print(f\"  \u2022 {error['loc']}: {error['msg']}\")\n</code></pre>"},{"location":"config_readme/#validation-rules","title":"Validation Rules","text":"<p>Server: - <code>port</code>: 1024-65535 - <code>log_level</code>: DEBUG, INFO, WARNING, ERROR, CRITICAL</p> <p>Robot: - <code>type</code>: \"niryo\" or \"widowx\" - <code>camera_update_rate_hz</code>: 0.1-30.0 - Motion parameters: sensible ranges (e.g., z_offset: 0.0-0.1m)</p> <p>Detection: - <code>model</code>: \"owlv2\" or \"yoloworld\" - <code>device</code>: \"cuda\" or \"cpu\" - <code>confidence_threshold</code>: 0.0-1.0 - <code>iou_threshold</code>: 0.0-1.0</p> <p>LLM: - <code>default_provider</code>: \"auto\", \"openai\", \"groq\", \"gemini\", \"ollama\" - <code>temperature</code>: 0.0-2.0 - <code>max_tokens</code>: 1-128000 - <code>max_iterations</code>: 1-50</p> <p>Workspace Bounds: - <code>x_max</code> must be &gt; <code>x_min</code> - <code>y_max</code> must be &gt; <code>y_min</code> - <code>center</code> must be [x, y] (2 values)</p>"},{"location":"config_readme/#custom-validation","title":"Custom Validation","text":"<p>Add custom validators in <code>config_manager.py</code>:</p> <pre><code>class ServerConfig(BaseModel):\n    port: int = Field(8000, ge=1024, le=65535)\n\n    @field_validator(\"port\")\n    @classmethod\n    def validate_port_available(cls, v: int) -&gt; int:\n        # Check if port is available\n        import socket\n        try:\n            sock = socket.socket()\n            sock.bind(('127.0.0.1', v))\n            sock.close()\n            return v\n        except OSError:\n            raise ValueError(f\"Port {v} is already in use\")\n</code></pre>"},{"location":"config_readme/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"config_readme/#1-keep-secrets-separate","title":"1. Keep Secrets Separate","text":"<p>\u274c Don't put API keys in config files:</p> <pre><code># \u274c BAD\nllm:\n  api_key: \"sk-123456789...\"  # Don't do this!\n</code></pre> <p>\u2705 Do use environment variables:</p> <pre><code># \u2705 GOOD\nllm:\n  default_provider: \"openai\"\n</code></pre> <pre><code># secrets.env\nOPENAI_API_KEY=sk-123456789...\n</code></pre>"},{"location":"config_readme/#2-use-environment-overrides","title":"2. Use Environment Overrides","text":"<p>\u274c Don't duplicate configs:</p> <pre><code>config/\n  dev_config.yaml\n  prod_config.yaml  # Duplication!\n  test_config.yaml\n</code></pre> <p>\u2705 Do use environment overrides:</p> <pre><code># Single config file with overrides\nenvironments:\n  development: { ... }\n  production: { ... }\n  testing: { ... }\n</code></pre>"},{"location":"config_readme/#3-validate-early","title":"3. Validate Early","text":"<p>\u274c Don't discover errors at runtime:</p> <pre><code># Program crashes after 10 minutes\nrobot_type = config[\"robt\"][\"tipe\"]  # Typo!\n</code></pre> <p>\u2705 Do validate on load:</p> <pre><code># Fails immediately with clear error\nconfig = ConfigManager.load()\n# ValidationError: Field 'robot.type' is required\n</code></pre>"},{"location":"config_readme/#4-use-type-safe-access","title":"4. Use Type-Safe Access","text":"<p>\u274c Don't use dictionary access:</p> <pre><code>port = config.to_dict()[\"server\"][\"port\"]  # No type checking\n</code></pre> <p>\u2705 Do use properties:</p> <pre><code>port: int = config.server.port  # Type-safe!\n</code></pre>"},{"location":"config_readme/#5-document-configuration","title":"5. Document Configuration","text":"<pre><code># \u2705 Add comments explaining options\ndetection:\n  # Object detection model\n  # Options: \"owlv2\" (accurate, slow) or \"yoloworld\" (fast, less accurate)\n  model: \"owlv2\"\n\n  # Minimum confidence score (0.0-1.0)\n  # Lower = more detections but more false positives\n  confidence_threshold: 0.15\n</code></pre>"},{"location":"config_readme/#6-version-your-config","title":"6. Version Your Config","text":"<pre><code># Track config changes in git\ngit add config/robot_config.yaml\ngit commit -m \"Update detection confidence threshold\"\n\n# But exclude sensitive files\necho \"secrets.env\" &gt;&gt; .gitignore\n</code></pre>"},{"location":"config_readme/#7-provide-defaults","title":"7. Provide Defaults","text":"<pre><code># Always provide sensible defaults\nclass ServerConfig(BaseModel):\n    host: str = Field(\"127.0.0.1\", description=\"Server host\")\n    port: int = Field(8000, ge=1024, le=65535)\n    max_workers: int = Field(4, ge=1)\n</code></pre>"},{"location":"config_readme/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"config_readme/#config-file-not-found","title":"Config File Not Found","text":"<pre><code>\u274c Configuration file not found: config/robot_config.yaml\n</code></pre> <p>Solution: <pre><code># Create from template\ncp config/robot_config.yaml.template config/robot_config.yaml\n\n# Or specify path\npython server/fastmcp_robot_server_with_config.py --config /path/to/config.yaml\n</code></pre></p>"},{"location":"config_readme/#validation-errors","title":"Validation Errors","text":"<pre><code>\u274c Configuration validation failed:\n  \u2022 server.port: must be between 1024 and 65535\n  \u2022 robot.type: must be one of: niryo, widowx\n</code></pre> <p>Solution: Fix the invalid values in your config file.</p>"},{"location":"config_readme/#environment-not-applied","title":"Environment Not Applied","text":"<pre><code>ROBOT_ENV=production python server/fastmcp_robot_server_with_config.py\n# Still using development settings?\n</code></pre> <p>Solution: Check environment name spelling and that overrides are defined:</p> <pre><code>environments:\n  production:  # Must match ROBOT_ENV exactly\n    server:\n      log_level: \"WARNING\"\n</code></pre>"},{"location":"config_readme/#import-errors","title":"Import Errors","text":"<pre><code>ModuleNotFoundError: No module named 'config.config_manager'\n</code></pre> <p>Solution: Ensure you're running from project root:</p> <pre><code># From project root\npython server/fastmcp_robot_server_with_config.py\n\n# Not from subdirectory\ncd server &amp;&amp; python fastmcp_robot_server_with_config.py  # \u274c Won't work\n</code></pre>"},{"location":"config_readme/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Configuration Reference: See <code>config/robot_config.yaml</code> for all options</li> <li>Pydantic Documentation: https://docs.pydantic.dev/</li> <li>YAML Syntax: https://yaml.org/</li> <li>Server Documentation: <code>docs/mcp_setup_guide.md</code></li> <li>API Reference: <code>docs/mcp_api_reference.md</code></li> </ul>"},{"location":"config_readme/#summary","title":"\ud83c\udf89 Summary","text":"<p>The configuration management system provides:</p> <p>\u2705 Single source of truth for all settings \u2705 Type-safe with Pydantic validation \u2705 Environment-specific configs (dev/prod/test) \u2705 Easy access via dot notation and properties \u2705 Runtime updates when needed \u2705 Clean CLI without dozens of arguments \u2705 Better maintainability - change once, apply everywhere</p> <p>Before: <pre><code>python server.py --host 0.0.0.0 --port 8000 --robot niryo --no-sim --camera-rate 2 --model owlv2 --confidence 0.15 --temp 0.7 --max-tokens 4096 ...\n</code></pre></p> <p>After: <pre><code>python server.py  # All settings in config\n</code></pre></p> <p>Happy configuring! \ud83d\ude80</p>"},{"location":"examples/","title":"Robot MCP - Examples","text":"<p>Common use cases and example workflows for robot control with natural language.</p>"},{"location":"examples/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Start Examples</li> <li>Basic Operations</li> <li>Object Manipulation</li> <li>Spatial Reasoning</li> <li>Complex Workflows</li> <li>Advanced Patterns</li> <li>Tips and Best Practices</li> </ul>"},{"location":"examples/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"examples/#example-1-hello-robot","title":"Example 1: Hello Robot","text":"<pre><code>You: What objects do you see?\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Detected 3 objects:\n   - pencil at [0.15, -0.05]\n   - red cube at [0.20, 0.10]\n   - pen at [0.18, -0.03]\n\n\ud83e\udd16 Assistant: I can see 3 objects: a pencil at [0.15, -0.05],\n   a red cube at [0.20, 0.10], and a pen at [0.18, -0.03]\n</code></pre>"},{"location":"examples/#example-2-simple-pick-and-place","title":"Example 2: Simple Pick and Place","text":"<pre><code>You: Pick up the pencil and place it at [0.2, 0.1]\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Detected objects...\n\n\ud83d\udd27 Calling tool: pick_place_object\n   Arguments: {\n     \"object_name\": \"pencil\",\n     \"pick_coordinate\": [0.15, -0.05],\n     \"place_coordinate\": [0.2, 0.1],\n     \"location\": \"none\"\n   }\n\u2713 Result: Successfully picked and placed\n\n\ud83e\udd16 Assistant: Done! I've picked up the pencil from [0.15, -0.05]\n   and placed it at [0.2, 0.1].\n</code></pre>"},{"location":"examples/#example-3-relative-placement","title":"Example 3: Relative Placement","text":"<pre><code>You: Move the pencil to the right of the red cube\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Found pencil and red cube\n\n\ud83d\udd27 Calling tool: pick_place_object\n   Arguments: {\n     \"object_name\": \"pencil\",\n     \"pick_coordinate\": [0.15, -0.05],\n     \"place_coordinate\": [0.20, 0.10],\n     \"location\": \"right next to\"\n   }\n\u2713 Result: Successfully placed\n\n\ud83e\udd16 Assistant: Perfect! I've moved the pencil to the right of\n   the red cube.\n</code></pre>"},{"location":"examples/#basic-operations","title":"Basic Operations","text":""},{"location":"examples/#workspace-scan","title":"Workspace Scan","text":"<pre><code>You: Scan the workspace and tell me everything you see\n\n\ud83d\udd27 Calling tool: move2observation_pose(\"niryo_ws\")\n\u2713 Result: Moved to observation pose\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Detected 4 objects...\n\n\ud83e\udd16 Assistant: I can see 4 objects in the workspace:\n\n   1. Pencil at [0.15, -0.05]\n      Size: 1.5cm \u00d7 12cm (area: 18 cm\u00b2)\n\n   2. Red cube at [0.20, 0.10]\n      Size: 4cm \u00d7 4cm (area: 16 cm\u00b2)\n\n   3. Pen at [0.18, -0.03]\n      Size: 1.2cm \u00d7 14cm (area: 16.8 cm\u00b2)\n\n   4. Blue square at [0.18, -0.10]\n      Size: 5cm \u00d7 5cm (area: 25 cm\u00b2)\n</code></pre>"},{"location":"examples/#find-specific-objects","title":"Find Specific Objects","text":"<pre><code>You: Where is the pencil?\n\n\ud83d\udd27 Calling tool: get_detected_objects(label=\"pencil\")\n\u2713 Result: Found pencil at [0.15, -0.05]\n\n\ud83e\udd16 Assistant: The pencil is located at coordinates [0.15, -0.05].\n</code></pre> <pre><code>You: Which object is the largest?\n\n\ud83d\udd27 Calling tool: get_largest_detected_object\n\u2713 Result: {\n  \"label\": \"blue square\",\n  \"position\": {\"x\": 0.18, \"y\": -0.10},\n  \"size\": {\"area_cm2\": 25.0}\n}\n\n\ud83e\udd16 Assistant: The largest object is the blue square at\n   [0.18, -0.10] with an area of 25 square centimeters.\n</code></pre> <pre><code>You: What objects are on the left side?\n\n\ud83d\udd27 Calling tool: get_detected_objects(\n     location=\"left next to\",\n     coordinate=[0.25, 0.0]\n   )\n\u2713 Result: Found 2 objects on left side\n\n\ud83e\udd16 Assistant: On the left side I found:\n   - Red cube at [0.20, 0.10]\n   - Pen at [0.18, -0.03]\n</code></pre>"},{"location":"examples/#safe-placement","title":"Safe Placement","text":"<pre><code>You: Find a safe place to put this object\n\n\ud83d\udd27 Calling tool: get_largest_free_space_with_center\n\u2713 Result: area=0.0050 m\u00b2, center=[0.25, -0.04]\n\n\ud83d\udd27 Calling tool: place_object([0.25, -0.04], None)\n\u2713 Result: Successfully placed\n\n\ud83e\udd16 Assistant: I've placed the object in the largest free space\n   at [0.25, -0.04] (50 cm\u00b2 available).\n</code></pre>"},{"location":"examples/#object-manipulation","title":"Object Manipulation","text":""},{"location":"examples/#sorting-by-size","title":"Sorting by Size","text":"<pre><code>You: Sort all objects by size from smallest to largest in a line\n\n\ud83d\udd27 Calling tool: get_detected_objects_sorted(ascending=True)\n\u2713 Result: Sorted 3 objects by size\n\n\ud83d\udd27 Calling tool: pick_place_object (object 1)\n   Arguments: {\"object_name\": \"pen\", ...}\n\u2713 Result: Placed smallest object\n\n\ud83d\udd27 Calling tool: pick_place_object (object 2)\n   Arguments: {\"object_name\": \"pencil\", ...}\n\u2713 Result: Placed medium object\n\n\ud83d\udd27 Calling tool: pick_place_object (object 3)\n   Arguments: {\"object_name\": \"cube\", ...}\n\u2713 Result: Placed largest object\n\n\ud83e\udd16 Assistant: Done! I've sorted all objects by size in a horizontal\n   line:\n   - Smallest: pen at [0.15, -0.05]\n   - Medium: pencil at [0.15, 0.03]\n   - Largest: cube at [0.15, 0.11]\n</code></pre>"},{"location":"examples/#color-based-grouping","title":"Color-Based Grouping","text":"<pre><code>You: Group objects by color: red on left, blue on right\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Found 4 objects\n\n\ud83d\udd27 Calling tool: pick_place_object (red cube)\n\u2713 Result: Moved to left side\n\n\ud83d\udd27 Calling tool: pick_place_object (blue square)\n\u2713 Result: Moved to right side\n\n\ud83e\udd16 Assistant: Objects are now grouped by color:\n   - Left side (red): red cube at [0.20, 0.06]\n   - Right side (blue): blue square at [0.20, -0.06]\n</code></pre>"},{"location":"examples/#stacking-objects","title":"Stacking Objects","text":"<pre><code>You: Stack the small cube on top of the large cube\n\n\ud83d\udd27 Calling tool: get_detected_objects(label=\"cube\")\n\u2713 Result: Found 2 cubes\n\n\ud83d\udd27 Calling tool: pick_place_object\n   Arguments: {\n     \"object_name\": \"small cube\",\n     \"pick_coordinate\": [0.16, 0.05],\n     \"place_coordinate\": [0.22, 0.10],\n     \"location\": \"on top of\"\n   }\n\u2713 Result: Successfully stacked\n\n\ud83e\udd16 Assistant: Perfect! I've stacked the small cube on top of\n   the large cube at [0.22, 0.10].\n</code></pre>"},{"location":"examples/#spatial-reasoning","title":"Spatial Reasoning","text":""},{"location":"examples/#triangle-pattern","title":"Triangle Pattern","text":"<pre><code>You: Arrange objects in a triangle\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Found 3 objects\n\n\ud83d\udd27 Calling tool: pick_place_object (object 1)\n   \u2192 Top vertex: [0.20, 0.00]\n\u2713 Result: Placed\n\n\ud83d\udd27 Calling tool: pick_place_object (object 2)\n   \u2192 Bottom right: [0.28, -0.06]\n\u2713 Result: Placed\n\n\ud83d\udd27 Calling tool: pick_place_object (object 3)\n   \u2192 Bottom left: [0.28, 0.06]\n\u2713 Result: Placed\n\n\ud83e\udd16 Assistant: Done! Objects arranged in triangle:\n        \u25cb [0.20, 0.00]\n       / \\\n      /   \\\n     \u25cb\u2500\u2500\u2500\u2500\u2500\u25cb\n   [0.28,  [0.28,\n    0.06]  -0.06]\n</code></pre>"},{"location":"examples/#distance-based-queries","title":"Distance-Based Queries","text":"<pre><code>You: What's the closest object to [0.2, 0.0]?\n\n\ud83d\udd27 Calling tool: get_detected_objects(\n     location=\"close to\",\n     coordinate=[0.2, 0.0]\n   )\n\u2713 Result: Found pencil at [0.20, 0.02]\n\n\ud83e\udd16 Assistant: The pencil at [0.20, 0.02] is closest,\n   only 2 centimeters away from [0.2, 0.0].\n</code></pre>"},{"location":"examples/#boundary-aware-placement","title":"Boundary-Aware Placement","text":"<pre><code>You: Place objects at the corners of the workspace\n\n\ud83d\udd27 Calling tool: get_workspace_coordinate_from_point\n   (\"niryo_ws\", \"upper left corner\")\n\u2713 Result: [0.337, 0.087]\n\n\ud83d\udd27 Calling tool: get_workspace_coordinate_from_point\n   (\"niryo_ws\", \"upper right corner\")\n\u2713 Result: [0.337, -0.087]\n\n\ud83d\udd27 Calling tool: pick_place_object (4 times)\n\u2713 Result: Placed all objects at corners\n\n\ud83e\udd16 Assistant: Objects placed at workspace corners:\n   - Upper left: [0.337, 0.087]\n   - Upper right: [0.337, -0.087]\n   - Lower left: [0.163, 0.087]\n   - Lower right: [0.163, -0.087]\n</code></pre>"},{"location":"examples/#complex-workflows","title":"Complex Workflows","text":""},{"location":"examples/#multi-step-task","title":"Multi-Step Task","text":"<pre><code>You: Execute: 1) Find the pencil 2) Move it to center 3) Find\n     largest object 4) Place it right of pencil 5) Report positions\n\n\ud83d\udd27 Calling tool: get_detected_objects(label=\"pencil\")\n\u2713 Result: Pencil at [0.15, -0.05]\n\n\ud83d\udd27 Calling tool: get_workspace_coordinate_from_point\n   (\"niryo_ws\", \"center point\")\n\u2713 Result: [0.25, 0.0]\n\n\ud83d\udd27 Calling tool: pick_place_object\n   \u2192 Move pencil to center\n\u2713 Result: Placed at [0.25, 0.0]\n\n\ud83d\udd27 Calling tool: get_largest_detected_object\n\u2713 Result: Red cube at [0.20, 0.10]\n\n\ud83d\udd27 Calling tool: pick_place_object\n   Arguments: {\n     \"place_coordinate\": [0.25, 0.0],\n     \"location\": \"right next to\"\n   }\n\u2713 Result: Placed right of pencil\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Updated positions\n\n\ud83e\udd16 Assistant: Task completed! Final positions:\n   - Pencil: [0.25, 0.0] (center)\n   - Red cube: [0.25, -0.04] (right of pencil)\n</code></pre>"},{"location":"examples/#conditional-logic","title":"Conditional Logic","text":"<pre><code>You: If there are more than 3 objects, arrange in grid.\n     Otherwise, arrange in line.\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Found 4 objects\n\n\ud83e\udd16 Assistant: Found 4 objects (more than 3), arranging in 2\u00d72 grid...\n\n\ud83d\udd27 Calling tool: pick_place_object (4 times)\n   Grid positions:\n   [0.18, -0.04] [0.18, 0.04]\n   [0.26, -0.04] [0.26, 0.04]\n\u2713 Result: All objects arranged\n\n\ud83e\udd16 Assistant: Grid arrangement complete:\n   \u25cb  \u25cb  [0.18, -0.04] [0.18, 0.04]\n   \u25cb  \u25cb  [0.26, -0.04] [0.26, 0.04]\n</code></pre>"},{"location":"examples/#error-recovery","title":"Error Recovery","text":"<pre><code>You: Try to pick the 'diamond'. If not found, pick any object instead.\n\n\ud83d\udd27 Calling tool: get_detected_objects(label=\"diamond\")\n\u2713 Result: No object with label 'diamond' found\n\n\ud83d\udd27 Calling tool: speak(\"Diamond not found. Picking alternative object.\")\n\u2713 Result: Speaking...\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Found 3 objects\n\n\ud83d\udd27 Calling tool: pick_object(\"pencil\", [0.15, -0.05])\n\u2713 Result: Picked pencil as alternative\n\n\ud83e\udd16 Assistant: I couldn't find a diamond, so I picked up the\n   pencil instead as an alternative.\n</code></pre>"},{"location":"examples/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<pre><code>You: Move all small objects (&lt; 20 cm\u00b2) to the left side\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Found 4 objects\n\n\ud83e\udd16 Assistant: Filtering objects under 20 cm\u00b2...\n   Found 2 small objects: pen (16.8 cm\u00b2), pencil (18 cm\u00b2)\n\n\ud83d\udd27 Calling tool: pick_place_object (pen)\n   \u2192 Target: [0.18, -0.06]\n\u2713 Result: Placed\n\n\ud83d\udd27 Calling tool: pick_place_object (pencil)\n   \u2192 Target: [0.18, -0.01]\n\u2713 Result: Placed\n\n\ud83d\udd27 Calling tool: speak(\"Moved 2 small objects to the left\")\n\u2713 Result: Speaking...\n\n\ud83e\udd16 Assistant: Done! Moved 2 small objects to the left side:\n   - Pen: [0.18, -0.06]\n   - Pencil: [0.18, -0.01]\n</code></pre>"},{"location":"examples/#push-operations","title":"Push Operations","text":"<pre><code>You: If the object is too large to pick, push it instead\n\n\ud83d\udd27 Calling tool: get_largest_detected_object\n\u2713 Result: Large box at [0.25, 0.05], width: 8cm\n\n\ud83e\udd16 Assistant: Object width (8cm) exceeds gripper limit (5cm).\n   Using push operation...\n\n\ud83d\udd27 Calling tool: speak(\"Object too large to grip. Using push.\")\n\u2713 Result: Speaking...\n\n\ud83d\udd27 Calling tool: push_object\n   Arguments: {\n     \"object_name\": \"large box\",\n     \"push_coordinate\": [0.25, 0.05],\n     \"direction\": \"right\",\n     \"distance\": 50.0\n   }\n\u2713 Result: Pushed 50mm to the right\n\n\ud83e\udd16 Assistant: Successfully pushed the large box 5cm to the right.\n   New position: [0.25, 0.0]\n</code></pre>"},{"location":"examples/#dynamic-workspace-adaptation","title":"Dynamic Workspace Adaptation","text":"<pre><code>You: Organize workspace: densely packed objects to right,\n     spread out objects to left\n\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Found 5 objects\n\n\ud83e\udd16 Assistant: Analyzing object density...\n   - Dense cluster (3 objects): right side\n   - Sparse objects (2 objects): left side\n\n\ud83d\udd27 Calling tool: pick_place_object (3 times for dense)\n   \u2192 Right positions: [0.28, -0.08], [0.28, -0.04], [0.28, 0.0]\n\u2713 Result: Dense objects positioned\n\n\ud83d\udd27 Calling tool: pick_place_object (2 times for sparse)\n   \u2192 Left positions: [0.18, -0.02], [0.18, 0.06]\n\u2713 Result: Sparse objects positioned\n\n\ud83e\udd16 Assistant: Workspace organized:\n   - Left (sparse): 2 objects with 8cm spacing\n   - Right (dense): 3 objects with 4cm spacing\n</code></pre>"},{"location":"examples/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"examples/#1-always-detect-before-manipulating","title":"1. Always Detect Before Manipulating","text":"<pre><code>\u2705 Good:\nYou: What objects do you see?\n     [Wait for response]\nYou: Pick up the pencil\n\n\ud83d\udd27 Uses fresh coordinates from detection\n\n\u274c Bad:\nYou: Pick up the pencil at [0.15, -0.05]\n\n\u26a0\ufe0f  Coordinates might be stale if object moved\n</code></pre>"},{"location":"examples/#2-use-exact-label-matching","title":"2. Use Exact Label Matching","text":"<pre><code>\u2705 Good:\n\ud83d\udd27 get_detected_objects\n\u2713 Result: Found \"pencil\" (lowercase)\n\n\ud83d\udd27 pick_object(\"pencil\", ...)  \u2190 Exact match\n\n\u274c Bad:\n\ud83d\udd27 pick_object(\"Pencil\", ...)  \u2190 Wrong case\n\ud83d\udd27 pick_object(\"pen\", ...)     \u2190 Wrong object\n</code></pre>"},{"location":"examples/#3-check-for-success","title":"3. Check for Success","text":"<pre><code>\u2705 Good:\nYou: Pick up the pencil\n     [Wait for confirmation]\nYou: Did that work?\n\n\ud83d\udd27 Calling tool: get_detected_objects(label=\"pencil\")\n\u2713 Result: Pencil position changed \u2190 Success!\n\n\u274c Bad:\nYou: Pick up pencil. Move cube. Stack items.\n     [No verification between steps]\n</code></pre>"},{"location":"examples/#4-use-safe-placement","title":"4. Use Safe Placement","text":"<pre><code>\u2705 Good:\nYou: Place object in a safe location\n\n\ud83d\udd27 get_largest_free_space_with_center\n\u2713 Result: area=0.0045 m\u00b2, center=[0.24, -0.03]\n\n\ud83d\udd27 place_object([0.24, -0.03], None)\n\u2713 Result: No collisions\n\n\u274c Risky:\nYou: Place object at [0.25, 0.0]\n\n\u26a0\ufe0f  Might collide with existing objects\n</code></pre>"},{"location":"examples/#5-provide-user-feedback","title":"5. Provide User Feedback","text":"<pre><code>\u2705 Good:\n\ud83d\udd27 speak(\"Scanning workspace...\")\n\ud83d\udd27 get_detected_objects\n\ud83d\udd27 speak(\"Found 3 objects\")\n\ud83d\udd27 pick_place_object\n\ud83d\udd27 speak(\"Task completed successfully\")\n\n\u274c Bad:\n\ud83d\udd27 get_detected_objects\n\ud83d\udd27 pick_place_object\n[Silent operation, user uncertain]\n</code></pre>"},{"location":"examples/#6-handle-workspace-bounds","title":"6. Handle Workspace Bounds","text":"<pre><code>\u2705 Good:\n\ud83d\udd27 get_workspace_coordinate_from_point(\"niryo_ws\", \"upper left\")\n\u2713 Result: [0.337, 0.087] \u2190 Valid bounds\n\n\ud83d\udd27 Ensure target within: X=[0.163, 0.337], Y=[-0.087, 0.087]\n\n\u274c Bad:\n\ud83d\udd27 place_object([0.50, 0.20], None)\n\u2717 Error: Coordinates out of workspace bounds\n</code></pre>"},{"location":"examples/#7-use-relative-placement","title":"7. Use Relative Placement","text":"<pre><code>\u2705 Good - Adaptive:\n\ud83d\udd27 pick_place_object(\n     ...,\n     place_coordinate=[0.22, 0.10],\n     location=\"right next to\"\n   )\n\u2713 Robot calculates exact position with safe spacing\n\n\u274c Less Flexible - Manual:\ntarget_y = 0.10 - 0.04  # Manual offset\n\ud83d\udd27 pick_place_object(..., [0.22, 0.06], None)\n\u26a0\ufe0f  Fixed spacing might not be optimal\n</code></pre>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":""},{"location":"examples/#interactive-mode","title":"Interactive Mode","text":"<pre><code>python client/fastmcp_universal_client.py\n\nYou: Sort all objects by size\nYou: Arrange them in a triangle\nYou: Tell me what you see\n</code></pre>"},{"location":"examples/#single-command-mode","title":"Single Command Mode","text":"<pre><code># Run one example\npython client/fastmcp_universal_client.py \\\n  --command \"Sort objects by size\"\n\n# Batch script\nfor cmd in \"Scan workspace\" \"Sort by size\" \"Create triangle\"; do\n  python client/fastmcp_universal_client.py --command \"$cmd\"\n  sleep 2\ndone\n</code></pre>"},{"location":"examples/#web-gui","title":"Web GUI","text":"<pre><code>python robot_gui/mcp_app.py --robot niryo\n\n# Use voice commands or text input\n# See live camera feed\n# Get visual feedback\n</code></pre>"},{"location":"examples/#example-scripts","title":"Example Scripts","text":"<pre><code># Run specific example\npython examples/universal_examples.py workspace_scan\n\n# Run all examples\npython examples/universal_examples.py all\n\n# Compare LLM providers\npython examples/universal_examples.py compare_providers\n</code></pre>"},{"location":"examples/#common-natural-language-commands","title":"Common Natural Language Commands","text":"<pre><code>\"What objects do you see?\"\n\"Pick up the pencil\"\n\"Move the red cube next to the blue square\"\n\"Sort all objects by size\"\n\"Arrange objects in a triangle\"\n\"What's the largest object?\"\n\"Place the smallest object in the center\"\n\"Find a safe place to put this\"\n\"Stack the small cube on the large cube\"\n\"Push the large box 5cm to the right\"\n\"Group objects by color\"\n\"Is there anything near [0.2, 0.0]?\"\n</code></pre> <p>For more information: - API Reference - Complete tool documentation - Setup Guide - Installation and configuration - Troubleshooting - Common issues and solutions</p>"},{"location":"gradio_setup_instructions/","title":"Robot MCP Gradio GUI - Setup Guide","text":""},{"location":"gradio_setup_instructions/#quick-start","title":"Quick Start","text":""},{"location":"gradio_setup_instructions/#1-create-anaconda-environment","title":"1. Create Anaconda Environment","text":"<pre><code># Navigate to robot_mcp directory\ncd robot_mcp\n\n# Create environment from YAML file\nconda env create -f robot_gui/environment.yml\n\n# Activate environment\nconda activate robot_mcp_gui\n</code></pre>"},{"location":"gradio_setup_instructions/#2-configure-api-keys","title":"2. Configure API Keys","text":"<p>Create or edit <code>secrets.env</code> file:</p> <pre><code># At least one LLM API key is required\n\n# OpenAI (GPT-4o, GPT-4o-mini)\nOPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx\n\n# Groq (Free tier available!)\nGROQ_API_KEY=gsk-xxxxxxxxxxxxxxxx\n\n# Google Gemini\nGEMINI_API_KEY=AIzaSy-xxxxxxxxxxxxxxxx\n\n# Optional: ElevenLabs for TTS (if using robot_environment audio)\nELEVENLABS_API_KEY=your_key\n</code></pre>"},{"location":"gradio_setup_instructions/#3-start-required-services","title":"3. Start Required Services","text":"<p>Terminal 1: Start Redis Server <pre><code>docker run -p 6379:6379 redis:alpine\n</code></pre></p> <p>Terminal 2: Start MCP Server <pre><code>conda activate robot_mcp_gui\npython server/fastmcp_robot_server_unified.py --robot niryo --verbose\n</code></pre></p> <p>Terminal 3: Start Object Detection (optional but recommended) <pre><code># In vision_detect_segment repository\npython scripts/detect_objects_publish_annotated_frames.py\n</code></pre></p>"},{"location":"gradio_setup_instructions/#4-launch-gradio-app","title":"4. Launch Gradio App","text":"<pre><code>conda activate robot_mcp_gui\npython robot_gui/mcp_app.py --api groq\n</code></pre>"},{"location":"gradio_setup_instructions/#usage-options","title":"Usage Options","text":""},{"location":"gradio_setup_instructions/#basic-usage","title":"Basic Usage","text":"<pre><code># Default: Groq API, Niryo robot, simulation mode\npython robot_gui/mcp_app.py\n</code></pre>"},{"location":"gradio_setup_instructions/#openai-api","title":"OpenAI API","text":"<pre><code>python robot_gui/mcp_app.py --api openai --model gpt-4o-mini\n</code></pre>"},{"location":"gradio_setup_instructions/#gemini-api","title":"Gemini API","text":"<pre><code>python robot_gui/mcp_app.py --api gemini --model gemini-2.0-flash-exp\n</code></pre>"},{"location":"gradio_setup_instructions/#real-robot-no-simulation","title":"Real Robot (no simulation)","text":"<pre><code>python robot_gui/mcp_app.py --api groq --no-simulation --robot niryo\n</code></pre>"},{"location":"gradio_setup_instructions/#custom-redis-server","title":"Custom Redis Server","text":"<pre><code>python robot_gui/mcp_app.py --redis-host 192.168.1.100 --redis-port 6380\n</code></pre>"},{"location":"gradio_setup_instructions/#public-share-link","title":"Public Share Link","text":"<pre><code>python robot_gui/mcp_app.py --share\n</code></pre>"},{"location":"gradio_setup_instructions/#custom-port","title":"Custom Port","text":"<pre><code>python robot_gui/mcp_app.py --server-port 8080\n</code></pre>"},{"location":"gradio_setup_instructions/#full-command-line-options","title":"Full Command-Line Options","text":"<pre><code>python robot_gui/mcp_app.py --help\n\nOptions:\n  --api {openai,groq,gemini,ollama}\n                        LLM provider (default: groq)\n  --model MODEL         Specific model name\n  --robot {niryo,widowx}\n                        Robot type (default: niryo)\n  --no-simulation       Use real robot hardware\n  --redis-host HOST     Redis server host (default: localhost)\n  --redis-port PORT     Redis server port (default: 6379)\n  --share               Create public Gradio link\n  --server-port PORT    Gradio server port (default: 7860)\n</code></pre>"},{"location":"gradio_setup_instructions/#features","title":"Features","text":""},{"location":"gradio_setup_instructions/#1-live-object-detection-visualization","title":"1. Live Object Detection Visualization","text":"<ul> <li>Real-time annotated frames from Redis</li> <li>Bounding boxes and segmentation masks</li> <li>Object labels and confidence scores</li> <li>Auto-refresh at ~10 FPS</li> </ul>"},{"location":"gradio_setup_instructions/#2-multi-llm-chat-interface","title":"2. Multi-LLM Chat Interface","text":"<ul> <li>Support for OpenAI, Groq, Gemini, Ollama</li> <li>Natural language robot control</li> <li>Tool call tracking</li> <li>Chain-of-thought reasoning display</li> </ul>"},{"location":"gradio_setup_instructions/#3-voice-input","title":"3. Voice Input","text":"<ul> <li>Speech-to-text via Whisper</li> <li>Click \"\ud83c\udfa4 Record Voice\" button</li> <li>Speak your command</li> <li>Automatic transcription to text</li> </ul>"},{"location":"gradio_setup_instructions/#4-system-status-monitor","title":"4. System Status Monitor","text":"<ul> <li>MCP server connection status</li> <li>Redis connection status</li> <li>Speech-to-text availability</li> <li>Current LLM provider and model</li> </ul>"},{"location":"gradio_setup_instructions/#5-example-commands","title":"5. Example Commands","text":"<ul> <li>Pre-filled example tasks</li> <li>Click to populate input field</li> <li>Covers common robot operations</li> </ul>"},{"location":"gradio_setup_instructions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"gradio_setup_instructions/#mcp-server-not-connected","title":"\"MCP server not connected\"","text":"<ol> <li>Check MCP server is running (Terminal 2)</li> <li>Verify server URL: http://localhost:8000</li> <li>Click \"\ud83d\udd0c Connect to MCP Server\" button</li> </ol>"},{"location":"gradio_setup_instructions/#redis-not-connected","title":"\"Redis not connected\"","text":"<ol> <li>Check Redis is running: <code>docker ps</code></li> <li>Test connection: <code>redis-cli ping</code> (should return \"PONG\")</li> <li>Verify host/port settings</li> </ol>"},{"location":"gradio_setup_instructions/#waiting-for-frames","title":"\"Waiting for frames...\"","text":"<ol> <li>Start object detection pipeline (Terminal 3)</li> <li>Check camera is publishing to Redis</li> <li>Verify stream name: <code>annotated_camera</code></li> </ol>"},{"location":"gradio_setup_instructions/#speech-to-text-not-working","title":"Speech-to-text not working","text":"<ol> <li>Check microphone permissions</li> <li>Verify Whisper model downloaded</li> <li>Check GPU/CPU availability for inference</li> </ol>"},{"location":"gradio_setup_instructions/#slow-response-times","title":"Slow response times","text":"<ol> <li>Use faster LLM: Groq (very fast, free tier)</li> <li>Use smaller model: <code>gpt-4o-mini</code>, <code>llama-3.1-8b-instant</code></li> <li>Enable GPU for speech-to-text</li> </ol>"},{"location":"gradio_setup_instructions/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Gradio    \u2502  MCP    \u2502              \u2502  Robot  \u2502             \u2502\n\u2502     GUI     \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  MCP Server  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   Niryo/    \u2502\n\u2502  (Browser)  \u2502Protocol \u2502   (FastMCP)  \u2502   API   \u2502   WidowX    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u25b2                         \u2502                       \u2502\n      \u2502                         \u25bc                       \u2502\n      \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n      \u2502                   \u2502  Redis   \u2502                 \u2502\n      \u2502                   \u2502 Streams  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                         \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         (Annotated Frames)\n</code></pre>"},{"location":"gradio_setup_instructions/#development-tips","title":"Development Tips","text":""},{"location":"gradio_setup_instructions/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<pre><code># MCP Server\npython server/fastmcp_robot_server_unified.py --verbose\n\n# Check logs\ntail -f log/mcp_server_*.log\n</code></pre>"},{"location":"gradio_setup_instructions/#test-redis-connection","title":"Test Redis Connection","text":"<pre><code>from redis_robot_comm import RedisImageStreamer\n\nstreamer = RedisImageStreamer()\nresult = streamer.get_latest_image()\nprint(\"Redis OK\" if result else \"No frames\")\n</code></pre>"},{"location":"gradio_setup_instructions/#test-mcp-connection","title":"Test MCP Connection","text":"<pre><code>curl http://localhost:8000/sse\n# Should return event stream\n</code></pre>"},{"location":"gradio_setup_instructions/#gpu-acceleration","title":"GPU Acceleration","text":""},{"location":"gradio_setup_instructions/#enable-cuda-if-available","title":"Enable CUDA (if available)","text":"<p>Edit <code>environment.yml</code>: <pre><code>dependencies:\n  - pytorch-cuda=11.8  # Or your CUDA version\n  # Remove: - cpuonly\n</code></pre></p> <p>Recreate environment: <pre><code>conda env remove -n robot_mcp_gui\nconda env create -f robot_gui/environment.yml\n</code></pre></p>"},{"location":"gradio_setup_instructions/#support","title":"Support","text":"<p>For issues or questions: - Check logs in <code>log/</code> directory - See main README.md - Open issue on GitHub</p>"},{"location":"gradio_setup_instructions/#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"mcp_api_reference/","title":"MCP Robot Control - API Reference &amp; Architecture","text":"<p>Complete API documentation and system architecture for the Robot MCP system with FastMCP and multi-LLM support.</p>"},{"location":"mcp_api_reference/#table-of-contents","title":"Table of Contents","text":"<ul> <li>System Architecture</li> <li>API Tools Reference</li> <li>Robot Control Tools</li> <li>Object Detection Tools</li> <li>Workspace Tools</li> <li>Feedback Tools</li> <li>Coordinate System</li> <li>Data Types</li> <li>Integration Guide</li> <li>Error Handling</li> <li>Examples</li> </ul>"},{"location":"mcp_api_reference/#system-architecture","title":"System Architecture","text":""},{"location":"mcp_api_reference/#three-layer-architecture","title":"Three-Layer Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Client Layer                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Universal    \u2502  \u2502 Gradio     \u2502  \u2502 Custom      \u2502  \u2502\n\u2502  \u2502 LLM Client   \u2502  \u2502 Web GUI    \u2502  \u2502 Scripts     \u2502  \u2502\n\u2502  \u2502 (OpenAI/     \u2502  \u2502            \u2502  \u2502             \u2502  \u2502\n\u2502  \u2502  Groq/       \u2502  \u2502            \u2502  \u2502             \u2502  \u2502\n\u2502  \u2502  Gemini/     \u2502  \u2502            \u2502  \u2502             \u2502  \u2502\n\u2502  \u2502  Ollama)     \u2502  \u2502            \u2502  \u2502             \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502 HTTP/SSE\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Server Layer                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 FastMCP      \u2502  \u2502 Tool       \u2502  \u2502 Robot       \u2502  \u2502\n\u2502  \u2502 Server       \u2502  \u2502 Handlers   \u2502  \u2502 Environment \u2502  \u2502\n\u2502  \u2502              \u2502  \u2502 (Pydantic  \u2502  \u2502             \u2502  \u2502\n\u2502  \u2502              \u2502  \u2502 Validation)\u2502  \u2502             \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502 Python API\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Hardware Layer                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Robot Arm    \u2502  \u2502 Camera     \u2502  \u2502 Redis       \u2502  \u2502\n\u2502  \u2502 (Niryo/WX)   \u2502  \u2502 (Vision)   \u2502  \u2502 (Comm)      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"mcp_api_reference/#why-fastmcp","title":"Why FastMCP?","text":"<p>FastMCP offers several advantages as a production-ready framework for the Model Context Protocol:</p> <ol> <li>HTTP/SSE Transport: More flexible than stdio, easier to debug</li> <li>Modern Python: Uses async/await patterns with decorators</li> <li>Better Error Handling: Clearer error messages and validation with Pydantic</li> <li>Easier Development: Simpler tool registration with <code>@mcp.tool</code> decorators</li> <li>Network Ready: Can run client and server on different machines</li> <li>Production Features: Enterprise auth, deployment tools, testing frameworks</li> </ol>"},{"location":"mcp_api_reference/#data-flow","title":"Data Flow","text":"<p>Complete Pick-and-Place Workflow:</p> <pre><code>1. USER INPUT (Natural Language)\n   \"Pick up the pencil and place it next to the red cube\"\n   \u2193\n2. LLM PROCESSING (OpenAI/Groq/Gemini/Ollama)\n   \u251c\u2500 Chain-of-Thought: Explains task understanding\n   \u251c\u2500 Planning: Lists execution steps\n   \u2514\u2500 Generates tool calls: get_detected_objects()\n   \u2193\n3. FASTMCP CLIENT \u2192 SERVER (HTTP/SSE)\n   Call: get_detected_objects()\n   \u2193\n4. PYDANTIC VALIDATION\n   \u251c\u2500 Validates input parameters\n   \u251c\u2500 Checks coordinate format [x, y]\n   \u2514\u2500 Verifies Location enum values\n   \u2193\n5. ROBOT ENVIRONMENT\n   \u251c\u2500 Query detected objects from memory\n   \u2514\u2500 Return: [{label: \"pencil\", position: [0.15, -0.05]}, ...]\n   \u2193\n6. LLM PROCESSES RESULT\n   \u2514\u2500 Generates: pick_place_object(...) with coordinates\n   \u2193\n7. ROBOT EXECUTION\n   \u251c\u2500 Move to observation pose\n   \u251c\u2500 Move to pick position\n   \u251c\u2500 Close gripper\n   \u251c\u2500 Move to place position\n   \u251c\u2500 Open gripper\n   \u2514\u2500 Return to observation pose\n   \u2193\n8. USER FEEDBACK\n   \"Done! Placed the pencil to the right of the red cube.\"\n</code></pre>"},{"location":"mcp_api_reference/#component-details","title":"Component Details","text":"<p>FastMCP Server (<code>server/fastmcp_robot_server.py</code>): - Exposes 16 robot control tools via <code>@mcp.tool</code> decorators - Handles HTTP/SSE communication on port 8000 - Uses Pydantic models for input validation (see <code>server/schemas.py</code>) - Manages robot environment lifecycle - Converts tool calls to robot actions with error handling</p> <p>Universal Client (<code>client/fastmcp_universal_client.py</code>): - Supports 4 LLM providers via <code>LLMClient</code>:   - OpenAI (GPT-4o, GPT-4o-mini)   - Groq (Kimi K2, Llama 3.3, Mixtral)   - Google Gemini (Gemini 2.0/2.5)   - Ollama (Local models: llama3.2, mistral, etc.) - Auto-detects available APIs based on environment keys - Implements Chain-of-Thought prompting for transparency - Manages conversation history with context limits - Converts tool schemas to LLM function calling format - Comprehensive logging to <code>log/mcp_client_*.log</code></p> <p>Robot Environment (from <code>robot-environment</code> package): - Hardware abstraction layer for Niryo Ned2 and WidowX - Vision-based object detection (OwlV2/YOLO-World) - Coordinate transformations (image \u2192 world) - Motion planning and collision avoidance - Camera streaming via Redis (<code>robot_camera</code> stream) - Object detection publishing (<code>detected_objects</code> stream)</p>"},{"location":"mcp_api_reference/#api-tools-reference","title":"API Tools Reference","text":""},{"location":"mcp_api_reference/#robot-control-tools","title":"Robot Control Tools","text":""},{"location":"mcp_api_reference/#pick_place_object","title":"pick_place_object","text":"<p>Complete pick-and-place operation in a single call.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\n@validate_input(PickPlaceInput)\ndef pick_place_object(\n    object_name: str,\n    pick_coordinate: List[float],\n    place_coordinate: List[float],\n    location: Optional[Union[Location, str]] = None\n) -&gt; str\n</code></pre></p> <p>Pydantic Validation: <pre><code>class PickPlaceInput(BaseModel):\n    object_name: str = Field(..., min_length=1)\n    pick_coordinate: List[float] = Field(..., min_length=2, max_length=2)\n    place_coordinate: List[float] = Field(..., min_length=2, max_length=2)\n    location: Optional[Union[Location, str]] = None\n</code></pre></p> <p>Parameters: - <code>object_name</code> (str): Object label (must match detection exactly, case-sensitive)   - Validated: Non-empty string - <code>pick_coordinate</code> (List[float]): World coordinates [x, y] in meters   - Validated: Exactly 2 numeric values - <code>place_coordinate</code> (List[float]): Target coordinates [x, y] in meters   - Validated: Exactly 2 numeric values - <code>location</code> (Optional[str]): Relative placement position   - Validated: Must be one of valid Location enum values   - Options:     - <code>\"left next to\"</code> - Place to the left     - <code>\"right next to\"</code> - Place to the right     - <code>\"above\"</code> - Place above (farther in X)     - <code>\"below\"</code> - Place below (closer in X)     - <code>\"on top of\"</code> - Stack on top     - <code>\"inside\"</code> - Place inside container     - <code>\"close to\"</code> - Near coordinate     - <code>None</code> - Exact coordinate</p> <p>Returns: - Success: <code>\"\u2713 Successfully picked 'pencil' from [0.150, -0.050] and placed it right next to coordinate [0.200, 0.100]\"</code> - Failure: <code>\"\u274c Failed to pick and place 'pencil'\"</code> or validation error message</p> <p>Example: <pre><code># Via natural language (Chain-of-Thought)\nUser: \"Pick up the pencil and place it right of the red cube\"\n\n\ud83e\udd16 CHAIN-OF-THOUGHT REASONING:\n\ud83c\udfaf Task Understanding: Move pencil to position right of red cube\n\ud83d\udccb Analysis: Need current positions of pencil and cube\n\ud83d\udd27 Execution Plan:\n   Step 1: get_detected_objects - Find all objects\n   Step 2: pick_place_object - Move pencil with location=\"right next to\"\n\n# Results in validated tool call\npick_place_object(\n    object_name=\"pencil\",\n    pick_coordinate=[0.15, -0.05],\n    place_coordinate=[0.20, 0.10],\n    location=\"right next to\"\n)\n</code></pre></p> <p>Validation Errors: <pre><code># Invalid coordinate format\npick_place_object(\n    object_name=\"pencil\",\n    pick_coordinate=[0.15],  # \u274c Only 1 value\n    ...\n)\n\u2192 \"\u274c Validation Error: pick_coordinate must have exactly 2 values\"\n\n# Invalid location\npick_place_object(\n    ...,\n    location=\"next_to\"  # \u274c Invalid enum value\n)\n\u2192 \"\u274c Validation Error: location must be one of: left next to, right next to, ...\"\n</code></pre></p> <p>Notes: - Always call <code>get_detected_objects()</code> first to get current coordinates - Object names are case-sensitive - Robot automatically moves to observation pose before/after - All tool calls are logged to <code>log/mcp_server_*.log</code></p>"},{"location":"mcp_api_reference/#pick_object","title":"pick_object","text":"<p>Pick up an object (without placing).</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\n@validate_input(PickObjectInput)\ndef pick_object(\n    object_name: str,\n    pick_coordinate: List[float]\n) -&gt; str\n</code></pre></p> <p>Parameters: - <code>object_name</code> (str): Object label - <code>pick_coordinate</code> (List[float]): World coordinates [x, y] in meters</p> <p>Returns: - Success: <code>\"\u2713 Successfully picked 'pen' from [0.180, -0.030]\"</code> - Failure: <code>\"\u274c Failed to pick 'pen'\"</code> or validation error</p> <p>Example: <pre><code>pick_object(\"pen\", [0.18, -0.03])\n</code></pre></p> <p>Notes: - Must be followed by <code>place_object()</code> to complete operation - Gripper can hold objects up to ~5cm width</p>"},{"location":"mcp_api_reference/#place_object","title":"place_object","text":"<p>Place a currently held object.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\n@validate_input(PlaceObjectInput)\ndef place_object(\n    place_coordinate: List[float],\n    location: Optional[Union[Location, str]] = None\n) -&gt; str\n</code></pre></p> <p>Parameters: - <code>place_coordinate</code> (List[float]): Target coordinates [x, y] in meters - <code>location</code> (Optional[str]): Relative placement (same options as <code>pick_place_object</code>)</p> <p>Returns: - Success: <code>\"\u2713 Successfully placed object left next to coordinate [0.180, -0.100]\"</code> - Failure: <code>\"\u274c Failed to place object\"</code> or validation error</p> <p>Example: <pre><code># First pick\npick_object(\"cube\", [0.20, 0.05])\n# Then place\nplace_object([0.18, -0.10], location=\"left next to\")\n</code></pre></p>"},{"location":"mcp_api_reference/#push_object","title":"push_object","text":"<p>Push an object (for items too large to grip).</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\n@validate_input(PushObjectInput)\ndef push_object(\n    object_name: str,\n    push_coordinate: List[float],\n    direction: str,\n    distance: float\n) -&gt; str\n</code></pre></p> <p>Pydantic Validation: <pre><code>class PushObjectInput(BaseModel):\n    object_name: str = Field(..., min_length=1)\n    push_coordinate: List[float] = Field(..., min_length=2, max_length=2)\n    direction: str = Field(...)\n    distance: float = Field(..., gt=0)\n\n    @field_validator(\"direction\")\n    @classmethod\n    def validate_direction(cls, v):\n        valid_directions = [\"up\", \"down\", \"left\", \"right\"]\n        if v.lower() not in valid_directions:\n            raise ValueError(f\"Direction must be one of: {', '.join(valid_directions)}\")\n        return v\n</code></pre></p> <p>Parameters: - <code>object_name</code> (str): Object label - <code>push_coordinate</code> (List[float]): Current position [x, y] in meters - <code>direction</code> (str): <code>\"up\"</code>, <code>\"down\"</code>, <code>\"left\"</code>, <code>\"right\"</code>   - Validated: Must be one of the four directions - <code>distance</code> (float): Distance in millimeters   - Validated: Must be greater than 0</p> <p>Returns: - Success: <code>\"\u2713 Successfully pushed 'large box' from [0.250, 0.050] right by 50.0mm\"</code> - Failure: <code>\"\u274c Failed to push 'large box'\"</code> or validation error</p> <p>Example: <pre><code>push_object(\"large box\", [0.25, 0.05], \"right\", 50.0)\n</code></pre></p> <p>Notes: - Use when object width &gt; 5cm - Direction is relative to robot's perspective</p>"},{"location":"mcp_api_reference/#move2observation_pose","title":"move2observation_pose","text":"<p>Move robot to observation position above workspace.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef move2observation_pose(workspace_id: str) -&gt; str\n</code></pre></p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID (e.g., <code>\"niryo_ws\"</code>, <code>\"gazebo_1\"</code>)   - Validated: Non-empty string</p> <p>Returns: - Success: <code>\"\u2713 Moved to observation pose for workspace 'niryo_ws'\"</code> - Error: <code>\"\u274c Error moving to observation pose: ...\"</code> or validation error</p> <p>Example: <pre><code>move2observation_pose(\"niryo_ws\")\n</code></pre></p> <p>Notes: - Called automatically before pick/place - Positions camera for optimal object detection</p>"},{"location":"mcp_api_reference/#clear_collision_detected","title":"clear_collision_detected","text":"<p>Reset collision detection flag (Niryo only).</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef clear_collision_detected() -&gt; str\n</code></pre></p> <p>Returns: - Success: <code>\"\u2713 Collision detection flag cleared\"</code> - Error: <code>\"\u274c Error clearing collision flag: ...\"</code></p> <p>Example: <pre><code>clear_collision_detected()\n</code></pre></p> <p>Notes: - Only needed after collision events - Niryo-specific function</p>"},{"location":"mcp_api_reference/#calibrate","title":"calibrate","text":"<p>Calibrate the robot.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef calibrate() -&gt; str\n</code></pre></p> <p>Returns: - Success: <code>\"\u2713 Robot calibration completed successfully\"</code> - Failure: <code>\"\u274c Robot calibration failed\"</code></p> <p>Example: <pre><code>calibrate()\n</code></pre></p>"},{"location":"mcp_api_reference/#object-detection-tools","title":"Object Detection Tools","text":""},{"location":"mcp_api_reference/#get_detected_objects","title":"get_detected_objects","text":"<p>Get list of all detected objects with optional filters.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\n@validate_input(GetDetectedObjectsInput)\ndef get_detected_objects(\n    location: Union[Location, str] = Location.NONE,\n    coordinate: Optional[List[float]] = None,\n    label: Optional[str] = None\n) -&gt; str\n</code></pre></p> <p>Pydantic Validation: <pre><code>class GetDetectedObjectsInput(BaseModel):\n    location: Optional[Union[Location, str]] = None\n    coordinate: Optional[List[float]] = Field(None, min_length=2, max_length=2)\n    label: Optional[str] = None\n</code></pre></p> <p>Parameters: - <code>location</code> (str, optional): Spatial filter relative to <code>coordinate</code>   - Validated: Must be valid Location enum value   - Options:     - <code>\"left next to\"</code> - Objects to the left     - <code>\"right next to\"</code> - Objects to the right     - <code>\"above\"</code> - Objects above (farther in X)     - <code>\"below\"</code> - Objects below (closer in X)     - <code>\"close to\"</code> - Within 2cm radius     - <code>None</code> - No filter (default) - <code>coordinate</code> (List[float], optional): Reference coordinate [x, y]   - Validated: If provided, must be exactly 2 numeric values - <code>label</code> (str, optional): Filter by object name</p> <p>Returns: - Success: JSON string with object list   <pre><code>\"\u2713 Found 3 object(s):\n[\n  {\n    \"label\": \"pencil\",\n    \"position\": {\"x\": 0.150, \"y\": -0.050},\n    \"size\": {\n      \"width_m\": 0.015,\n      \"height_m\": 0.120,\n      \"area_cm2\": 18.0\n    },\n    \"orientation_rad\": 0.785\n  },\n  ...\n]\"\n</code></pre> - No objects: <code>\"\u2713 No objects detected matching the criteria\"</code> - Error: <code>\"\u274c Error getting detected objects: ...\"</code> or validation error</p> <p>Examples: <pre><code># Get all objects\nall_objects = get_detected_objects()\n\n# Get objects near [0.2, 0.0]\nnearby = get_detected_objects(\n    location=\"close to\",\n    coordinate=[0.2, 0.0]\n)\n\n# Get all pencils\npencils = get_detected_objects(label=\"pencil\")\n\n# Get cubes to the left of [0.20, 0.0]\nleft_cubes = get_detected_objects(\n    location=\"left next to\",\n    coordinate=[0.20, 0.0],\n    label=\"cube\"\n)\n</code></pre></p> <p>Notes: - Always call this before pick/place to get current positions - Object positions update continuously via camera - Coordinates are center of mass (COM) of objects - Robot automatically moves to observation pose first</p>"},{"location":"mcp_api_reference/#get_detected_object","title":"get_detected_object","text":"<p>Find specific object at or near a coordinate.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef get_detected_object(\n    coordinate: List[float],\n    label: Optional[str] = None\n) -&gt; str\n</code></pre></p> <p>Parameters: - <code>coordinate</code> (List[float]): World coordinates [x, y] to search near   - Validated: Exactly 2 numeric values - <code>label</code> (str, optional): Filter by object name</p> <p>Returns: - Success: JSON string with single object   <pre><code>\"\u2713 Found object near [0.180, -0.050]:\n{\n  \"label\": \"pen\",\n  \"position\": {\"x\": 0.180, \"y\": -0.050},\n  ...\n}\"\n</code></pre> - Not found: <code>\"\u2713 No object found near [0.180, -0.050]\"</code> - Error: <code>\"\u274c Error getting detected object: ...\"</code> or validation error</p> <p>Example: <pre><code># Find any object at [0.18, -0.05]\nobj = get_detected_object([0.18, -0.05])\n\n# Find specifically a \"pen\" at that location\npen = get_detected_object([0.18, -0.05], label=\"pen\")\n</code></pre></p> <p>Notes: - Searches within 2cm radius of coordinate - Returns first match if multiple objects found</p>"},{"location":"mcp_api_reference/#get_largest_detected_object","title":"get_largest_detected_object","text":"<p>Get the largest object by area.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef get_largest_detected_object() -&gt; str\n</code></pre></p> <p>Returns: - Success: JSON string with largest object and size   <pre><code>\"\u2713 Largest object (0.0025 m\u00b2):\n{\n  \"label\": \"blue square\",\n  \"position\": {\"x\": 0.180, \"y\": -0.100},\n  \"size\": {\"area_cm2\": 25.0, ...}\n}\"\n</code></pre> - No objects: <code>\"\u2713 No objects detected\"</code></p> <p>Example: <pre><code>largest_obj = get_largest_detected_object()\n</code></pre></p>"},{"location":"mcp_api_reference/#get_smallest_detected_object","title":"get_smallest_detected_object","text":"<p>Get the smallest object by area.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef get_smallest_detected_object() -&gt; str\n</code></pre></p> <p>Returns: - Success: JSON string with smallest object and size - No objects: <code>\"\u2713 No objects detected\"</code></p> <p>Example: <pre><code>smallest_obj = get_smallest_detected_object()\n</code></pre></p>"},{"location":"mcp_api_reference/#get_detected_objects_sorted","title":"get_detected_objects_sorted","text":"<p>Get objects sorted by size.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef get_detected_objects_sorted(\n    ascending: bool = True\n) -&gt; str\n</code></pre></p> <p>Parameters: - <code>ascending</code> (bool): If True, smallest to largest; if False, largest to smallest   - Validated: Must be boolean</p> <p>Returns: - Success: JSON string with sorted object list   <pre><code>\"\u2713 Found 3 object(s) sorted smallest to largest:\n[...]\"\n</code></pre> - No objects: <code>\"\u2713 No objects detected\"</code></p> <p>Example: <pre><code># Smallest to largest\nsorted_objs = get_detected_objects_sorted(ascending=True)\n\n# Largest to smallest\nsorted_objs = get_detected_objects_sorted(ascending=False)\n</code></pre></p> <p>Notes: - Useful for size-based sorting tasks - Sorting is by area (width \u00d7 height)</p>"},{"location":"mcp_api_reference/#workspace-tools","title":"Workspace Tools","text":""},{"location":"mcp_api_reference/#get_largest_free_space_with_center","title":"get_largest_free_space_with_center","text":"<p>Find largest empty space in workspace.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef get_largest_free_space_with_center() -&gt; str\n</code></pre></p> <p>Returns: - Success: <code>\"\u2713 Largest free space: 0.0045 m\u00b2 at center coordinates [0.240, -0.030]\"</code> - Error: <code>\"\u274c Error getting largest free space: ...\"</code></p> <p>Example: <pre><code>result = get_largest_free_space_with_center()\n# Use center coordinates for safe placement\n</code></pre></p> <p>Notes: - Useful for finding safe placement locations - Considers all detected objects as obstacles - Returns center of largest contiguous free area</p>"},{"location":"mcp_api_reference/#get_workspace_coordinate_from_point","title":"get_workspace_coordinate_from_point","text":"<p>Get coordinate of workspace corner or center.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\n@validate_input(WorkspacePointInput)\ndef get_workspace_coordinate_from_point(\n    workspace_id: str,\n    point: str\n) -&gt; str\n</code></pre></p> <p>Pydantic Validation: <pre><code>class WorkspacePointInput(BaseModel):\n    workspace_id: str = Field(..., min_length=1)\n    point: str = Field(...)\n\n    @field_validator(\"point\")\n    @classmethod\n    def validate_point(cls, v):\n        valid_points = [\n            \"upper left corner\", \"upper right corner\",\n            \"lower left corner\", \"lower right corner\", \"center point\"\n        ]\n        if v.lower() not in valid_points:\n            raise ValueError(f\"Point must be one of: {', '.join(valid_points)}\")\n        return v\n</code></pre></p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID (e.g., <code>\"niryo_ws\"</code>) - <code>point</code> (str): Point name   - Validated: Must be one of valid point names   - Options:     - <code>\"upper left corner\"</code>     - <code>\"upper right corner\"</code>     - <code>\"lower left corner\"</code>     - <code>\"lower right corner\"</code>     - <code>\"center point\"</code></p> <p>Returns: - Success: <code>\"\u2713 Coordinate of 'center point' in workspace 'niryo_ws': [0.250, 0.000]\"</code> - Invalid: <code>\"\u274c Could not get coordinate for 'invalid_point' in workspace 'niryo_ws'\"</code> or validation error</p> <p>Example: <pre><code>upper_left = get_workspace_coordinate_from_point(\"niryo_ws\", \"upper left corner\")\ncenter = get_workspace_coordinate_from_point(\"niryo_ws\", \"center point\")\n</code></pre></p> <p>Notes: - Niryo workspace: upper_left=[0.337, 0.087], lower_right=[0.163, -0.087] - Useful for boundary-aware placement - Center is at approximately [0.25, 0.0]</p>"},{"location":"mcp_api_reference/#get_object_labels_as_string","title":"get_object_labels_as_string","text":"<p>Get list of recognizable object types.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef get_object_labels_as_string() -&gt; str\n</code></pre></p> <p>Returns: - Success: <code>\"\u2713 Detectable objects: pencil, pen, cube, cylinder, chocolate bar, cigarette, ...\"</code></p> <p>Example: <pre><code>labels = get_object_labels_as_string()\n</code></pre></p> <p>Notes: - Shows all labels the vision system can detect - Labels are used in pick/place operations - Case-sensitive matching required</p>"},{"location":"mcp_api_reference/#add_object_name2object_labels","title":"add_object_name2object_labels","text":"<p>Add new object type to recognition system.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef add_object_name2object_labels(object_name: str) -&gt; str\n</code></pre></p> <p>Parameters: - <code>object_name</code> (str): New object label to add   - Validated: Non-empty string</p> <p>Returns: - Success: <code>\"\u2713 Added 'screwdriver' to the list of recognizable objects\"</code> - Error: <code>\"\u274c Validation Error: object_name must be a non-empty string\"</code> or other error</p> <p>Example: <pre><code>result = add_object_name2object_labels(\"screwdriver\")\n</code></pre></p> <p>Notes: - Extends detection capabilities dynamically - New labels available immediately - Vision model will attempt to detect new objects</p>"},{"location":"mcp_api_reference/#feedback-tools","title":"Feedback Tools","text":""},{"location":"mcp_api_reference/#speak","title":"speak","text":"<p>Text-to-speech output for audio feedback.</p> <p>Function Signature: <pre><code>@mcp.tool\n@log_tool_call\ndef speak(text: str) -&gt; str\n</code></pre></p> <p>Parameters: - <code>text</code> (str): Message to speak   - Validated: Non-empty string</p> <p>Returns: - Success: <code>\"\u2713 Speaking: 'Task completed successfully'\"</code> - Error: <code>\"\u274c Validation Error: text must be a non-empty string\"</code> or other error</p> <p>Example: <pre><code>speak(\"I have picked up the pencil\")\nspeak(\"Task completed successfully\")\n</code></pre></p> <p>Notes: - Asynchronous - doesn't block execution - Uses ElevenLabs or Kokoro TTS (based on configuration) - Useful for user feedback during long operations</p>"},{"location":"mcp_api_reference/#coordinate-system","title":"Coordinate System","text":""},{"location":"mcp_api_reference/#robot-base-frame","title":"Robot Base Frame","text":"<pre><code>        Y (left)\n        \u2191\n        \u2502\n0.087 \u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Upper workspace boundary\n        \u2502\n    0 \u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Center line (Y=0)\n        \u2502\n-0.087 \u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Lower workspace boundary\n        \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 X (forward)\n      0.163        0.337\n     (closer)     (farther)\n</code></pre> <p>Key Points: - Origin: Robot base - X-axis: Forward/backward (values increase going forward) - Y-axis: Left/right   - Positive Y = left side (when facing robot)   - Negative Y = right side   - Y = 0 = center line - Z-axis: Up/down (not used in 2D pick-and-place) - Units: Meters</p>"},{"location":"mcp_api_reference/#niryo-workspace-bounds","title":"Niryo Workspace Bounds","text":"<pre><code>X_MIN = 0.163  # Closer to robot\nX_MAX = 0.337  # Farther from robot\nY_MIN = -0.087  # Right side\nY_MAX = 0.087   # Left side\n</code></pre>"},{"location":"mcp_api_reference/#coordinate-examples","title":"Coordinate Examples","text":"<pre><code># Center of workspace\n[0.25, 0.0]\n\n# Upper left corner (far and left)\n[0.337, 0.087]\n\n# Lower right corner (close and right)\n[0.163, -0.087]\n\n# Left side, middle distance\n[0.25, 0.06]\n\n# Right side, far\n[0.30, -0.05]\n</code></pre>"},{"location":"mcp_api_reference/#location-semantics","title":"Location Semantics","text":"<pre><code>Location.LEFT_NEXT_TO   # y &gt; reference_y (positive direction)\nLocation.RIGHT_NEXT_TO  # y &lt; reference_y (negative direction)\nLocation.ABOVE          # x &gt; reference_x (farther from robot)\nLocation.BELOW          # x &lt; reference_x (closer to robot)\nLocation.CLOSE_TO       # distance &lt;= 0.02m (2cm radius)\n</code></pre>"},{"location":"mcp_api_reference/#data-types","title":"Data Types","text":""},{"location":"mcp_api_reference/#object-dictionary-json-response","title":"Object Dictionary (JSON Response)","text":"<pre><code>{\n    \"label\": str,           # Object name (e.g., \"pencil\")\n    \"position\": {\n        \"x\": float,         # X coordinate in meters\n        \"y\": float          # Y coordinate in meters\n    },\n    \"size\": {\n        \"width_m\": float,   # Width in meters\n        \"height_m\": float,  # Height in meters\n        \"area_cm2\": float   # Area in square centimeters\n    },\n    \"orientation_rad\": float  # Gripper rotation in radians\n}\n</code></pre>"},{"location":"mcp_api_reference/#location-enum","title":"Location Enum","text":"<pre><code>from robot_workspace import Location\n\nclass Location(Enum):\n    LEFT_NEXT_TO = \"left next to\"\n    RIGHT_NEXT_TO = \"right next to\"\n    ABOVE = \"above\"\n    BELOW = \"below\"\n    ON_TOP_OF = \"on top of\"\n    INSIDE = \"inside\"\n    CLOSE_TO = \"close to\"\n    NONE = \"none\"\n</code></pre>"},{"location":"mcp_api_reference/#pydantic-input-models","title":"Pydantic Input Models","text":"<p>All tool inputs are validated using Pydantic models in <code>server/schemas.py</code>:</p> <pre><code># Example: PickPlaceInput model\nclass PickPlaceInput(BaseModel):\n    \"\"\"Input validation for pick_place_object.\"\"\"\n\n    object_name: str = Field(..., min_length=1, description=\"Name of the object to pick\")\n    pick_coordinate: List[float] = Field(..., min_length=2, max_length=2)\n    place_coordinate: List[float] = Field(..., min_length=2, max_length=2)\n    location: Optional[Union[Location, str]] = Field(None, description=\"Relative placement location\")\n\n    class Config:\n        arbitrary_types_allowed = True  # Allow enum types\n\n    @field_validator(\"pick_coordinate\", \"place_coordinate\")\n    @classmethod\n    def validate_coordinates(cls, v):\n        if not all(isinstance(x, (int, float)) for x in v):\n            raise ValueError(\"Coordinates must be numeric values [x, y]\")\n        return v\n\n    @field_validator(\"location\")\n    @classmethod\n    def validate_location(cls, v):\n        if v is None:\n            return v\n        if isinstance(v, Location):\n            return v\n        if isinstance(v, str):\n            valid_locations = [loc.value for loc in Location if loc\n</code></pre>"},{"location":"mcp_api_reference/#todo-example-above-uncomplete","title":"TODO: example above uncomplete","text":""},{"location":"mcp_api_reference/#integration-guide","title":"Integration Guide","text":""},{"location":"mcp_api_reference/#programmatic-usage","title":"Programmatic Usage","text":"<p>Basic Integration:</p> <pre><code>from client.fastmcp_universal_client import RobotUniversalMCPClient\nimport asyncio\n\nasync def main():\n    # Initialize client (auto-detects API)\n    client = RobotUniversalMCPClient()\n\n    # Or specify provider\n    # client = RobotUniversalMCPClient(\n    #     api_choice=\"openai\",\n    #     model=\"gpt-4o\",\n    #     temperature=0.7\n    # )\n\n    # Connect to server\n    await client.connect()\n\n    # Execute commands\n    response1 = await client.chat(\"What objects do you see?\")\n    print(response1)\n\n    response2 = await client.chat(\"Pick up the largest object\")\n    print(response2)\n\n    response3 = await client.chat(\"Place it in the center\")\n    print(response3)\n\n    # Disconnect\n    await client.disconnect()\n\nasyncio.run(main())\n</code></pre>"},{"location":"mcp_api_reference/#advanced-integration-patterns","title":"Advanced Integration Patterns","text":"<p>Batch Processing:</p> <pre><code>async def batch_commands(commands: List[str]):\n    \"\"\"Execute multiple commands sequentially.\"\"\"\n    client = RobotUniversalMCPClient()\n    await client.connect()\n\n    results = []\n    for cmd in commands:\n        response = await client.chat(cmd)\n        results.append(response)\n        await asyncio.sleep(1)  # Pause between commands\n\n    await client.disconnect()\n    return results\n\ncommands = [\n    \"What objects do you see?\",\n    \"Sort objects by size\",\n    \"Create a triangle pattern\"\n]\nresults = asyncio.run(batch_commands(commands))\n</code></pre> <p>Custom System Prompt:</p> <pre><code>client = RobotUniversalMCPClient()\nawait client.connect()\n\n# Modify system prompt for specific behavior\nclient.system_prompt = \"\"\"You are a precision robot assistant.\nAlways verify coordinates before moving.\nSpeak aloud what you're doing.\nIf uncertain, ask for clarification.\nCRITICAL: Never place objects where other objects exist.\"\"\"\n\nresponse = await client.chat(\"Organize the workspace\")\n</code></pre> <p>Provider Switching During Runtime:</p> <pre><code># Start with OpenAI\nclient = RobotUniversalMCPClient(api_choice=\"openai\")\nawait client.connect()\n\n# Do some work\nawait client.chat(\"Pick up the pencil\")\n\n# Switch to Groq for faster inference\nfrom llm_client import LLMClient\nclient.llm_client = LLMClient(\n    api_choice=\"groq\",\n    model=\"llama-3.3-70b-versatile\"\n)\n\n# Continue with new provider\nawait client.chat(\"Place it at the center\")\n</code></pre> <p>Error Handling:</p> <pre><code>async def safe_robot_command(command: str):\n    \"\"\"Execute command with comprehensive error handling.\"\"\"\n    client = RobotUniversalMCPClient()\n\n    try:\n        await client.connect()\n        response = await client.chat(command)\n        return {\"success\": True, \"response\": response}\n\n    except ConnectionError as e:\n        return {\"success\": False, \"error\": f\"Connection failed: {e}\"}\n\n    except TimeoutError as e:\n        return {\"success\": False, \"error\": f\"Command timeout: {e}\"}\n\n    except Exception as e:\n        return {\"success\": False, \"error\": f\"Unexpected error: {e}\"}\n\n    finally:\n        try:\n            await client.disconnect()\n        except:\n            pass\n\nresult = asyncio.run(safe_robot_command(\"What objects do you see?\"))\nif result[\"success\"]:\n    print(result[\"response\"])\nelse:\n    print(f\"Error: {result['error']}\")\n</code></pre>"},{"location":"mcp_api_reference/#direct-fastmcp-client-usage","title":"Direct FastMCP Client Usage","text":"<p>Lower-Level Control:</p> <pre><code>from fastmcp import Client\nfrom fastmcp.client.transports import SSETransport\nimport asyncio\n\nasync def direct_mcp_usage():\n    \"\"\"Use FastMCP client directly without LLM.\"\"\"\n\n    # Initialize transport and client\n    transport = SSETransport(\"http://127.0.0.1:8000/sse\")\n    client = Client(transport)\n\n    # Connect\n    await client.__aenter__()\n\n    # List available tools\n    tools = await client.list_tools()\n    print(f\"Available tools: {[t.name for t in tools]}\")\n\n    # Call tool directly\n    result = await client.call_tool(\n        \"get_detected_objects\",\n        {}\n    )\n\n    # Extract text from result\n    if result.content:\n        text = \"\\n\".join([\n            item.text for item in result.content\n            if hasattr(item, \"text\")\n        ])\n        print(text)\n\n    # Disconnect\n    await client.__aexit__(None, None, None)\n\nasyncio.run(direct_mcp_usage())\n</code></pre> <p>Manual Tool Execution:</p> <pre><code>async def manual_pick_and_place():\n    \"\"\"Execute pick-and-place without LLM reasoning.\"\"\"\n    transport = SSETransport(\"http://127.0.0.1:8000/sse\")\n    client = Client(transport)\n\n    await client.__aenter__()\n\n    try:\n        # Get detected objects\n        objects_result = await client.call_tool(\n            \"get_detected_objects\",\n            {\"label\": \"pencil\"}\n        )\n\n        # Parse result (simplified)\n        # In production, parse JSON properly\n        print(\"Objects:\", objects_result.content[0].text)\n\n        # Execute pick and place\n        pick_result = await client.call_tool(\n            \"pick_place_object\",\n            {\n                \"object_name\": \"pencil\",\n                \"pick_coordinate\": [0.15, -0.05],\n                \"place_coordinate\": [0.20, 0.10],\n                \"location\": \"right next to\"\n            }\n        )\n\n        print(\"Pick-place result:\", pick_result.content[0].text)\n\n    finally:\n        await client.__aexit__(None, None, None)\n\nasyncio.run(manual_pick_and_place())\n</code></pre>"},{"location":"mcp_api_reference/#multi-provider-comparison","title":"Multi-Provider Comparison","text":"<p>Benchmark Different LLM Providers:</p> <pre><code>import time\nfrom typing import Dict, Any\n\nasync def compare_providers(task: str) -&gt; Dict[str, Any]:\n    \"\"\"Compare performance across all available providers.\"\"\"\n    providers = [\"openai\", \"groq\", \"gemini\", \"ollama\"]\n    results = {}\n\n    for provider in providers:\n        try:\n            client = RobotUniversalMCPClient(api_choice=provider)\n            await client.connect()\n\n            start = time.time()\n            response = await client.chat(task)\n            elapsed = time.time() - start\n\n            results[provider] = {\n                \"response\": response,\n                \"time_seconds\": elapsed,\n                \"model\": client.llm_client.llm,\n                \"success\": True\n            }\n\n            await client.disconnect()\n\n        except Exception as e:\n            results[provider] = {\n                \"error\": str(e),\n                \"success\": False\n            }\n\n    return results\n\n# Usage\ntask = \"What objects do you see and where are they?\"\nresults = asyncio.run(compare_providers(task))\n\nfor provider, result in results.items():\n    if result[\"success\"]:\n        print(f\"\\n{provider.upper()}: {result['time_seconds']:.2f}s\")\n        print(f\"Model: {result['model']}\")\n        print(f\"Response: {result['response'][:100]}...\")\n    else:\n        print(f\"\\n{provider.upper()}: FAILED - {result['error']}\")\n</code></pre>"},{"location":"mcp_api_reference/#conditional-execution","title":"Conditional Execution","text":"<p>Smart Task Execution:</p> <pre><code>async def smart_placement():\n    \"\"\"Find best placement location automatically.\"\"\"\n    client = RobotUniversalMCPClient()\n    await client.connect()\n\n    # LLM will use get_largest_free_space_with_center tool\n    response = await client.chat(\n        \"Find the largest free space and place the pencil there\"\n    )\n\n    await client.disconnect()\n    return response\n\nasync def conditional_pickup():\n    \"\"\"Pick object with fallback if too large.\"\"\"\n    client = RobotUniversalMCPClient()\n    await client.connect()\n\n    response = await client.chat(\n        \"Try to pick up the large box. \"\n        \"If it's too large for the gripper (width &gt; 5cm), \"\n        \"push it 50mm to the right instead.\"\n    )\n\n    await client.disconnect()\n    return response\n</code></pre>"},{"location":"mcp_api_reference/#state-management","title":"State Management","text":"<p>Stateful Robot Operations:</p> <pre><code>class RobotController:\n    \"\"\"Wrapper for stateful robot operations.\"\"\"\n\n    def __init__(self, api_choice: str = None):\n        self.client = RobotUniversalMCPClient(api_choice=api_choice)\n        self.connected = False\n        self.task_history = []\n\n    async def connect(self):\n        \"\"\"Connect to MCP server.\"\"\"\n        if not self.connected:\n            await self.client.connect()\n            self.connected = True\n\n    async def execute_task(self, task: str) -&gt; str:\n        \"\"\"Execute task and track history.\"\"\"\n        if not self.connected:\n            await self.connect()\n\n        response = await self.client.chat(task)\n\n        self.task_history.append({\n            \"task\": task,\n            \"response\": response,\n            \"timestamp\": time.time()\n        })\n\n        return response\n\n    async def get_workspace_state(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current workspace state.\"\"\"\n        objects_response = await self.execute_task(\n            \"List all detected objects with their positions\"\n        )\n\n        return {\n            \"objects\": objects_response,\n            \"task_count\": len(self.task_history),\n            \"last_task\": self.task_history[-1] if self.task_history else None\n        }\n\n    async def cleanup(self):\n        \"\"\"Cleanup resources.\"\"\"\n        if self.connected:\n            await self.client.disconnect()\n            self.connected = False\n\n# Usage\nasync def main():\n    controller = RobotController(api_choice=\"groq\")\n\n    try:\n        await controller.connect()\n\n        # Execute multiple tasks\n        await controller.execute_task(\"What objects do you see?\")\n        await controller.execute_task(\"Pick up the largest object\")\n        await controller.execute_task(\"Place it in the center\")\n\n        # Get final state\n        state = await controller.get_workspace_state()\n        print(f\"Completed {state['task_count']} tasks\")\n\n    finally:\n        await controller.cleanup()\n\nasyncio.run(main())\n</code></pre>"},{"location":"mcp_api_reference/#integration-with-external-systems","title":"Integration with External Systems","text":"<p>ROS Integration Example:</p> <pre><code># Pseudo-code for ROS integration\nimport rospy\nfrom std_msgs.msg import String\n\nclass ROSMCPBridge:\n    \"\"\"Bridge between ROS and MCP robot control.\"\"\"\n\n    def __init__(self):\n        rospy.init_node('mcp_bridge')\n        self.client = RobotUniversalMCPClient()\n\n        # Subscribe to ROS command topic\n        rospy.Subscriber('/robot/command', String, self.command_callback)\n\n        # Publisher for results\n        self.result_pub = rospy.Publisher('/robot/result', String, queue_size=10)\n\n    async def command_callback(self, msg):\n        \"\"\"Handle incoming ROS commands.\"\"\"\n        command = msg.data\n\n        # Execute via MCP\n        response = await self.client.chat(command)\n\n        # Publish result\n        self.result_pub.publish(response)\n\n    async def run(self):\n        \"\"\"Start the bridge.\"\"\"\n        await self.client.connect()\n        rospy.spin()\n        await self.client.disconnect()\n</code></pre> <p>REST API Wrapper:</p> <pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n# Global client (in production, use connection pooling)\nrobot_client = None\n\nclass RobotCommand(BaseModel):\n    command: str\n    provider: str = \"auto\"\n\n@app.on_event(\"startup\")\nasync def startup():\n    global robot_client\n    robot_client = RobotUniversalMCPClient()\n    await robot_client.connect()\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    if robot_client:\n        await robot_client.disconnect()\n\n@app.post(\"/execute\")\nasync def execute_command(cmd: RobotCommand):\n    \"\"\"Execute robot command via REST API.\"\"\"\n    try:\n        response = await robot_client.chat(cmd.command)\n        return {\"success\": True, \"response\": response}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/status\")\nasync def get_status():\n    \"\"\"Get robot status.\"\"\"\n    try:\n        status = await robot_client.chat(\"What objects do you see?\")\n        return {\"connected\": True, \"status\": status}\n    except:\n        return {\"connected\": False}\n\n# Run with: uvicorn script_name:app --reload\n</code></pre>"},{"location":"mcp_api_reference/#testing-and-development","title":"Testing and Development","text":"<p>Mock Client for Testing:</p> <pre><code>class MockMCPClient:\n    \"\"\"Mock client for testing without real robot.\"\"\"\n\n    def __init__(self):\n        self.connected = False\n        self.call_log = []\n\n    async def connect(self):\n        self.connected = True\n\n    async def disconnect(self):\n        self.connected = False\n\n    async def chat(self, message: str) -&gt; str:\n        \"\"\"Return mock responses.\"\"\"\n        self.call_log.append(message)\n\n        if \"what objects\" in message.lower():\n            return \"I can see a pencil at [0.15, -0.05] and a cube at [0.20, 0.10]\"\n        elif \"pick\" in message.lower():\n            return \"Successfully picked up the object\"\n        elif \"place\" in message.lower():\n            return \"Successfully placed the object\"\n        else:\n            return \"Command executed successfully\"\n\n# Use in tests\nasync def test_workflow():\n    client = MockMCPClient()\n    await client.connect()\n\n    response1 = await client.chat(\"What objects do you see?\")\n    assert \"pencil\" in response1\n\n    response2 = await client.chat(\"Pick up the pencil\")\n    assert \"Successfully\" in response2\n\n    assert len(client.call_log) == 2\n\n    await client.disconnect()\n</code></pre>"},{"location":"mcp_api_reference/#performance-optimization","title":"Performance Optimization","text":"<p>Connection Pooling:</p> <pre><code>from asyncio import Semaphore\n\nclass RobotClientPool:\n    \"\"\"Pool of robot clients for concurrent operations.\"\"\"\n\n    def __init__(self, size: int = 3):\n        self.size = size\n        self.clients = []\n        self.semaphore = Semaphore(size)\n\n    async def initialize(self):\n        \"\"\"Create client pool.\"\"\"\n        for i in range(self.size):\n            client = RobotUniversalMCPClient()\n            await client.connect()\n            self.clients.append(client)\n\n    async def execute(self, command: str) -&gt; str:\n        \"\"\"Execute command using available client.\"\"\"\n        async with self.semaphore:\n            # Get first available client\n            client = self.clients[0]  # Simplified - use proper pooling\n            return await client.chat(command)\n\n    async def cleanup(self):\n        \"\"\"Close all clients.\"\"\"\n        for client in self.clients:\n            await client.disconnect()\n\n# Usage for high-throughput scenarios\nasync def batch_parallel():\n    pool = RobotClientPool(size=3)\n    await pool.initialize()\n\n    commands = [\"Command 1\", \"Command 2\", \"Command 3\"]\n    tasks = [pool.execute(cmd) for cmd in commands]\n    results = await asyncio.gather(*tasks)\n\n    await pool.cleanup()\n    return results\n</code></pre>"},{"location":"mcp_api_reference/#logging-and-monitoring","title":"Logging and Monitoring","text":"<p>Enhanced Logging:</p> <pre><code>import logging\nfrom datetime import datetime\n\nclass LoggedRobotClient:\n    \"\"\"Robot client with comprehensive logging.\"\"\"\n\n    def __init__(self):\n        self.client = RobotUniversalMCPClient()\n\n        # Setup logging\n        self.logger = logging.getLogger(\"RobotClient\")\n        handler = logging.FileHandler(\n            f\"robot_client_{datetime.now():%Y%m%d_%H%M%S}.log\"\n        )\n        handler.setFormatter(\n            logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        )\n        self.logger.addHandler(handler)\n        self.logger.setLevel(logging.INFO)\n\n    async def connect(self):\n        self.logger.info(\"Connecting to MCP server...\")\n        await self.client.connect()\n        self.logger.info(\"Connected successfully\")\n\n    async def execute(self, command: str) -&gt; str:\n        self.logger.info(f\"Executing command: {command}\")\n\n        try:\n            response = await self.client.chat(command)\n            self.logger.info(f\"Command succeeded: {response[:100]}...\")\n            return response\n        except Exception as e:\n            self.logger.error(f\"Command failed: {e}\")\n            raise\n\n    async def disconnect(self):\n        self.logger.info(\"Disconnecting...\")\n        await self.client.disconnect()\n        self.logger.info(\"Disconnected\")\n</code></pre>"},{"location":"mcp_api_reference/#best-practices","title":"Best Practices","text":""},{"location":"mcp_api_reference/#1-always-handle-connections-properly","title":"1. Always Handle Connections Properly","text":"<pre><code># \u2705 Good - Use async context manager pattern\nasync def good_practice():\n    client = RobotUniversalMCPClient()\n    try:\n        await client.connect()\n        # Do work\n        result = await client.chat(\"command\")\n    finally:\n        await client.disconnect()\n\n# \u274c Bad - No cleanup\nasync def bad_practice():\n    client = RobotUniversalMCPClient()\n    await client.connect()\n    result = await client.chat(\"command\")\n    # Connection never closed!\n</code></pre>"},{"location":"mcp_api_reference/#2-validate-user-input","title":"2. Validate User Input","text":"<pre><code># \u2705 Good - Validate before sending\ndef validate_command(cmd: str) -&gt; bool:\n    if not cmd or not cmd.strip():\n        return False\n    if len(cmd) &gt; 500:  # Too long\n        return False\n    return True\n\nif validate_command(user_input):\n    response = await client.chat(user_input)\n</code></pre>"},{"location":"mcp_api_reference/#3-use-appropriate-provider-for-task","title":"3. Use Appropriate Provider for Task","text":"<pre><code># Complex reasoning - use OpenAI GPT-4o\nclient = RobotUniversalMCPClient(\n    api_choice=\"openai\",\n    model=\"gpt-4o\"\n)\n\n# Simple tasks - use Groq (faster, free)\nclient = RobotUniversalMCPClient(\n    api_choice=\"groq\",\n    model=\"llama-3.1-8b-instant\"\n)\n\n# Offline/privacy - use Ollama\nclient = RobotUniversalMCPClient(\n    api_choice=\"ollama\",\n    model=\"llama3.2:1b\"\n)\n</code></pre>"},{"location":"mcp_api_reference/#4-monitor-and-log","title":"4. Monitor and Log","text":"<pre><code># Always log important operations\nlogger.info(f\"Starting task: {task_description}\")\nresponse = await client.chat(command)\nlogger.info(f\"Task completed: {response}\")\n</code></pre>"},{"location":"mcp_api_reference/#5-handle-rate-limits","title":"5. Handle Rate Limits","text":"<pre><code>import asyncio\n\nasync def rate_limited_execution(commands: List[str], delay: float = 2.0):\n    \"\"\"Execute commands with rate limiting.\"\"\"\n    results = []\n    for cmd in commands:\n        result = await client.chat(cmd)\n        results.append(result)\n        await asyncio.sleep(delay)  # Avoid rate limits\n    return results\n</code></pre>"},{"location":"mcp_api_reference/#quick-reference","title":"Quick Reference","text":""},{"location":"mcp_api_reference/#common-workflows","title":"Common Workflows","text":"<pre><code># 1. Workspace scan\nresponse = await client.chat(\"What objects do you see?\")\n\n# 2. Pick and place\nresponse = await client.chat(\n    \"Pick up the pencil at [0.15, -0.05] and place it at [0.2, 0.1]\"\n)\n\n# 3. Sort by size\nresponse = await client.chat(\n    \"Sort all objects by size from smallest to largest\"\n)\n\n# 4. Find safe placement\nresponse = await client.chat(\n    \"Place the cube in the largest free space\"\n)\n\n# 5. Conditional execution\nresponse = await client.chat(\n    \"If the object is too large to pick, push it instead\"\n)\n</code></pre>"},{"location":"mcp_api_reference/#environment-setup","title":"Environment Setup","text":"<pre><code># Load API keys\nfrom dotenv import load_dotenv\nload_dotenv(\"secrets.env\")\n\n# Initialize with specific provider\nclient = RobotUniversalMCPClient(\n    api_choice=\"groq\",  # or \"openai\", \"gemini\", \"ollama\"\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.7,\n    max_tokens=4096\n)\n</code></pre> <p>For more examples, see: - Setup Guide - Examples - Troubleshooting</p>"},{"location":"mcp_setup_guide/","title":"MCP Robot Control - Setup &amp; Usage Guide","text":"<p>Complete guide for setting up and using natural language robot control with FastMCP and multi-LLM support.</p>"},{"location":"mcp_setup_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Quick Start</li> <li>Installation</li> <li>Configuration</li> <li>Usage Modes</li> <li>Available LLM Providers</li> <li>Common Tasks</li> <li>Troubleshooting</li> </ul>"},{"location":"mcp_setup_guide/#overview","title":"Overview","text":"<p>The Robot MCP system enables natural language control of robotic arms (Niryo Ned2, WidowX) using:</p> <ul> <li>FastMCP Server - Exposes robot control tools via HTTP/SSE</li> <li>Universal Client - Supports OpenAI, Groq, Gemini, and Ollama</li> <li>Vision System - Real-time object detection</li> <li>Web Interface - Gradio GUI with voice input</li> </ul>"},{"location":"mcp_setup_guide/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Multi-    \u2502  HTTP   \u2502              \u2502  Python \u2502             \u2502\n\u2502   LLM       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 FastMCP      \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   Niryo/    \u2502\n\u2502  (OpenAI/   \u2502  SSE    \u2502 Server       \u2502   API   \u2502   WidowX    \u2502\n\u2502 Groq/Gemini)\u2502         \u2502              \u2502         \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u25b2                                                 \u2502\n      \u2502 Natural Language                     Physical   \u2502\n      \u2502 Commands                             Actions    \u2502\n   \u250c\u2500\u2500\u2534\u2500\u2500\u2510                                          \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510\n   \u2502User \u2502                                          \u2502Objects\u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2518                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"mcp_setup_guide/#quick-start","title":"Quick Start","text":""},{"location":"mcp_setup_guide/#prerequisites","title":"Prerequisites","text":"<pre><code># System requirements\n- Python 3.8+\n- Redis server\n- Niryo Ned2 or WidowX robot (or simulation)\n- At least one LLM API key (OpenAI, Groq, or Gemini) OR Ollama installed\n</code></pre>"},{"location":"mcp_setup_guide/#3-step-setup","title":"3-Step Setup","text":"<p>Step 1: Install Dependencies</p> <pre><code>git clone https://github.com/dgaida/robot_mcp.git\ncd robot_mcp\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\npip install -e .\n</code></pre> <p>Step 2: Configure API Keys</p> <pre><code>cp secrets.env.template secrets.env\n# Edit secrets.env and add at least one API key:\n</code></pre> <pre><code># OpenAI (best reasoning)\nOPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx\n\n# Groq (fastest, free tier available)\nGROQ_API_KEY=gsk-xxxxxxxxxxxxxxxx\n\n# Google Gemini (long context)\nGEMINI_API_KEY=AIzaSy-xxxxxxxxxxxxxxxx\n\n# Ollama - No API key needed (runs locally)\n# Just install: curl -fsSL https://ollama.ai/install.sh | sh\n</code></pre> <p>Step 3: Start System</p> <pre><code># Terminal 1: Start Redis\ndocker run -p 6379:6379 redis:alpine\n\n# Terminal 2: Start FastMCP Server\npython server/fastmcp_robot_server.py --robot niryo\n\n# Terminal 3: Run Universal Client (auto-detects available API)\npython client/fastmcp_universal_client.py\n</code></pre> <p>You're ready! The client will automatically use the first available LLM provider (priority: OpenAI &gt; Groq &gt; Gemini &gt; Ollama).</p>"},{"location":"mcp_setup_guide/#installation","title":"Installation","text":""},{"location":"mcp_setup_guide/#standard-installation","title":"Standard Installation","text":"<pre><code># Clone repository\ngit clone https://github.com/dgaida/robot_mcp.git\ncd robot_mcp\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# Install package\npip install -e .\n</code></pre>"},{"location":"mcp_setup_guide/#dependencies-included","title":"Dependencies Included","text":"<ul> <li><code>fastmcp</code> - Modern MCP implementation</li> <li><code>openai</code> - OpenAI API client</li> <li><code>groq</code> - Groq API client</li> <li><code>google-generativeai</code> - Gemini API client</li> <li><code>ollama</code> - Local LLM support</li> <li><code>robot-environment</code> - Robot control (from GitHub)</li> <li><code>text2speech</code> - TTS integration (from GitHub)</li> <li><code>gradio</code> - Web interface</li> </ul>"},{"location":"mcp_setup_guide/#configuration","title":"Configuration","text":""},{"location":"mcp_setup_guide/#environment-variables","title":"Environment Variables","text":"<p>Create <code>secrets.env</code> in project root:</p> <pre><code># LLM API Keys (add at least one)\nOPENAI_API_KEY=sk-proj-xxxxxxxx\nGROQ_API_KEY=gsk-xxxxxxxx\nGEMINI_API_KEY=AIzaSy-xxxxxxxx\n\n# Optional: ElevenLabs for better TTS\nELEVENLABS_API_KEY=your_key_here\n</code></pre>"},{"location":"mcp_setup_guide/#server-configuration","title":"Server Configuration","text":"<p>Start server with options:</p> <pre><code># Real Niryo robot\npython server/fastmcp_robot_server.py --robot niryo --no-simulation\n\n# Simulated robot\npython server/fastmcp_robot_server.py --robot niryo\n\n# WidowX robot\npython server/fastmcp_robot_server.py --robot widowx --no-simulation\n\n# Custom host/port\npython server/fastmcp_robot_server.py --host 0.0.0.0 --port 8080\n\n# Disable camera (testing)\npython server/fastmcp_robot_server.py --no-camera\n\n# Verbose logging\npython server/fastmcp_robot_server.py --verbose\n</code></pre>"},{"location":"mcp_setup_guide/#client-configuration","title":"Client Configuration","text":"<p>Universal client with auto-detection:</p> <pre><code># Auto-detect API (prefers OpenAI &gt; Groq &gt; Gemini &gt; Ollama)\npython client/fastmcp_universal_client.py\n\n# Force specific provider\npython client/fastmcp_universal_client.py --api openai --model gpt-4o\npython client/fastmcp_universal_client.py --api groq\npython client/fastmcp_universal_client.py --api gemini --model gemini-2.0-flash\npython client/fastmcp_universal_client.py --api ollama --model llama3.2:1b\n\n# Single command mode\npython client/fastmcp_universal_client.py --command \"What objects do you see?\"\n\n# Adjust parameters\npython client/fastmcp_universal_client.py --temperature 0.5 --max-tokens 2048\n</code></pre>"},{"location":"mcp_setup_guide/#usage-modes","title":"\ud83d\udcda Usage Modes","text":""},{"location":"mcp_setup_guide/#1-interactive-chat-mode-default","title":"1. Interactive Chat Mode (Default)","text":"<p>Best for: Exploration, learning, development</p> <pre><code>python client/fastmcp_universal_client.py\n\n\ud83e\udd16 ROBOT CONTROL ASSISTANT (Universal LLM)\nUsing: OPENAI - gpt-4o-mini\n\nYou: What objects do you see?\n\ud83d\udd27 Calling tool: get_detected_objects\n\u2713 Result: Detected 3 objects...\n\n\ud83e\udd16 Assistant: I can see 3 objects:\n   1. A pencil at coordinates [0.15, -0.05]\n   2. A red cube at [0.20, 0.10]\n   3. A blue square at [0.18, -0.10]\n\nYou: Move the pencil next to the red cube\n\ud83d\udd27 Calling tool: pick_place_object\n\u2713 Result: Successfully picked and placed\n\n\ud83e\udd16 Assistant: Done! I've placed the pencil to the right of the red cube.\n\n# Special commands:\nYou: tools          # List available tools\nYou: clear          # Clear conversation history\nYou: switch         # Switch LLM provider\nYou: quit           # Exit\n</code></pre>"},{"location":"mcp_setup_guide/#2-single-command-mode","title":"2. Single Command Mode","text":"<p>Best for: Scripting, automation, testing</p> <pre><code># Execute one command\npython client/fastmcp_universal_client.py --command \"Sort objects by size\"\n\n# Batch script\n#!/bin/bash\ncommands=(\n  \"What objects do you see?\"\n  \"Move the largest object to [0.2, 0.0]\"\n  \"Arrange all objects in a line\"\n)\n\nfor cmd in \"${commands[@]}\"; do\n  python client/fastmcp_universal_client.py --command \"$cmd\"\n  sleep 2\ndone\n</code></pre>"},{"location":"mcp_setup_guide/#3-gradio-web-interface","title":"3. Gradio Web Interface","text":"<p>Best for: User-friendly interaction, demonstrations</p> <pre><code>python robot_gui/mcp_app.py --robot niryo\n\n# Then open browser to http://localhost:7860\n</code></pre> <p>Features: - \ud83d\udcac Chat interface with robot - \ud83d\udcf9 Live camera feed with object annotations - \ud83c\udfa4 Voice input (Whisper-based) - \ud83d\udcca System status monitoring - \ud83d\udd04 Switch LLM providers on-the-fly</p>"},{"location":"mcp_setup_guide/#4-example-scripts","title":"4. Example Scripts","text":"<p>Best for: Learning, templates</p> <pre><code># Run specific example\npython examples/universal_examples.py workspace_scan\n\n# Run all examples\npython examples/universal_examples.py all\n\n# Compare LLM providers\npython examples/universal_examples.py compare_providers\n</code></pre>"},{"location":"mcp_setup_guide/#5-claude-desktop-integration","title":"5. Claude Desktop Integration","text":"<p>Best for: Using with Claude's interface</p> <pre><code>TODO - not yet implemented\n</code></pre> <p>Add to Claude Desktop, restart, and use tools directly in Claude!</p>"},{"location":"mcp_setup_guide/#available-llm-providers","title":"Available LLM Providers","text":""},{"location":"mcp_setup_guide/#provider-comparison","title":"Provider Comparison","text":"Provider Function Calling Speed Cost Offline Best For OpenAI \u2705 Excellent Fast $$ \u274c Production, complex reasoning Groq \u2705 Excellent Very Fast Free tier \u274c Development, prototyping Gemini \u2705 Excellent Fast Free tier \u274c Long context, multimodal Ollama \u26a0\ufe0f Limited Variable Free \u2705 Local testing, privacy"},{"location":"mcp_setup_guide/#recommended-models","title":"Recommended Models","text":"<p>For Complex Tasks: <pre><code># OpenAI - Best reasoning\n--api openai --model gpt-4o\n\n# Groq - Fastest inference\n--api groq --model moonshotai/kimi-k2-instruct-0905\n</code></pre></p> <p>For Development: <pre><code># OpenAI - Fast and cheap\n--api openai --model gpt-4o-mini\n\n# Groq - Free and fast\n--api groq --model llama-3.3-70b-versatile\n</code></pre></p> <p>For Local/Offline: <pre><code># Ollama - No internet required\n--api ollama --model llama3.2:1b\n</code></pre></p>"},{"location":"mcp_setup_guide/#provider-auto-detection","title":"Provider Auto-Detection","text":"<p>If you have multiple API keys configured, the client uses this priority:</p> <ol> <li>OpenAI (if <code>OPENAI_API_KEY</code> set)</li> <li>Groq (if <code>GROQ_API_KEY</code> set)</li> <li>Gemini (if <code>GEMINI_API_KEY</code> set)</li> <li>Ollama (fallback, no key needed)</li> </ol> <p>Override with <code>--api</code> flag: <pre><code># Force Gemini even if OpenAI key exists\npython client/fastmcp_universal_client.py --api gemini\n</code></pre></p>"},{"location":"mcp_setup_guide/#switching-providers","title":"Switching Providers","text":"<p>During Interactive Session: <pre><code>You: switch\n\ud83d\udd04 Current provider: GROQ\nAvailable: openai, groq, gemini, ollama\nSwitch to: openai\n\u2713 Switched to OPENAI - gpt-4o-mini\n</code></pre></p>"},{"location":"mcp_setup_guide/#common-tasks","title":"Common Tasks","text":""},{"location":"mcp_setup_guide/#basic-operations","title":"Basic Operations","text":"<p>1. Scan Workspace <pre><code>You: What objects do you see?\n</code></pre></p> <p>2. Simple Pick and Place <pre><code>You: Pick up the pencil and place it at [0.2, 0.1]\n</code></pre></p> <p>3. Relative Placement <pre><code>You: Move the red cube to the right of the blue square\n</code></pre></p>"},{"location":"mcp_setup_guide/#advanced-tasks","title":"Advanced Tasks","text":"<p>4. Sort by Size <pre><code>You: Sort all objects by size from smallest to largest in a line\n</code></pre></p> <p>5. Create Patterns <pre><code>You: Arrange objects in a triangle pattern\n</code></pre></p> <p>6. Group by Color <pre><code>You: Group objects by color: red on left, blue on right\n</code></pre></p>"},{"location":"mcp_setup_guide/#complex-workflows","title":"Complex Workflows","text":"<p>7. Multi-Step Task <pre><code>You: Execute: 1) Find all objects 2) Move smallest to [0.15, 0.1]\n     3) Move largest right of smallest 4) Report positions\n</code></pre></p> <p>8. Conditional Logic <pre><code>You: If there are more than 3 objects, arrange them in a grid.\n     Otherwise, arrange them in a line.\n</code></pre></p> <p>9. Workspace Cleanup <pre><code>You: Organize the workspace: cubes on left, cylinders in middle,\n     everything else on right, aligned in rows\n</code></pre></p>"},{"location":"mcp_setup_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp_setup_guide/#server-wont-start","title":"Server Won't Start","text":"<p>Problem: Port 8000 already in use</p> <pre><code># Check what's using the port\nlsof -i :8000  # Linux/Mac\nnetstat -ano | findstr :8000  # Windows\n\n# Kill the process\nkill -9 &lt;PID&gt;  # Linux/Mac\ntaskkill /PID &lt;PID&gt; /F  # Windows\n</code></pre> <p>Problem: Redis connection error</p> <pre><code># Start Redis\ndocker run -p 6379:6379 redis:alpine\n\n# Or install locally\n# Linux: sudo apt install redis-server\n# Mac: brew install redis\n</code></pre>"},{"location":"mcp_setup_guide/#client-cant-connect","title":"Client Can't Connect","text":"<p>Problem: \"Connection refused\"</p> <p>Solutions: 1. Verify server is running:    <pre><code>curl http://127.0.0.1:8000/sse\n</code></pre></p> <ol> <li> <p>Check firewall settings</p> </li> <li> <p>Ensure server started successfully (check logs in <code>log/</code> directory)</p> </li> </ol>"},{"location":"mcp_setup_guide/#api-key-issues","title":"API Key Issues","text":"<p>Problem: \"Invalid API key\"</p> <p>Solutions: 1. Verify API key in <code>secrets.env</code>:    <pre><code>cat secrets.env | grep API_KEY\n</code></pre></p> <ol> <li> <p>Test API key directly:    <pre><code>from openai import OpenAI\nclient = OpenAI(api_key=\"your_key\")\n# Should not raise error\n</code></pre></p> </li> <li> <p>Regenerate key:</p> </li> <li>OpenAI: https://platform.openai.com/api-keys</li> <li>Groq: https://console.groq.com/keys</li> <li>Gemini: https://aistudio.google.com/apikey</li> </ol>"},{"location":"mcp_setup_guide/#llm-not-calling-tools","title":"LLM Not Calling Tools","text":"<p>Problem: LLM responds in text only, no robot actions</p> <p>Solutions: 1. Verify tools are registered:    <pre><code>You: tools\n# Should list: pick_place_object, get_detected_objects, etc.\n</code></pre></p> <ol> <li> <p>Use specific commands:    <pre><code>\u2705 Good: \"Pick up the pencil at [0.15, -0.05]\"\n\u274c Bad: \"Do something with the pencil\"\n</code></pre></p> </li> <li> <p>Try different model (some better at tool calling):    <pre><code>--model gpt-4o  # Better than gpt-3.5-turbo\n</code></pre></p> </li> </ol>"},{"location":"mcp_setup_guide/#no-objects-detected","title":"No Objects Detected","text":"<p>Problem: <code>get_detected_objects()</code> returns empty</p> <p>Solutions: 1. Move to observation pose first:    <pre><code>You: Move to observation pose\n</code></pre></p> <ol> <li>Check camera:</li> <li>Is camera connected?</li> <li>Is Redis running?</li> <li> <p>Check camera feed in Gradio GUI</p> </li> <li> <p>Verify lighting and object visibility</p> </li> <li> <p>Check object labels:    <pre><code>You: What objects can you recognize?\n</code></pre></p> </li> </ol>"},{"location":"mcp_setup_guide/#slow-performance","title":"Slow Performance","text":"<p>Problem: Long response times</p> <p>Solutions: 1. Use faster model:    <pre><code>--api groq --model llama-3.1-8b-instant\n</code></pre></p> <ol> <li> <p>Clear conversation history:    <pre><code>You: clear\n</code></pre></p> </li> <li> <p>Reduce detection frequency (edit server):    <pre><code>time.sleep(1.0)  # In camera loop\n</code></pre></p> </li> </ol>"},{"location":"mcp_setup_guide/#common-error-messages","title":"Common Error Messages","text":"<pre><code>Error: \"Maximum iterations reached\"\n\u2192 Task too complex, break into smaller steps\n\nError: \"Object not found\"\n\u2192 Verify object name matches detection exactly (case-sensitive)\n\nError: \"Coordinates out of bounds\"\n\u2192 Valid range: X=[0.163, 0.337], Y=[-0.087, 0.087]\n\nError: \"Rate limit exceeded\"\n\u2192 Wait 60 seconds or upgrade API plan\n</code></pre>"},{"location":"mcp_setup_guide/#best-practices","title":"Best Practices","text":""},{"location":"mcp_setup_guide/#1-always-detect-before-manipulating","title":"1. Always Detect Before Manipulating","text":"<pre><code>\u2705 Good: First ask \"What objects do you see?\"\n         Then use coordinates from detection\n\n\u274c Bad: Assuming coordinates without checking\n</code></pre>"},{"location":"mcp_setup_guide/#2-use-exact-label-matching","title":"2. Use Exact Label Matching","text":"<pre><code>\u2705 Good: \"pencil\" (exact match)\n\u274c Bad: \"Pencil\", \"PENCIL\", \"pen\" (won't match)\n</code></pre>"},{"location":"mcp_setup_guide/#3-provide-clear-instructions","title":"3. Provide Clear Instructions","text":"<pre><code>\u2705 Good: \"Pick up the pencil at [0.15, -0.05] and place it at [0.2, 0.1]\"\n\u274c Bad: \"Move that thing over there\"\n</code></pre>"},{"location":"mcp_setup_guide/#4-check-for-success","title":"4. Check for Success","text":"<pre><code>\u2705 Good: After action, ask \"Did that work?\" or \"Show me the result\"\n\u274c Bad: Assuming success without verification\n</code></pre>"},{"location":"mcp_setup_guide/#5-use-safe-placement","title":"5. Use Safe Placement","text":"<pre><code>\u2705 Good: \"Place object in a safe location\" (LLM will find free space)\n\u274c Bad: Hard-coded coordinates that might collide\n</code></pre>"},{"location":"mcp_setup_guide/#quick-reference","title":"Quick Reference","text":""},{"location":"mcp_setup_guide/#essential-commands","title":"Essential Commands","text":"<pre><code># Start system\ndocker run -p 6379:6379 redis:alpine\npython server/fastmcp_robot_server.py --robot niryo\npython client/fastmcp_universal_client.py\n\n# Test connection\ncurl http://127.0.0.1:8000/sse\n\n# View logs\ntail -f log/mcp_server_*.log\n\n# Stop server\n# Press Ctrl+C in server terminal\n</code></pre>"},{"location":"mcp_setup_guide/#natural-language-examples","title":"Natural Language Examples","text":"<pre><code>\"What objects do you see?\"\n\"Pick up the pencil\"\n\"Move the red cube next to the blue square\"\n\"Sort all objects by size\"\n\"Arrange objects in a triangle\"\n\"What's the largest object?\"\n\"Place the smallest object in the center\"\n</code></pre>"},{"location":"mcp_setup_guide/#interactive-commands","title":"Interactive Commands","text":"<pre><code>tools   - List available tools\nclear   - Clear conversation history\nswitch  - Switch LLM provider\nquit    - Exit interactive mode\n</code></pre>"},{"location":"mcp_setup_guide/#use-cases","title":"\ud83c\udfaf Use Cases","text":""},{"location":"mcp_setup_guide/#1-research-development","title":"1. Research &amp; Development","text":"<ul> <li>Rapid prototyping of robot behaviors</li> <li>Testing manipulation strategies</li> <li>Human-robot interaction studies</li> </ul>"},{"location":"mcp_setup_guide/#2-education","title":"2. Education","text":"<ul> <li>Teaching robotics concepts</li> <li>Demonstrating AI integration</li> <li>Student projects</li> </ul>"},{"location":"mcp_setup_guide/#3-industrial-automation","title":"3. Industrial Automation","text":"<ul> <li>Pick-and-place tasks</li> <li>Quality control sorting</li> <li>Assembly line operations</li> </ul>"},{"location":"mcp_setup_guide/#4-warehouse-logistics","title":"4. Warehouse &amp; Logistics","text":"<ul> <li>Object sorting</li> <li>Inventory management</li> <li>Package handling</li> </ul>"},{"location":"mcp_setup_guide/#5-assistive-robotics","title":"5. Assistive Robotics","text":"<ul> <li>Object retrieval</li> <li>Workspace organization</li> <li>Personalized assistance</li> </ul>"},{"location":"mcp_setup_guide/#getting-help","title":"Getting Help","text":"<p>Resources: - API Reference &amp; Architecture - Complete API documentation - GitHub Issues - Report bugs - Example Scripts - See working examples - MCP Documentation: https://modelcontextprotocol.io</p> <p>Before Opening an Issue:</p> <ul> <li>[ ] Redis is running</li> <li>[ ] Server started successfully (check logs)</li> <li>[ ] At least one API key configured</li> <li>[ ] Client can connect (test with curl)</li> <li>[ ] Objects are visible to camera</li> <li>[ ] Tried examples first</li> </ul>"},{"location":"mcp_setup_guide/#next-steps","title":"Next Steps","text":"<ol> <li>\u2705 Complete Quick Start setup</li> <li>\u2705 Try interactive mode with basic commands</li> <li>\u2705 Run example scripts to see capabilities</li> <li>\u2705 Explore different LLM providers</li> <li>\u2705 Try Gradio web interface</li> <li>\u2705 Review API Reference for advanced features</li> <li>\u2705 Create your own automation scripts</li> </ol> <p>Happy robot commanding! \ud83e\udd16\u2728</p>"},{"location":"troubleshooting/","title":"Robot MCP - Troubleshooting","text":"<p>Common issues and solutions for the Robot MCP system.</p>"},{"location":"troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Connection Issues</li> <li>Object Detection Problems</li> <li>Robot Movement Issues</li> <li>LLM and Tool Call Problems</li> <li>Performance Issues</li> <li>Hardware Problems</li> <li>Development and Testing</li> </ul>"},{"location":"troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"troubleshooting/#fastmcp-server-wont-start","title":"FastMCP Server Won't Start","text":"<p>Symptoms: - Server process exits immediately - Connection timeout errors - \"Port already in use\" messages</p> <p>Solutions:</p> <ol> <li> <p>Check if port 8000 is already in use: <pre><code># Linux/Mac\nlsof -i :8000\n\n# Windows\nnetstat -ano | findstr :8000\n\n# Kill existing process if found\nkill -9 &lt;PID&gt;  # Linux/Mac\ntaskkill /PID &lt;PID&gt; /F  # Windows\n</code></pre></p> </li> <li> <p>Check Python dependencies: <pre><code>pip install --upgrade fastmcp groq robot-environment\n</code></pre></p> </li> <li> <p>Start server manually with verbose logging: <pre><code>python server/fastmcp_robot_server.py --robot niryo --verbose\n</code></pre></p> </li> <li> <p>Check log files: <pre><code>cat log/mcp_server_*.log\n</code></pre></p> </li> </ol> <p>Common Error Messages:</p> <pre><code>ImportError: No module named 'fastmcp'\n\u2192 Solution: pip install fastmcp\n\nModuleNotFoundError: No module named 'robot_environment'\n\u2192 Solution: pip install git+https://github.com/dgaida/robot_environment.git\n\nredis.exceptions.ConnectionError\n\u2192 Solution: Start Redis: docker run -p 6379:6379 redis:alpine\n</code></pre>"},{"location":"troubleshooting/#client-cant-connect-to-server","title":"Client Can't Connect to Server","text":"<p>Symptoms: - \"Connection refused\" errors - Timeout after 30 seconds - No response from server</p> <p>Solutions:</p> <ol> <li> <p>Verify server is running: <pre><code># Check if server is listening\ncurl http://127.0.0.1:8000/sse\n\n# Should return: Connection established or SSE stream\n</code></pre></p> </li> <li> <p>Check firewall settings: <pre><code># Linux: Allow port 8000\nsudo ufw allow 8000\n\n# Windows: Add firewall rule for Python\n</code></pre></p> </li> <li> <p>Verify server/client on same network: <pre><code># In client code, ensure correct URL\ntransport = SSETransport(\"http://127.0.0.1:8000/sse\")\n# Not: http://localhost:8000/sse (can cause issues)\n</code></pre></p> </li> <li> <p>Test with simple HTTP request: <pre><code>import httpx\nasyncio.run(httpx.get(\"http://127.0.0.1:8000/sse\"))\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#groq-api-key-issues","title":"Groq API Key Issues","text":"<p>Symptoms: - \"Invalid API key\" errors - Authentication failures - Empty responses from LLM</p> <p>Solutions:</p> <ol> <li> <p>Verify API key is set: <pre><code># Check secrets.env\ncat secrets.env | grep GROQ_API_KEY\n\n# Should show: GROQ_API_KEY=gsk_...\n</code></pre></p> </li> <li> <p>Test API key: <pre><code>from groq import Groq\nclient = Groq(api_key=\"your_key\")\nresponse = client.chat.completions.create(\n    model=\"llama-3.1-8b-instant\",\n    messages=[{\"role\": \"user\", \"content\": \"test\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre></p> </li> <li> <p>Regenerate API key:</p> </li> <li>Visit https://console.groq.com/keys</li> <li>Create new key</li> <li> <p>Update <code>secrets.env</code></p> </li> <li> <p>Check rate limits: <pre><code>Error: Rate limit exceeded\n\u2192 Solution: Wait or upgrade Groq plan\n\u2192 Free tier: 30 requests/minute\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#object-detection-problems","title":"Object Detection Problems","text":"<p>See troubleshooting.md.</p>"},{"location":"troubleshooting/#robot-movement-issues","title":"Robot Movement Issues","text":"<p>See troubleshooting.md.</p>"},{"location":"troubleshooting/#llm-and-tool-call-problems","title":"LLM and Tool Call Problems","text":""},{"location":"troubleshooting/#llm-not-calling-tools","title":"LLM Not Calling Tools","text":"<p>Symptoms: - LLM responds in text only - No robot actions performed - Tools ignored</p> <p>Solutions:</p> <ol> <li> <p>Verify tools are registered: <pre><code>client = RobotFastMCPClient(groq_api_key=\"key\")\nawait client.connect()\n\ntools = client.available_tools\nprint(f\"Available tools: {[t.name for t in tools]}\")\n# Should list: pick_place_object, get_detected_objects, etc.\n</code></pre></p> </li> <li> <p>Check tool call format: <pre><code># Groq expects specific format\ntools_groq = client._convert_tools_to_groq_format()\nprint(json.dumps(tools_groq[0], indent=2))\n\n# Should have: type, function.name, function.description, function.parameters\n</code></pre></p> </li> <li> <p>Improve prompts: <pre><code># \u2705 Good: Specific command\n\"Pick up the pencil at [0.15, -0.05] and place it at [0.2, 0.1]\"\n\n# \u274c Bad: Vague request\n\"Do something with the pencil\"\n</code></pre></p> </li> <li> <p>Check model capabilities: <pre><code># Some models better at tool calling\n# Recommended:\nmodel=\"moonshotai/kimi-k2-instruct-0905\"  # Excellent\nmodel=\"llama-3.3-70b-versatile\"          # Very good\n\n# Less recommended:\nmodel=\"llama-3.1-8b-instant\"  # May miss some tool calls\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#tool-calls-failing","title":"Tool Calls Failing","text":"<p>Symptoms: - \"Error executing tool\" messages - Partial execution - Tool returns errors</p> <p>Solutions:</p> <ol> <li> <p>Check parameter types: <pre><code># \u2705 Good: Correct types\npick_place_object(\n    object_name=\"pencil\",        # str\n    pick_coordinate=[0.15, -0.05],  # List[float]\n    place_coordinate=[0.2, 0.1],    # List[float]\n    location=\"right next to\"        # str or None\n)\n\n# \u274c Bad: Wrong types\npick_place_object(\n    object_name=123,  # Should be string\n    pick_coordinate=\"[0.15, -0.05]\",  # Should be list\n    ...\n)\n</code></pre></p> </li> <li> <p>Validate coordinates: <pre><code>def is_valid_coordinate(coord):\n    x, y = coord\n    return (0.163 &lt;= x &lt;= 0.337 and\n            -0.087 &lt;= y &lt;= 0.087)\n\nif not is_valid_coordinate([x, y]):\n    print(\"Coordinate out of workspace bounds!\")\n</code></pre></p> </li> <li> <p>Check object exists: <pre><code># Before pick_place_object\nobj = get_detected_object([x, y], label=\"target\")\nif obj is None:\n    print(\"Object not found at specified location\")\n    return\n</code></pre></p> </li> <li> <p>Review server logs: <pre><code>tail -f log/mcp_server_*.log\n\n# Look for:\n# - Tool execution errors\n# - Parameter validation failures\n# - Robot controller errors\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#max-iterations-reached","title":"Max Iterations Reached","text":"<p>Symptoms: - \"Maximum iterations reached\" message - Task incomplete - Loop detected</p> <p>Solutions:</p> <ol> <li> <p>Increase iteration limit: <pre><code># In client chat() method\nmax_iterations = 10  # Increase from 4\n\n# For complex tasks, may need more\n</code></pre></p> </li> <li> <p>Break down complex tasks: <pre><code># \u2705 Good: Step-by-step\n\"First, find all objects\"\n# Wait for response\n\"Now, move the pencil to [0.2, 0.1]\"\n# Wait for response\n\"Finally, move the cube next to it\"\n\n# \u274c Bad: Everything at once\n\"Find objects, sort by size, create triangle pattern, report positions\"\n</code></pre></p> </li> <li> <p>Provide intermediate coordinates: <pre><code># Help LLM with explicit coordinates\n\"Move pencil from [0.15, -0.05] to [0.2, 0.1], then move cube from [0.20, 0.10] to [0.25, 0.0]\"\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-response-times","title":"Slow Response Times","text":"<p>Symptoms: - Long delays between commands - System feels sluggish - Timeouts</p> <p>Solutions:</p> <ol> <li> <p>Use faster Groq model: <pre><code>client = RobotFastMCPClient(\n    groq_api_key=\"key\",\n    model=\"llama-3.1-8b-instant\"  # Fastest\n)\n</code></pre></p> </li> <li> <p>Optimize object detection: <pre><code># Use YOLO-World instead of OwlV2\nobjdetect_model_id=\"yoloworld\"\n\n# Reduce detection frequency\ntime.sleep(1.0)  # In camera loop\n</code></pre></p> </li> <li> <p>Enable GPU acceleration: <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n</code></pre></p> </li> <li> <p>Reduce conversation history: <pre><code># Limit history size\nmax_history = 10\nclient.conversation_history = client.conversation_history[-max_history:]\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptoms: - System running out of memory - Crashes during operation - Slow performance</p> <p>Solutions:</p> <ol> <li> <p>Use float16 instead of float32: <pre><code>Speech2Text(\n    device=\"cuda\",\n    torch_dtype=torch.float16  # Half precision\n)\n</code></pre></p> </li> <li> <p>Clear detection cache: <pre><code># Periodically clear old detections\nenv.clear_detection_cache()\n</code></pre></p> </li> <li> <p>Limit image resolution: <pre><code># In camera capture\nimg = cv2.resize(img, (640, 480))  # Standard resolution\n</code></pre></p> </li> <li> <p>Close unused resources: <pre><code># After task completion\nawait client.disconnect()\nenv.cleanup()\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#rate-limit-errors","title":"Rate Limit Errors","text":"<p>Problem: Too many requests to Groq</p> <p>Solutions: 1. Add delays between commands 2. Use a less powerful model (llama-3.1-8b-instant) 3. Upgrade Groq plan for higher limits 4. Clear conversation history: type <code>clear</code></p>"},{"location":"troubleshooting/#hardware-problems","title":"Hardware Problems","text":"<p>See troubleshooting.md.</p>"},{"location":"troubleshooting/#development-and-testing","title":"Development and Testing","text":""},{"location":"troubleshooting/#testing-without-robot","title":"Testing Without Robot","text":"<p>Use Simulation Mode: <pre><code># Start server in simulation\npython server/fastmcp_robot_server.py --robot niryo\n# (without --no-simulation flag)\n</code></pre></p> <p>Mock Objects for Testing: <pre><code># Create test objects\ntest_objects = [\n    {\"label\": \"pencil\", \"x\": 0.15, \"y\": -0.05, \"width_m\": 0.015, \"height_m\": 0.12},\n    {\"label\": \"cube\", \"x\": 0.20, \"y\": 0.10, \"width_m\": 0.04, \"height_m\": 0.04},\n]\n</code></pre></p>"},{"location":"troubleshooting/#debugging-tips","title":"Debugging Tips","text":"<p>Enable Verbose Logging: <pre><code>python server/fastmcp_robot_server.py --robot niryo --verbose\n</code></pre></p> <p>Add Debug Prints: <pre><code># In client code\nprint(f\"Calling tool: {tool_name}\")\nprint(f\"Arguments: {json.dumps(arguments, indent=2)}\")\n</code></pre></p> <p>Use Interactive Python: <pre><code># Test components directly\nfrom robot_environment import Environment\nenv = Environment(...)\nobjects = env.get_detected_objects()\nprint(objects)\n</code></pre></p> <p>Check Redis Data: <pre><code>redis-cli\n&gt; KEYS *\n&gt; GET robot_camera\n&gt; GET detected_objects\n</code></pre></p>"},{"location":"troubleshooting/#common-development-errors","title":"Common Development Errors","text":"<p>Import Errors: <pre><code># \u2705 Good\nfrom client.fastmcp_groq_client import RobotFastMCPClient\n\n# \u274c Bad\nfrom fastmcp_groq_client import RobotFastMCPClient\n</code></pre></p> <p>Async/Await Issues: <pre><code># \u2705 Good\nasync def main():\n    await client.connect()\n\nasyncio.run(main())\n\n# \u274c Bad\ndef main():\n    client.connect()  # Missing await!\n</code></pre></p> <p>Path Issues: <pre><code># \u2705 Good\nserver_path = Path(__file__).parent.parent / \"server\" / \"fastmcp_robot_server.py\"\n\n# \u274c Bad\nserver_path = \"server/fastmcp_robot_server.py\"  # Relative to cwd\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#information-to-include-in-bug-reports","title":"Information to Include in Bug Reports","text":"<ol> <li> <p>System Information: <pre><code>python --version\npip list | grep -E \"fastmcp|groq|robot-environment\"\nuname -a  # Linux/Mac\n</code></pre></p> </li> <li> <p>Error Messages: <pre><code># Full stack trace\n# Server logs: log/mcp_server_*.log\n# Client output\n</code></pre></p> </li> <li> <p>Reproduction Steps: <pre><code>1. Start server with: python server/...\n2. Run client command: ...\n3. Expected: ...\n4. Actual: ...\n</code></pre></p> </li> <li> <p>Configuration: <pre><code># secrets.env (without actual keys)\n# Server startup command\n# Client configuration\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#resources","title":"Resources","text":"<ul> <li>GitHub Issues: https://github.com/dgaida/robot_mcp/issues</li> <li>Architecture: Architecture Guide</li> <li>Examples: docs/examples.md</li> <li>API Reference: docs/api.md</li> </ul>"},{"location":"troubleshooting/#quick-diagnostic-checklist","title":"Quick Diagnostic Checklist","text":"<p>Before opening an issue, check:</p> <ul> <li>[ ] Redis is running</li> <li>[ ] Groq API key is valid</li> <li>[ ] Server is started and listening on port 8000</li> <li>[ ] Client can connect (test with curl)</li> <li>[ ] Robot is powered on (if using real robot)</li> <li>[ ] Camera is working (check Redis stream)</li> <li>[ ] Object detection is running (check for detections)</li> <li>[ ] Coordinates are within workspace bounds</li> <li>[ ] Object names match detected labels exactly</li> <li>[ ] All dependencies are installed</li> <li>[ ] Log files checked for errors</li> </ul> <p>If all checked and still having issues, please open a GitHub issue with the information above!</p>"}]}